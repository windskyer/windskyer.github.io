<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[nginx日志时间输出]]></title>
    <url>%2F2020%2F04%2F26%2Fnginx%E6%97%A5%E5%BF%97%E6%97%B6%E9%97%B4%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[nginx 日志打印响应时间 request_time 和 upstream_response_time设置log_format，添加request_time，$upstream_response_time，位置随意12345log_format main '"$request_time" "$upstream_response_time" $remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; 日志输出效果：1"0.015" "0.015" 10.1.2.3 - - [20/Mar/2017:04:05:49 +0800] "GET /myApp/servlet/TestServlet HTTP/1.1" 200 52 "-" "Mozilla/4.0 (compatible; MSIE 4.0; Windows NT)" "-" 根据nginx的accesslog中$request_time进行程序优化时，发现有个接口，直接返回数据，平均的$request_time也比较大。原来$request_time包含了用户数据接收时间，而真正程序的响应时间应该用$upstream_response_time。 下面介绍下2者的差别：1、request_time 官网描述：request processing time in seconds with a milliseconds resolution; time elapsed between the first bytes were read from the client and the log write after the last bytes were sent to the client 。 指的就是从接受用户请求的第一个字节到发送完响应数据的时间，即包括接收请求数据时间、程序响应时间、输出 响应数据时间。 2、upstream_response_time 官网描述：keeps times of responses obtained from upstream servers; times are kept in seconds with a milliseconds resolution. Several response times are separated by commas and colons like addresses in the $upstream_addr variable 是指从Nginx向后端建立连接开始到接受完数据然后关闭连接为止的时间。 从上面的描述可以看出，$request_time肯定比$upstream_response_time值大，特别是使用POST方式传递参数时，因为Nginx会把request body缓存住，接受完毕后才会把数据一起发给后端。所以如果用户网络较差，或者传递数据较大时，$request_time会比$upstream_response_time大很多。 所以如果使用nginx的accesslog查看php程序中哪些接口比较慢的话，记得在log_format中加入$upstream_response_time。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s节点下线]]></title>
    <url>%2F2020%2F04%2F20%2Fk8s%E8%8A%82%E7%82%B9%E4%B8%8B%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[下线一般有两种情况,一般是故障或者是迁移。故障节点下线只需要直接摘除下来就可以，因为会从新调度到新的节点。而正常节点迁移则需要先排干节点，即将所有pod在此节点上迁移出去其他节点。 正常节点下线查看节点1kubectl get node xxx 排干,排干时他们提示忽略了ds的pod1kubectl drain hdss7-21.host.com --delete-local-data --force --ignore-daemonsets 注意: 在排干的过程中,此节点标记为不可以调度的状态 删除节点1kubectl delete node hdss7-21.host.com 异常节点直接删除即可，修复后启动服务，节点将自动加入集群 异常节点下线删除节点1kubectl delete node hdss7-21.host.com]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang sync.Mutex用法（互斥量用法）]]></title>
    <url>%2F2020%2F04%2F18%2FGolang-sync-Mutex%E7%94%A8%E6%B3%95%EF%BC%88%E4%BA%92%E6%96%A5%E9%87%8F%E7%94%A8%E6%B3%95%EF%BC%89%2F</url>
    <content type="text"><![CDATA[介绍golang 中的 sync 包实现了两种锁： Mutex：互斥锁 RWMutex：读写锁，RWMutex 基于 Mutex 实现 Mutex（互斥锁） Mutex 为互斥锁，Lock() 加锁，Unlock() 解锁 在一个 goroutine 获得 Mutex 后，其他 goroutine 只能等到这个 goroutine 释放该 Mutex 使用 Lock() 加锁后，不能再继续对其加锁，直到利用 Unlock() 解锁后才能再加锁 在 Lock() 之前使用 Unlock() 会导致 panic 异常 已经锁定的 Mutex 并不与特定的 goroutine 相关联，这样可以利用一个 goroutine 对其加锁，再利用其他 goroutine 对其解锁 在同一个 goroutine 中的 Mutex 解锁之前再次进行加锁，会导致死锁 适用于读写不确定，并且只有一个读或者写的场景 示例加锁和解锁示例1234567891011121314151617181920212223242526272829303132333435package mainimport ( "time" "fmt" "sync")func main() { var mutex sync.Mutex fmt.Println("Lock the lock") mutex.Lock() fmt.Println("The lock is locked") channels := make([]chan int, 4) for i := 0; i < 4; i++ { channels[i] = make(chan int) go func(i int, c chan int) { fmt.Println("Not lock: ", i) mutex.Lock() fmt.Println("Locked: ", i) time.Sleep(time.Second) fmt.Println("Unlock the lock: ", i) mutex.Unlock() c]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kernel 4.x]]></title>
    <url>%2F2020%2F04%2F15%2Fcentos7%E5%AE%89%E8%A3%85kernel-4-x%2F</url>
    <content type="text"><![CDATA[Install Linux Kernel 4.4.5 LTS in CentOS 7I tested this tutorial on CentOS 7 64 bit edition. Although, these steps should work on RHEL 7. Note: Since this kernel is just released, the latest kernel haven’t pushed into the ELRepo yet. It stills shows the 4.4.4 version. I think the latest kernel will be pushed to the repository in few hours. To install the latest kernel, add ELRepo repository. Add ELRepo GPG key: 1# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org Then, add ELRepo in CentOS 7 / RHEL 7 / Scientific Linux 7 using command: 1# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm Enable ELRepo fastest mirror using by installing the following package: 1# yum install yum-plugin-fastestmirror We have added the ELRepo. Now, it is time to install Linux kernel 4.4.1 LTS. 12# uname -r# yum --enablerepo=elrepo-kernel install kernel-ml After installing the Kernel, Reboot your system and select the latest Kernel from the Grub boot menu. Check the Kernel version using command: 1# uname -r Sample output:14.4.4-1.el7.elrepo.x86_64 Congratulation! Your CentOS 7 system is currently running on Kernel 4.4.4. Have any problems after installing Linux Kernel 4.4.x? No worries, reboot your system. Select your previously working Kernel from the Boot menu. Then, remove the newly installed Kernel using command: 1# yum remove kernel-ml]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[理解Go Context机制]]></title>
    <url>%2F2020%2F04%2F03%2F%E7%90%86%E8%A7%A3Go-Context%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1 什么是Context最近在公司分析gRPC源码，proto文件生成的代码，接口函数第一个参数统一是ctx context.Context接口，公司不少同事都不了解这样设计的出发点是什么，其实我也不了解其背后的原理。今天趁着妮妲台风妹子正面登陆深圳，全市停工、停课、停业，在家休息找了一些资料研究把玩一把。 Context通常被译作上下文，它是一个比较抽象的概念。在公司技术讨论时也经常会提到上下文。一般理解为程序单元的一个运行状态、现场、快照，而翻译中上下又很好地诠释了其本质，上下上下则是存在上下层的传递，上会把内容传递给下。在Go语言中，程序单元也就指的是Goroutine。 每个Goroutine在执行之前，都要先知道程序当前的执行状态，通常将这些执行状态封装在一个Context变量中，传递给要执行的Goroutine中。上下文则几乎已经成为传递与请求同生存周期变量的标准方法。在网络编程下，当接收到一个网络请求Request，处理Request时，我们可能需要开启不同的Goroutine来获取数据与逻辑处理，即一个请求Request，会在多个Goroutine中处理。而这些Goroutine可能需要共享Request的一些信息；同时当Request被取消或者超时的时候，所有从这个Request创建的所有Goroutine也应该被结束。 2 context包Go的设计者早考虑多个Goroutine共享数据，以及多Goroutine管理机制。Context介绍请参考Go Concurrency Patterns: Context，golang.org/x/net/context包就是这种机制的实现。 context包不仅实现了在程序单元之间共享状态变量的方法，同时能通过简单的方法，使我们在被调用程序单元的外部，通过设置ctx变量值，将过期或撤销这些信号传递给被调用的程序单元。在网络编程中，若存在A调用B的API, B再调用C的API，若A调用B取消，那也要取消B调用C，通过在A,B,C的API调用之间传递Context，以及判断其状态，就能解决此问题，这是为什么gRPC的接口中带上ctx context.Context参数的原因之一。 Go1.7(当前是RC2版本)已将原来的golang.org/x/net/context包挪入了标准库中，放在$GOROOT/src/context下面。标准库中net、net/http、os/exec都用到了context。同时为了考虑兼容，在原golang.org/x/net/context包下存在两个文件，go17.go是调用标准库的context包，而pre_go17.go则是之前的默认实现，其介绍请参考go程序包源码解读。 context包的核心就是Context接口，其定义如下： 123456type Context interface { Deadline() (deadline time.Time, ok bool) Done()]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker看veth对]]></title>
    <url>%2F2020%2F04%2F01%2Fdocker%E7%9C%8Bveth%E5%AF%B9%2F</url>
    <content type="text"><![CDATA[找到网卡对应的方式，在主机上执行如下命令1234567docker exec -it bash -c 'cat /sys/class/net/eth0/iflink'# 假设返回 12grep -l 12 /sys/class/net/veth*/ifindex# 此时会有如下类似返回/sys/class/net/veth11d4238/ifindex# veth11d4238 即主机上的另一半 做成一个脚本 vethfinder 12345678#!/bin/bashfor container in $(docker ps -q); do iflink=`docker exec -it $container bash -c 'cat /sys/class/net/eth0/iflink'` iflink=`echo $iflink|tr -d '\r'` veth=`grep -l $iflink /sys/class/net/veth*/ifindex` veth=`echo $veth|sed -e 's;^.*net/\(.*\)/ifindex$;\1;'` echo $container:$vethdone 执行 123456789$ docker ps -qc4d8096eff4334ac6e9f1e6ed5a2aa5f3de3 $ sudo ./vethfinderc4d8096eff43:veth11d423834ac6e9f1e6e:veth7d52cd1d5a2aa5f3de3:vethe46073d]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python 图片文字识别+二维码识别]]></title>
    <url>%2F2020%2F03%2F11%2Fpython-%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB-%E4%BA%8C%E7%BB%B4%E7%A0%81%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[文字识别python的pytesseract为文字识别提供了很好的支持。整个实现只需要一行关键代码即可。 前提安装12yum install -y tesseract-langpack-chi_sim tesseract-langpack-chi_tra tesseractpip install pytesseract 代码示例1234from PIL import Imageimportimport pytesseracttext=pytesseract.image_to_string(Image.open(file_path), lang='chi_sim')print(text) 识别语言： 中文简体(chi_sim), 繁体(chi_tra) 二维码识别在没接触 Python 之前，曾使用 Zbar 的客户端进行识别，测了大概几百张相对模糊的图片，Zbar的识别速度要快很多，识别率也比 Zxing 稍微准确那边一丢丢，但是，稍微模糊一点就无法识别。 前提安装12345# 升级 pip 并安装第三方库pip install -U pippip install Pillowpip install pyzbarpip install qrcode 代码示例1234567from PIL import Imageimportimport pyzbar.pyzbar as pyzbarbar_codes = pyzbar.decode(Image.open(file_path))for bar_code in bar_codes: bar_code_info += bar_code.data.decode("utf-8")print(bar_code_info) 整体代码1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding: utf-8 -*-import ioimport requestsimport pytesseractfrom PIL import Imageimport pyzbar.pyzbar as pyzbardef check_image_qrcode(image): bar_code_info = "" buf_image = io.BytesIO() ir = requests.get(image, stream=True) if ir.ok: for chunk in ir: buf_image.write(chunk) else: return bar_code_info img = Image.open(buf_image) if img: bar_codes = pyzbar.decode(img) for bar_code in bar_codes: bar_code_info += bar_code.data.decode("utf-8") if len(bar_code_info) > 0: return bar_code_info if img: bar_code_info = pytesseract.image_to_string(img, lang='chi_tra') return bar_code_infodef main(): image = "https://pic2.zhimg.com/80/v2-50eaea949ac63de5d5a84813d9efe491_720w.jpg" image_info = check_image_qrcode(image) if len(image_info) == 0 : print("11111") else: print(image_info)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群日志审计]]></title>
    <url>%2F2020%2F03%2F10%2F%E9%9B%86%E7%BE%A4%E6%97%A5%E5%BF%97%E5%AE%A1%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[集群做了什么？产生了什么影响？在容器引擎的日常运维中，经常遇到客户反馈线上问题，例如: cloud-controller-manager 服务无法启动，集群初始化卡状态，kubectl top no 命令返回错误，master节点某些pod被kill掉等；值班同学遇到这些问题时，通常会进入集群各个节点查看服务日志，找到产生问题的原因；经过一段时间对值班反馈问题的整理，发现线上服务出现异常的根本原因主要集中在以下几类： 1. 客户修改或删除集群服务配置文件 2. 客户勿删集群资源 3. 节点系统服务异常或资源不足 ；为了更加便捷高效的排查问题，我们引入了集群审计，用于追踪系统日志，配置文件变更，k8s资源操作，将集群在某个时间段内做了什么，产生了什么影响，变得一目了然。 服务架构 集群审计数据源Kubernetes-Auditkubernetes资源操作追踪审计日志 启动和配置kube-apiserver源生支持kubernetes集群操作审计功能，在启动kube-apiserver服务时加入以下参数即可： 12345--audit-policy-file=/etc/kubernetes/audit-policy.yaml # 指定审计策略配置文件路径--audit-log-path=/var/log/kubernetes/audit.log # 指定审计日志输出路径--audit-log-maxage=7 # 审计日志保留天数--audit-log-maxbackup=4 # 审计日志备份数量--audit-log-maxsize=10 # 审计日志文件大小，文件超过指定大小后将循环覆盖写入 kube-apiserver1/etc/kubernetes/manifests/kube-apiserver.yaml 展开源码 审计策略文件1/etc/kubernetes/audit-policy.yaml 展开源码 Inotifywaitinotify日志，记录集群节点文件变更（包括：创建，修改，删除，移动操作） 启动和配置1/usr/bin/inotifywait -mrq -d -o /var/log/inotify.log --timefmt '%Y-%m-%d %H:%M:%S' --format '{"datetime": "%T", "event": "%e", "fpath": "%w%f"}' -e create,delete,modify,move --exclude "(.swp|.inc|.svn|.rar|.tar.gz|.gz|.txt|.zip|.bak|.log|sed*[[:alpha:]])" /etc Sysloglinux系统日志 FilebeatFilebeat是一个轻量级日志传输Agent，可以将指定日志转发到Logstash、Elasticsearch、Kafka、Redis等中。Filebeat占用资源少，而且安装配置也比较简单，支持目前各类主流OS及Docker平台。 启动和配置/etc/filebeat/filebeat.yml 展开源码 审计日志汇总展示Kibana使用Kibana服务地址：http://kibana.kce.ksyun.com:8601/app/kibana#/home?_g=() 进入Kibana页面后，导航栏中选择”Discover” => 在index下拉菜单中选择 “kce-online-audit-*” => 在页面最上面的输入框中输入 “cluster_uuid: ${cluster_uuid}” 按集群uuid过滤审计日志 => “回车”进行搜索查询 审计日志包含三个数据源，在审计中使用”tags”进行标识区分； objectRef.resource和verb 是 kube-apiserver字段，用于快速查看集群中哪些资源做了什么操作； event和fpath 是 inotify字段，用于快速查看哪些文件产生了哪些变更事件 由于syslog的message字段内容数据量大小不一致，切内容非格式化，故没有在Kibana展示模板进行单独配置，如果需要批量查看syslog审计日志可按tags过滤，并定义查看指定审计字段]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多集群资源统一管理之Federation v2]]></title>
    <url>%2F2020%2F03%2F08%2F%E5%A4%9A%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%BB%9F%E4%B8%80%E7%AE%A1%E7%90%86%E4%B9%8BFederation-v2%2F</url>
    <content type="text"><![CDATA[介绍Kubernetes Cluster Federation 又名 KubeFed 或 Federtation v2，是 Kubernetes SIG Multi-Cluster 团队新提出的集群联邦架构。新架构在 Federation v1 基础之上，简化扩展 Federated API过程，并加强跨集群服务发现与编排的功能。 KubeFed 是Kubernetes官方多集群联邦解决方案。它允许用户使用在“Host cluster”中定义的一组简单的APIs，在多个Kubernetes集群中联合统一调度工作负载。 KubeFed 与v1版本使用一套独立的Federation APIs不同，它创建和扩展了一套自定义资源。 在 KubeFed 设计之初，有两个最重要的核心理念是其希望实现的，分别为 Modularization（模块化）和 Customizable（定制化）。这两个理念是希望 KubeFed 能够跟随着 Kubernetes 生态发展，并与之保持相容性和扩展性。 与 v1 版本相比，KubeFed 最大的改变是将 API Server 移除，并通过 CRD 机制来完成 Federated Resources 的扩充，KubeFed Controller 负责管理这些 CRD，并实现同步 Resources 、跨集群编排等功能。 概念及原理Host cluster用于提供 KubeFed API和控制平面的集群，控制平面中配置了 KubeFedConfig 和 KubeFedCluster 资源，这些资源中配置了该 Host cluster 管理的集群联邦中包含了哪些 member cluster。所有“联邦化”了的资源都会被部署到Host cluster中，并以相同的副本数复制部署到每一个 member cluster 中。控制平面的部署可以使用Helm chart。通过使用命令行工具kubefedctl，可以为联邦添加、删除集群，“联邦化”资源类型。根据 FeatrueGates 的具体配置，KubeFed 控制平面会相应启动多个 controller。 Host cluster 也可以作为 member cluster 加入联邦。 Member cluster集群联邦中的成员集群，用来部署“联邦化”的工作负载资源。 通过 KubeFed API 注册的集群，并提供相关身份凭证来让 KubeFed Controller 能够存取集群。 Template定义跨级群通用资源的描述信息（representation） Placement定义一个“联邦化”资源应该部署到哪些 member 集群中 Overrides定义Template中，单机群的（per-cluster）、字段级别的变量（filed-level） Federated Resources被“联邦化”的资源 Cluster Configuration用来定义哪些集群要被联邦。可以使用命令行工具 kubefedctl join/unjoin 来加入/删除集群。当集群成功加入联邦后，会建立一个 KubeFedCluster 组件来存储集群相关信息，如 API Endpoint、CA Bundle等。这些信息 KubeFed Controller 会用来管理 member 集群，以确保能够建立 Kubernetes API 资源 示意图如下所示： KubeFed API GroupKubeFed 通过 CRD 方式新增了4种 API 群组来实现联邦机制的核心功能： API Group 用途 core.kubefed.k8s.io 集群组态、联邦资源组态、KubeFed Controller 设定档等。 types.kubefed.k8s.io 被联邦的Kubernetes API 资源。 scheduling.kubefed.k8s.io 副本编排策略。 multiclusterdns.kubefed.k8s.io 跨集群服务发现设定。 Type Configuration用来定义哪些 Kubernetes API 资源要被联邦化。 举个例子，要将 ConfigMap 资源通过联邦机制建立在不同的集群上时，首先要在 Host 集群中通过 CRD 创建新资源 FederatedConfigMap，接着要创建名称为 configmaps 的 Type Configuration（FederatedTypeConfig）资源，描述 ConfigMap 要被 FederatedConfigMap 所管理。这样，KubeFed Controllers 才能知道如何创建 Federated ConfigMap 资源。 下面为范例：123456789101112131415161718apiVersion: core.kubefed.k8s.io/v1beta1kind: FederatedTypeConfigmetadata: name: configmaps namespace: kube-federation-systemspec: federatedType: group: types.kubefed.k8s.io kind: FederatedConfigMap pluralName: federatedconfigmaps scope: Namespaced version: v1beta1 propagation: Enabled targetType: kind: ConfigMap pluralName: configmaps scope: Namespaced version: v1 也可以使用命令行工具新增 CRD 资源，kubefedctl enable ，举个例子：1234567$ kubefedctl enable etcdclusters$ kubectl api-resources | grep etcdetcdclusters etcd etcd.database.coreos.com true EtcdClusterfederatedetcdclusters fetcd types.kubefed.k8s.io true FederatedEtcdCluster $ kubectl -n kube-federation-system get federatedtypeconfigs | grep etcdetcdclusters.etcd.database.coreos.com 3m16s 一个 federated 资源一般具备三个主要功能，这些功能信息能够在 spec 中由使用者自行定义，举个例子：123456789101112131415161718192021apiVersion: types.kubefed.k8s.io/v1beta1kind: FederatedDeploymentmetadata: name: test-deployment namespace: test-namespacespec: template: # 定义 Deployment 的所有內容，可理解成 Deployment 与 Pod 之间的关联。 metadata: labels: app: nginx spec: ... placement: clusters: - name: cluster2 - name: cluster1 overrides: - clusterName: cluster2 clusterOverrides: - path: spec.replicas value: 5 Template：定义 FederatedDeployment 的所有内容，类似普通 Deployment 的定义，比如 Deployment 与 Pod 之间的关联、副本数等等。 Placement：定义联邦化的 Deployment 资源发布到哪些集群中，如果没有定义该部分，则不会发布到任何集群。如果 placement 中定义了多个集群，这些集群中都会创建相同的 Deployment。另外也支持使用 spec.placement.clusterSelector 的方式来选择要放置的集群。 Override：用来修改制定集群中 Federated 资源的 spec.template 的内容。如例子 yaml 中使用 override 字段来修改 FederatedDeployment 在集群 cluster2 中的副本数为5。 SchedulingKubeFed 提供了一种自动化机制将工作负载实例分散的到不同的集群中，主要是基于资源的总副本数与集群的定义策略来将资源（Deployment或ReplicaSet）进行编排。 编排策略是通过建立 ReplicaSchedulingPreference（RSP）文件，再由 KubeFed RSP Controller 监听和获取 RSP 内容来将工作负载实例创建到指定的集群中。 举个 RSP 的例子，假设有三个集群被联邦管理，名称分别为 ap-northeast、us-east、us-west：12345678910111213141516apiVersion: scheduling.kubefed.k8s.io/v1alpha1kind: ReplicaSchedulingPreferencemetadata: name: test-deployment namespace: test-nsspec: targetKind: FederatedDeployment totalReplicas: 15 clusters: "*": weight: 2 maxReplicas: 12 ap-northeast: minReplicas: 1 maxReplicas: 3 weight: 1 上面的 yaml 创建后，RSP Controller 监听并获取到资源，并匹配对应 namespace/name 下的 FederatedDeployment 与 FederatedReplicaSet 是否存在。若存在，会根据设定的策略计算出每个集群预期的副本数，之后覆写 Federated 资源中 spec.overrides 的内容以修改每个集群的副本数。最后由 KubeFed Sync Controller 来同步到每个集群的 Deployment。 以上面为例，结果为 ap-northeast 集群会拥有3个 Pod，us-east 和 us-weat 会分别拥有6个 Pod。12345若spec.clusters未定义的话，则预设为{“*”:{Weight: 1}}。若有定义spec.replicas 的overrides 时，副本会以RSP 为优先考量。分配的计算机制可以参考kubefed/pkg/controller/util/planner/planner.go。 创建 Federated 资源PushReconciler，其作用是自动将“联邦化”资源的变化推送到由 Placement 选择的 member cluster 中。举个例子，如果一个“联邦化”的资源，其在 member cluster 中的实际状态跟联邦资源中的期望状态不同时，比如副本数量因某种原因被删除了一个，pushreconciler会再次同步 member cluster 中该资源的实际状态与期望状态一致。 使用上面提到的 FederatedTypeConfig、KubeFedCluster、KubeFedConfig 三种配置文件，可以将任何类型的 Kubernetes 资源“联邦化”，包括用户自定义资源。 当“联邦化”一种 Kubernetes 资源后，会生成一个 FederatedTypeConfig 配置，该配置说明了 KubeFed 如何处理该种类型的资源。同时还会生成一个CRD，它描述了 Kubernetes API Server应该如何处理这种“联邦化”的资源。 下面看一个“联邦化”的例子，FederatedDeployment：1234567891011121314151617181920apiVersion: types.kubefed.k8s.io/v1beta1kind: FederatedDeploymentmetadata: name: test-deployment namespace: testspec: template: placement: clusters: - name: cluster2 - name: cluster1 clusterSelector: matchLabels: region: eu-west-1 overrides: - clusterName: cluster2 clusterOverrides: - path: spec.parallelism value: 2 在 placement 段中，用户可以显示的以名称的方式制定 member cluster，也可以使用 clusterSelector 通过集群的 lable 指定。 在 overrides 段中，用户可以显示的使用 filed 变量名称，对于更复杂的描述信息可以使用json片段。 “联邦化”的资源是用 namespace 限定的，KubeFed可以配置为全局 scope 或者指定一个 namespace的。KubeFed 控制面会监听所配置的 namespace 中的联邦化资源。 如果设置的 scope 是 global，用户可以在任何 namespace 中创建联邦资源。 Multi-Cluster DNS（跨集群服务发现）这个 featrue 生效后，KubeFed 会 watch Service 和 Ingress 资源，将它们注册到外部 DNS服务器。 我们只要创建 Domain 资源，以及一个 ServiceDNSRecord（或者IngressDNSRecord，如果我们有 Ingress 资源的话）。 如下面yaml文件所示，创建 Domain 和 ServiceDNSRecord 资源，将 exampleDomain 和 exampleService 做关联：123456789apiVersion: multiclusterdns.kubefed.k8s.io/v1alpha1kind: Domainmetadata: # Corresponds to in the resource records. name: exampleDomain # The namespace running the KubeFed control plane. namespace: federation-system# The domain/subdomain that is set up in your external-dns provider.domain: example.beta.banzaicloud.io 1234567891011apiVersion: multiclusterdns.kubefed.k8s.io/v1alpha1kind: ServiceDNSRecordmetadata: # The name of the sample service. name: exampleService # The namespace of the sample deployment/service. namespace: testspec: # The name of the corresponding `Domain`. domainRef: exampleDomain recordTTL: 300 一旦我们创建以上两种资源，KubeFed 就会创建一个 DNSEndpoint 对象，该对象中存有所有 DNS 名称及其所有 targets 。 首先假设已建立一个名称为nginx的FederatedDeployment，然后放到development namespace中，并且也建立了对应的FederatedService提供LoadBalancer。这时当建立上述Domain与ServiceDNSRecord后，KubeFed的Service DNS Controller会依据ServiceDNSRecord文件内容，去收集不同集群的Service信息，并将这些信息更新至ServiceDNSRecord状态中，接着DNS Endpoint Controller会依据该ServiceDNSRecord的状态内容，建立一个DNSEndpoint文件，并产生DNS records资源，最后再由ExternalDNS Controller来同步更新DNS records至DNS供应商。下图是Service DNS建立的架构。 配置信息KubeFed v2配置信息有两种 FederatedTypeConfig 用来指定哪些API类型 KubeFed 需要处理。 KubeFedCluster 和 KubeFedConfig 都是集群的配置项，用来指定哪些集群加入了 KubeFed 联邦管理。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubefed环境搭建]]></title>
    <url>%2F2020%2F03%2F01%2Fkubefed%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[kubefed客户端安装下载地址：https://github.com/kubernetes-sigs/kubefed/releases/tag/v0.2.0-alpha.11234curl -LO https://github.com/kubernetes-sigs/kubefed/releases/download/v0.2.0-alpha.1/kubefedctl-0.2.0-alpha.1-linux-amd64.tgztar -zxvf kubefedctl-*.tgzchmod u+x kubefedctlsudo mv kubefedctl /usr/local/bin/ # make sure the location is in the PATH 使用helm安装kubefed控制面服务 下载并安装helm命令行 初始化： helm init –client-only因 https://kubernetes-charts.storage.googleapis.com 网络不通，需要手动创建$HOME/.helm/repository目录下把helm的repositories.yaml文件新建即可，参考wiki:Helm 基本使用 helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts helm repo list 能看到已添加的库 12345[root@vm192-168-0-203 cache]# helm repo listNAME URLstable https://kubernetes-charts.storage.googleapis.comhub https://hub.kce.ksyun.com/chartrepo/zhaoqikubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts helm search kubefed 查询kubefed chart包 12345[root@vm192-168-0-203 cache]# helm search kubefedWARNING: Repo "stable" is corrupt or missing. Try 'helm repo update'.NAME CHART VERSION APP VERSION DESCRIPTIONkubefed-charts/kubefed 0.2.0-alpha.1 KubeFed helm chartkubefed-charts/federation-v2 0.0.10 Kubernetes Federation V2 helm chart helm install kubefed-charts/kubefed –name kubefed –version 0.2.0-alpha.1 –namespace kube-federation-system123456789101112131415161718192021222324252627[root@vm192-168-0-203 cache]# kubectl get crd | grep kubefedclusterpropagatedversions.core.kubefed.io 2020-04-27T09:05:34Zdnsendpoints.multiclusterdns.kubefed.io 2020-04-27T09:05:34Zdomains.multiclusterdns.kubefed.io 2020-04-27T09:05:34Zfederatedclusterroles.types.kubefed.io 2020-04-27T09:05:34Zfederatedconfigmaps.types.kubefed.io 2020-04-27T09:05:34Zfederateddeployments.types.kubefed.io 2020-04-27T09:05:34Zfederatedingresses.types.kubefed.io 2020-04-27T09:05:34Zfederatedjobs.types.kubefed.io 2020-04-27T09:05:34Zfederatednamespaces.types.kubefed.io 2020-04-27T09:05:34Zfederatedreplicasets.types.kubefed.io 2020-04-27T09:05:34Zfederatedsecrets.types.kubefed.io 2020-04-27T09:05:34Zfederatedserviceaccounts.types.kubefed.io 2020-04-27T09:05:34Zfederatedservices.types.kubefed.io 2020-04-27T09:05:34Zfederatedservicestatuses.core.kubefed.io 2020-04-27T09:05:34Zfederatedtypeconfigs.core.kubefed.io 2020-04-27T09:05:34Zingressdnsrecords.multiclusterdns.kubefed.io 2020-04-27T09:05:34Zkubefedclusters.core.kubefed.io 2020-04-27T09:05:34Zkubefedconfigs.core.kubefed.io 2020-04-27T09:05:34Zpropagatedversions.core.kubefed.io 2020-04-27T09:05:34Zreplicaschedulingpreferences.scheduling.kubefed.io 2020-04-27T09:05:34Zservicednsrecords.multiclusterdns.kubefed.io 2020-04-27T09:05:34Z[root@vm192-168-0-203 cache]# kubectl get deploy -n kube-federation-systemNAME READY UP-TO-DATE AVAILABLE AGEkubefed-admission-webhook 1/1 1 1 69mkubefed-controller-manager 2/2 2 2 69m 将两个集群加入联邦123456[root@vm192-168-0-203 ~]# kubefedctl join --cluster-context context-1580a941-b081-4663-a998-4426ea561192 cluster1 --host-cluster-context context-1580a941-b081-4663-a998-4426ea561192[root@vm192-168-0-203 ~]# kubefedctl join --cluster-context context-1580a941-b081-4663-a998-4426ea561192 cluster2 --host-cluster-context context-1580a941-b081-4663-a998-4426ea561192[root@vm192-168-0-203 ~]# kubectl get kubefedcluster -n kube-federation-systemNAME AGE READYcluster1 4h Truecluster2 1s True 联邦化的资源类型12345678910111213[root@vm192-168-0-203 kubefed]# kubectl get FederatedTypeConfig -n kube-federation-systemNAME AGEclusterroles.rbac.authorization.k8s.io 1dconfigmaps 1dcustomresourcedefinitions.apiextensions.k8s.io 5hdeployments.apps 1dingresses.extensions 1djobs.batch 3hnamespaces 1dreplicasets.apps 1dsecrets 1dserviceaccounts 1dservices 1d 可以使用kubefedctl disable / enable 联邦化某种资源，包括crd资源类型12345678910[root@vm192-168-0-203 kubefed]# kubefedctl disable jobDisabled propagation for FederatedTypeConfig "kube-federation-system/jobs.batch"Verifying propagation controller is stopped for FederatedTypeConfig "kube-federation-system/jobs.batch"Propagation controller for FederatedTypeConfig "kube-federation-system/jobs.batch" is stoppedfederatedtypeconfig "kube-federation-system/jobs.batch" deleted [root@vm192-168-0-203 kubefed]# kubefedctl enable jobcustomresourcedefinition.apiextensions.k8s.io/federatedjobs.types.kubefed.io updatedfederatedtypeconfig.core.kubefed.io/jobs.batch created in namespace kube-federation-system]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[golang依赖管理]]></title>
    <url>%2F2020%2F02%2F26%2Fgolang%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景govendor 缺点govendor依赖管理太松散，同一个依赖项目，不同的组件引用，版本可以不一样，例如vendor.json： 123456789101112{ "checksumSHA1": "ehAUZgg3BT4gz3WA5B9l2o4NOHw=", "path": "k8s.io/apimachinery/pkg/api/equality", "revision": "035e418f1ad9b6da47c4e01906a0cfe32f4ee2e7", "revisionTime": "2019-07-31T12:28:47Z"},{ "checksumSHA1": "bmva3UAPnGM9sI9Ap5hXRhlH4wA=", "path": "k8s.io/apimachinery/pkg/api/errors", "revision": "1f8faeb8119141131b81637c896fc4c30e7075ae", "revisionTime": "2019-07-30T15:53:30Z"}, 这样，当有其他依赖如 k8s.io/client-go，也依赖 k8s.io/apimachinery 的不通版本时，会造成依赖冲突 go mod 优点 官方推出的依赖管理工具 可以摆脱GOPATH 可以通过代理下载墙外依赖 环境准备 golang 升级到 1.13 设置环境变量：12345GOPROXY=https://goproxy.cnGOPRIVATE=*.flftuu.comGONOPROXY=*.flftuu.comGONOSUMDB=*.flftuu.comGO111MODULE=auto 初始化项目 newgit上新建项目，例如：newgit.op.flftuu.com/my/my_project 本地新建项目根目录，并初始化 go mod 123$ mkdir my_project $ cd my_project$ go mod init newgit.op.flftuu.com/my/my_project 写完代码后，更新依赖并保存到vendor目录12$ go mod tidy$ go mod vendor git 提交代码 从 govendor 改造 下载项目，并删除vendor目录 初始化 go mod123$ cd my_project$ go mod init newgit.op.flftuu.com/my/my_project$ go mod tidy 添加依赖12$ go get k8s.io/api@kubernetes-1.14.0$ go get -v -insecure newgit.op.flftuu.com/kce/appclient@develop #添加非 https 服务的依赖，需要将 git 账号密码保存到 windows ，否则拉取会失败 保存依赖到vendor1$ go mod vendor 注意事项 使用go mod管理的依赖，执行 go get 时需要指定版本，例如：123以分支为版本：go get k8s.io/api@master以 tag 为版本：go get k8s.io/api@kubernetes-1.14.0以 commit 为版本: go get k8s.io/api@40a48860b5abbba9aa891b02b32da429b08d96a0 由于 client-go 对 kubernetes 项目兼容性不足，需要使用client-go对应的kubernetes相关依赖版本，详见 https://github.com/kubernetes/client-go#compatibility-matrix 本次改动，基于 kubernetes 的版本全部为 1.14 ，例如：12345678k8s.io/apimachinery@kubernetes-1.14.0k8s.io/client-go@v11.0.0 k8s.io/api@kubernetes-1.14.0k8s.io/apiextensions-apiserver@kubernetes-1.14.0k8s.io/apiserver@kubernetes-1.14.0k8s.io/component-base@kubernetes-1.14.0k8s.io/apiextensions-apiserver@v0.0.0-20190315093550-53c4693659edk8s.io/api]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客同时部署到github和VPS上]]></title>
    <url>%2F2020%2F02%2F18%2Fhexo%E5%8D%9A%E5%AE%A2%E5%90%8C%E6%97%B6%E9%83%A8%E7%BD%B2%E5%88%B0github%E5%92%8CVPS%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[前言有一台VPS一直空闲，未免有点浪费，所以想把博客部署到VPS上，并且绑定域名 之前尝试过这么做过，但是一直都没有成功，因为这其中有很多细节都是需要注意的，所以还是写一篇博客来记录这次的部署过程 以后换VPS的时候就不用像这次一样到处查找资料了 原理部署到VPS的原理即是在VPS上搭建git服务器，然后每次提交public文件夹中的文件到VPS时，通过git-hooks钩子来同时复制文件到网站的根目录上 其实关键的是git服务器的搭建，因为想要免密码ssh登录的话需要配置一系列东西 部署到github就不说了，因为这个非常好部署，github其实也是一个git服务器，但是其已经搭建好了，我们只需要将公钥交给github即可完成免密码登录 git服务器的搭建安装git并配置用户与邮箱首先创建好git用户并且输入密码 1$ useradd git 随后就是配置git的用户名和邮箱12$ git config --global user.name "username" $ git config --global user.email "mail@gmail.com" 配置完成后生成SSH密钥1$ ssh-keygen -t rsa -C "mail@gmail.com" 我们选择保存在 /home/git/.ssh/id_rsa中，以后这个文件夹还是有用的 添加公钥新建一个名为authorized_keys的文件，并将公钥复制进去 1$ vim .ssh/authorized_keys 随后打开RSA认证 1$ vim /etc/ssh/sshd_config 找到下面这一行，修改为 AuthorizedKeysFile .ssh/authorized_keys 这样公钥就已经配置好了 修改权限修改权限这一步是非常重要的，我之前的原因应该就是卡在这一步，所以无法达到免密码ssh登录 1234$ chmod 700 .ssh $ chmod 600 .ssh/authorized_keys$ cd /home$ chown -R git:git git 关闭git的shell登录为了安全起见，我们拒绝git的shell权限 修改下面文件内容 1234567$ vim /etc/passwd将git:x:1001:1001:,,,:/home/git:/bin/bash改成git:x:1001:1001:,,,:/home/git:/usr/bin/git-shell Nginx的配置与Githooks的脚本实现Nginx配置文件修改配置文件前先备份 123$ cd /etc/nginx/sites-available $ cp default default.bak $ vim default 然后在文件中输入下面配置内容123456789101112131415161718192021server { listen 80 default; root /var/www/blog; index index.html index.htm index.nginx-debian.html; server_name vhyz.me www.vhyz.me; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } location ~* ^.+\.(css|js|txt|xml|swf|wav)$ { root /var/www/blog; access_log off; expires 10m; } location ~* ^.+\.(ico|gif|jpg|jpeg|png)$ { root /var/www/blog; access_log off; expires 1d; }} 修改完保存，重启nginx 1$ sudo service nginx restart 新建blog.git新建blog.git裸仓库与网站根目录1234$ cd /home/git$ git init --bare blog.git$ cd /var/www$ mkdir blog 然后修改用户组权限12$ chown git:git -R /var/www/blog$ chown git:git -R /home/git/blog.git 记住每个仓库都需要这样设置权限 配置Git Hooks新建post-receive文件12345678910111213$ cd /home/git/blog.git/hooks $ vim post-receive ``` 然后输入下面脚本内容```bash#!/bin/bash GIT_REPO=/home/git/blog.git TMP_GIT_CLONE=/tmp/blog PUBLIC_WWW=/var/www/blog rm -rf ${TMP_GIT_CLONE} git clone $GIT_REPO $TMP_GIT_CLONE rm -rf ${PUBLIC_WWW}/* cp -rf ${TMP_GIT_CLONE}/* ${PUBLIC_WWW} 保存完之后赋予可执行权限1$ chmod +x post-receive hexo本地配置在站点配置文件中deploy修改为下面内容,your_ip代表了服务器的地址 123456789deploy:- type: git repo: https://github.com/vhyz/vhyz.github.io branch: master message: - type: git repo: git@your_ip:blog.git branch: master message: 如果服务器端口不是默认22，则需要在本地的.ssh文件夹中创建config配置文件 输入下面内容12345Host HostName User gitPort IdentityFile ~/.ssh/id_rsa Host与HostName均为你的服务器IP地址 然后输入1hexo g -d 即可完成一次对两个服务器的部署 转载自]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[coredns解析集群node节点]]></title>
    <url>%2F2020%2F02%2F02%2Fcoredns%E8%A7%A3%E6%9E%90%E9%9B%86%E7%BE%A4node%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[k8s_nodeNamek8s_node - resolves node hostname and node IPs from Kubernetes clusters. DescriptionThis plugin resolves node external IP and internal IP address(es) of Kubernetes clusters.This plugin is only useful if the kubernetes plugin is also loaded. The plugin resolve node IP addresses. It only handles queries for A and AAAA records;all others result in NODATA responses. Syntax1k8s_node If you want to change the apex domain or use a different TTL for the returned records you can usethis extended syntax. 123k8s_node { ttl TTL} ttl allows you to set a custom TTL for responses. The default is 5 (seconds). Examples1234. { kubernetes cluster.local k8s_node} With the Corefile above, the following Service will get an A record for vm10-0-10-80.ksc.com with the IP address 10.0.10.80. 123456status: addresses: - address: 10.0.10.80 type: InternalIP - address: vm10-0-10-80.ksc.com type: Hostname BugsPTR queries for the reverse zone is not supported. Also Seesource codedocker image]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubemark测试k8s集群]]></title>
    <url>%2F2020%2F01%2F07%2Fkubemark%E6%B5%8B%E8%AF%95k8s%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[搭建 kubemark 流程由于官方搭建的 kubemark 集群依赖比较多且不是特别符合我们的场景，所以以下针对金上云的集群环境搭建测试集群。 先使用金山云平台搭建两个真实的集群。集群规格是 3 master(2c4g), 2 node(32c64g)，一个作为 kubemark 集群， 另一个用来部署 hollow-node pod，称为 support 集群。 配置本地 kubectl 连接到support集群。 在support集群创建如下资源 集群配置测试集群： master个数： 3 master规格： 1c2g ~ 2c4g node： 使用kubemark模拟 一共测试了以下10个case： 50 node 100 node 150 node 200 node 250 node 300 node 350 node 400 node 450 node 500 node]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[etcd性能测试]]></title>
    <url>%2F2019%2F12%2F27%2Fetcd%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[理解性能决定 etcd 性能的关键因素，包括： 延迟(latency)：延迟是完成操作的时间。 吞吐量(throughput)：吞吐量是在某个时间期间之内完成操作的总数量。 当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。 etcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。 一致性性能，特别是提交延迟，受限于两个物理约束： 网络IO延迟 磁盘IO延迟 完成一个 etcd 请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。 在一个数据中心内的 RTT 可能有数百毫秒。 机械硬盘的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms。 为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。 这个批量策略让 etcd 在重负载试获得高吞吐量。 有其他子系统影响到 etcd 的整体性能。 每个序列化的 etcd 请求必须通过 etcd 的 boltdb支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。 etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。 虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。 进行中的压缩可以影响 etcd 的性能。 幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。 RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API。 但是它也引入了额外的延迟，尤其是本地读取。 评测性能可以通过 etcd 自带的 benchmark CLI 工具来评测 etcd 的性能。 对于某些基线性能数字，考虑搭建3节点 etcd 集群，带有下列硬件配置： 搭建 etcd 集群：3 副本etcd pod， 1 vCPUs + 2GB Memory + ssd磁盘（每个机器都有不少进程在上面） 评测 etcd 节点：1 台机器(客户端)，16 vCPUs + 32GB Memory + 普通磁盘，用于做benchmark 操作系统：Centos 7.5 容器镜像：quay.io/coreos/etcd:3.3.10 对应的pod ip列表如下： 机器名 IP pod 1 10.245.29.16 pod 2 10.245.21.35 pod 3 10.245.17.55 CLIENT 10.246.12.0/24 使用这些配置，采样命令如下: 123456789101112HOST_1=http://10.245.29.16:2379 HOST_2=http://10.245.21.35:2379 HOST_3=http://10.245.17.55:2379# 假定 HOST_1 是 leader, 写入请求发到 leader，连接数1，客户端数1benchmark --endpoints=${HOST_1} --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256# 假定 HOST_1 是 leader, 写入请求发到 leader，连接数100，客户端数1000benchmark --endpoints=${HOST_1} --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256# 写入发到所有成员，连接数100，客户端数1000benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256 etcd 近似的写入指标如下： key的数量 Key的大小 Value的大小 连接数量 客户端数量 目标 etcd 服务器 平均写入 QPS 每请求平均延迟 内存 10,000 8 256 1 1 只有主 126 7.7ms 100,000 8 256 100 1000 只有主 16,033 60ms 100,000 8 256 100 1000 所有成员 15,102 63ms 注：key和value的大小单位是 字节 / bytes 为了一致性，线性化(Linearizable)读取请求要通过集群成员的法定人数来获取最新的数据。串行化(Serializable)读取请求比线性化读取要廉价一些，因为他们是通过任意单台 etcd 服务器来提供服务，而不是成员的法定人数，代价是可能提供过期数据。 采样命令如下: 123456789101112131415HOST_1=http://10.245.29.16:2379 HOST_2=http://10.245.21.35:2379 HOST_3=http://10.245.17.55:2379# Linearizable 读取请求benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=1 --clients=1 range foo --consistency=l --total=10000benchmark --endpoints=${HOST_1},${HOST_2},${HOST_3} --conns=100 --clients=1000 range foo --consistency=l --total=100000# Serializable 读取请求，使用每个成员然后将数字加起来for endpoint in ${HOST_1} ${HOST_2} ${HOST_3}; do benchmark --endpoints=$endpoint --conns=1 --clients=1 range foo --consistency=s --total=10000done for endpoint in ${HOST_1} ${HOST_2} ${HOST_3}; do benchmark --endpoints=$endpoint --conns=100 --clients=1000 range foo --consistency=s --total=100000done etcd 近似读取指标如下： 请求数量 Key 大小 Value 大小 连接数量 客户端数量 一致性 每请求平均延迟 平均读取 QPS 10,000 8 256 1 1 线性化 1.2ms 824 10,000 8 256 1 1 串行化 0.6ms 1,628 100,000 8 256 100 1000 线性化 23.8ms 37,679 100,000 8 256 100 1000 串行化 43.5ms 22,458 分析读取指标的时候，按理说串行化要比线性化要好 影响性能的因素：磁盘（是否SSD），CPU（很多其他应用），内存（这个相对比较充足）]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim使用方式]]></title>
    <url>%2F2019%2F12%2F26%2Fvim%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[vim1234vim python 语法的检测vim 多功能设置git 工具的简单 别名配置ssh config file 配置 Usage1234mv vim ~/.vimmv vimrc ~/.vimrcmv gitconfig ~/.gitconfigmv ssh ~/.ssh input < ; + m > input < ; + l >]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod访问svc不通问题]]></title>
    <url>%2F2019%2F12%2F16%2Fpod%E8%AE%BF%E9%97%AEsvc%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[集群环境k8s: v.1.15.3cni: flannelproxy: ipvs 网络访问情景pod 访问自身svcpod A -> svc -> pod A 原因: 开启 网桥”hairpinMode” 默认 解决：cni config文件中配置 1234567891011121314cni-conf.json: | { "name":"cni0", "cniVersion":"0.3.1", "plugins":[ { "type":"flannel", "delegate":{ "forceAddress":true, "isDefaultGateway":true, "hairpinMode":true } }, { pod 访问通node节点上pod的svcpod A -> svc -> pod B (pod A 和 pod B 在同一node节点上同一cni0 网桥内) 原因： net.bridge.bridge-nf-call-arptables net.bridge.bridge-nf-call-ip6tables 没开启 解决：sysctl -w net.bridge.bridge-nf-call-arptables = 1 sysctl -w net.bridge.bridge-nf-call-ip6tables = 1 pod 访问https的svc(LoadBalancer 类型)pod A(https) -> lb(公网ip) -> pod B 原因： prxoy 给iptables 添加转发规则，导致集群内部访问svc的公网ip会直接转给相应的pod ip 解决：把tls证书放在ingress 上。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 实战-Operator Finalizers 实现]]></title>
    <url>%2F2019%2F12%2F13%2FKubernetes-%E5%AE%9E%E6%88%98-Operator-Finalizers-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[背景最近在写 k8s Operator，在看示例的时候看到 controller 都会设置 Finalizers，今天来聊一聊 Finalizers 和相关实现。 FinalizersFinalizers 允许 Operator 控制器实现异步的 pre-delete hook。比如你给 API 类型中的每个对象都创建了对应的外部资源，你希望在 k8s 删除对应资源时同时删除关联的外部资源，那么可以通过 Finalizers 来实现。 Finalizers 是由字符串组成的列表，当 Finalizers 字段存在时，相关资源不允许被强制删除。存在 Finalizers 字段的的资源对象接收的第一个删除请求设置 metadata.deletionTimestamp 字段的值， 但不删除具体资源，在该字段设置后， finalizer 列表中的对象只能被删除，不能做其他操作。 当 metadata.deletionTimestamp 字段非空时，controller watch 对象并执行对应 finalizers 的动作，当所有动作执行完后，需要清空 finalizers ，之后 k8s 会删除真正想要删除的资源。 Operator finalizers 使用介绍了 Finalizers 概念，那么我们来看看在 Operator 中如何使用，在 Operator Controller 中，最重要的逻辑就是 Reconcile 方法，finalizers 也是在 Reconcile 中实现的。要注意的是，设置了 Finalizers 会导致 k8s 的 delete 动作转为设置 metadata.deletionTimestamp 字段，如果你通过 kubectl get 命令看到资源存在这个字段，则表示资源正在删除（deleting）。 有以下几点需要理解： 1. 如果资源对象未被删除且未设置 finalizers，则添加 finalizer并更新 k8s 资源对象； 2. 如果正在删除资源对象并且 finalizers 仍然存在于 finalizers 列表中，则执行 pre-delete hook并删除 finalizers ，更新资源对象； 3. 由于以上两点，需要确保 pre-delete hook是幂等的。 kuberbuilder 示例我们来看一个 kubebuilder 官方示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667func (r *CronJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { ctx := context.Background() log := r.Log.WithValues("cronjob", req.NamespacedName) var cronJob batch.CronJob if err := r.Get(ctx, req.NamespacedName, &cronJob); err != nil { log.Error(err, "unable to fetch CronJob") return ctrl.Result{}, ignoreNotFound(err) } // 声明 finalizer 字段，类型为字符串 myFinalizerName := "storage.finalizers.tutorial.kubebuilder.io" // 通过检查 DeletionTimestamp 字段是否为0 判断资源是否被删除 if cronJob.ObjectMeta.DeletionTimestamp.IsZero() { // 如果为0 ，则资源未被删除，我们需要检测是否存在 finalizer，如果不存在，则添加，并更新到资源对象中 if !containsString(cronJob.ObjectMeta.Finalizers, myFinalizerName) { cronJob.ObjectMeta.Finalizers = append(cronJob.ObjectMeta.Finalizers, myFinalizerName) if err := r.Update(context.Background(), cronJob); err != nil { return ctrl.Result{}, err } } } else { // 如果不为 0 ，则对象处于删除中 if containsString(cronJob.ObjectMeta.Finalizers, myFinalizerName) { // 如果存在 finalizer 且与上述声明的 finalizer 匹配，那么执行对应 hook 逻辑 if err := r.deleteExternalResources(cronJob); err != nil { // 如果删除失败，则直接返回对应 err，controller 会自动执行重试逻辑 return ctrl.Result{}, err } // 如果对应 hook 执行成功，那么清空 finalizers， k8s 删除对应资源 cronJob.ObjectMeta.Finalizers = removeString(cronJob.ObjectMeta.Finalizers, myFinalizerName) if err := r.Update(context.Background(), cronJob); err != nil { return ctrl.Result{}, err } } return ctrl.Result{}, err }}func (r *Reconciler) deleteExternalResources(cronJob *batch.CronJob) error { // // 删除 crobJob关联的外部资源逻辑 // // 需要确保实现是幂等的}func containsString(slice []string, s string) bool { for _, item := range slice { if item == s { return true } } return false}func removeString(slice []string, s string) (result []string) { for _, item := range slice { if item == s { continue } result = append(result, item) } return} 总结在开发 Operator 时，pre-delete hook 是一个很常见的需求，目前只发现了 Finalizers 适合实现这个功能，需要好好掌握。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s容器固定ip]]></title>
    <url>%2F2019%2F11%2F06%2Fk8s%E5%AE%B9%E5%99%A8%E5%9B%BA%E5%AE%9Aip%2F</url>
    <content type="text"><![CDATA[需求kubernetes集群支持容器IP不变（具体来讲，是通过Statefulset创建出来的pod，保持IP不变），固定IP的使用场景是这样的：在开发测试环境中，给开发人员分配一个容器，开发人员通过容器的IP ssh登录进去，进行开发调试。由于这个环境开发人员会长期使用，因此希望容器在发生漂移、重启等变动时，仍然能通过原来的IP登录上去。 方案IP分配的功能是CNI组件实现的，如flannel作为网络模型，其容器IP分配是和node绑定的（每个node分配的IP都是一个固定网段的）。当容器从一个节点漂移另一个节点后，无法保持原来的IP。 考虑基于calico网络模型来实现。关于calico的介绍及实现参考： 容器网络-从CNI到Calico calico代码解读 calico支持两种类型的ipam，host-local和calico-ipam。host-local的ip分配方式，也是和node节点绑定的。calico-ipam，可以在用户指定的整个容器地址网段内分配IP地址。 分析calico-ipam的代码，发现其结构清晰，易于改造。实现固定IP的思路如下： 分配IP时，检查当前pod是不是statefulset创建出来的。如果是，检查configmap中有没有保存过pod名字和ip的对应关系，如果有，跳转到指定IP分配逻辑。如果没有，转入calico-ipam的自动IP分配逻辑，并在结束时将pod名字和ip的对应关系，记录到configmap中。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s集群 iptables vs ipvs性能测试]]></title>
    <url>%2F2019%2F11%2F01%2Fk8s%E9%9B%86%E7%BE%A4-iptables-vs-ipvs%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[对比 iptables 和 ipvs 的性能差异测试环境规格： master：32C64G node: 500个, 规格：2C4G iptables vs ipvs 规则增加延迟 service 数 1 100 500 1000 1500 2000 5000 10000 15000 20000 rule 数 4 400 2000 4000 6000 8000 10000 40000 60000 80000 增加一条 iptables 规则 77 ms 100 ms 491 ms 1302 ms 2476 ms 2662 ms 6547 ms 9491 ms 15125 ms 20000 ms 增加一条 ipvs 规则 0.8 ms 0.85 ms 0.85 ms 0.85 ms 0.90 ms 0.90 ms 0.90 ms 0.97 ms 1 ms 1.52 ms 上图可知：iptables 模式下，随着 服务 规则的增加，增加一条 iptables 规则的时间快速增大。ipvs 模式下，随着 服务 规则的增加，增加一条 ipvs 规则 的时间基本维持在 1 ms 左右。 iptables vs ipvs 服务请求延迟在一定 服务 基数下，对规则链最后的服务 进行 100 并发，1000次请求的 压力测试，得出如下 分布在 iptables 和ipvs 模式下的服务响应延迟。 上图可知：iptables 模式下，随着 服务 规则的增加，服务请求 延迟 增加明显ipvs 模式下，随着 服务 规则的增加，服务请求 延迟 相对平稳维持在 10 ms左右 iptables vs ipvs cpu 消耗 上图可知：iptables 模式下对 node 节点的资源消耗明显大于 ipvs 模式 Iptables 存在的问题: 规则顺序匹配延迟大 访问 service 时需要遍历每条链知道匹配，时间复杂度 O(N)，当规则数增加时，匹配时间也增加。 规则更新延迟大 iptables 规则更新不是增量式的，每更新一条规则，都会把全部规则加载刷新一遍。 规则数大时，会出现 kernel lock svc 数增加到 5000 时，会频繁出现 Another app is currently holding the xtables lock. Stopped waiting after 5s， 导致规则更新延迟变大，kube-proxy 定期同步时也会因为 超时导致 CrashLoopBackOff。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx+geoip2+docker实现禁止某个地区或国家访问]]></title>
    <url>%2F2019%2F10%2F28%2Fnginx-geoip2-docker%E5%AE%9E%E7%8E%B0%E7%A6%81%E6%AD%A2%E6%9F%90%E4%B8%AA%E5%9C%B0%E5%8C%BA%E6%88%96%E5%9B%BD%E5%AE%B6%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[nginx 部署网站禁止访问方式安装docker 参考 安装nginx-geoip2 服务 1docker run -d --name nginx flftuu/nginx-geoip2:1.15.12 geoip2 配置禁止访问1234567891011121314151617181920212223242526272829cat nginx/default.confserver { listen 443 ssl http2; listen [::]:443 ssl http2; add_header Strict-Transport-Security "max-age=31536000" always; ssl_session_cache shared:SSL:20m; ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers "ECDH+AESGCM:ECDH+AES256:ECDH+AES128:!ADH:!AECDH:!MD5;"; ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4; root /var/www/html; index index.php; if ( $geoip2_data_country_code = CN ) { return 403; } if ( $geoip2_data_city_name = Zhengzhou ) { return 403; } geoip2_data_country_code 设置国家代码 geoip2_data_city_name 设置城市代码 geoip2更多配置参考]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php-fpm优化]]></title>
    <url>%2F2019%2F10%2F11%2Fphp-fpm%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[分析判断php-fpm内存占用情况使用Glances命令，再按下m，就可以查看到当前VPS主机进程内存占用情况了，按照占用内存由多到少排序（或者使用Top命令，按下M，效果是一样的）。如下图（点击放大）： 这是一张重启后进程内存占用情况图，从前后对比中可以发现：随着开机时间的增长，php-fpm占用的内存越来越大，最终php-fpm耗尽了VPS所有物理内存。 查看当前php-fpm总进程数，命令：ps -ylC php-fpm –sort:rss。其中RSS就是占用的内存情况。如下图： 查看当前php-fpm进程的内存占用情况及启动时间，命令如下： 1ps -e -o 'pid,comm,args,pcpu,rsz,vsz,stime,user,uid'|grep www|sort -nrk5 从下图可以看出当前php-fpm所有进程平均每个进程占用了60-70MB的内存，启动时间，是当天的话就是3：12，否则会显示是X月X日。 查看当前php-fpm进程平均占用内存情况，一般来说一个php-fpm进程占用的内存为30-40MB，本次查询的结果是60MB，显然是多了。命令如下： 12ps --no-headers -o "rss,cmd" -C php-fpm | awk '{ sum+=$1 } END { printf ("%d%s\n", sum/NR/1024,"M") }'结果61M 熟悉php-fpm配置文件说明php-fpm.conf就是php-fpm的配置文件，路径一般在：/usr/local/php/etc，如下图： php-fpm.conf几个重要的参数说明如下： 12345678pm = dynamic #指定进程管理方式，有3种可供选择：static、dynamic和ondemand。pm.max_children = 16 #static模式下创建的子进程数或dynamic模式下同一时刻允许最大的php-fpm子进程数量。pm.start_servers = 10 #动态方式下的起始php-fpm进程数量。pm.min_spare_servers = 8 #动态方式下服务器空闲时最小php-fpm进程数量。pm.max_spare_servers = 16 #动态方式下服务器空闲时最大php-fpm进程数量。pm.max_requests = 2000 #php-fpm子进程能处理的最大请求数。pm.process_idle_timeout = 10srequest_terminate_timeout = 120 pm三种进程管理模式说明如下： 12345pm = static，始终保持一个固定数量的子进程，这个数由pm.max_children定义，这种方式很不灵活，也通常不是默认的。pm = dynamic，启动时会产生固定数量的子进程（由pm.start_servers控制）可以理解成最小子进程数，而最大子进程数则由pm.max_children去控制，子进程数会在最大和最小数范围中变化。闲置的子进程数还可以由另2个配置控制，分别是pm.min_spare_servers和pm.max_spare_servers。如果闲置的子进程超出了pm.max_spare_servers，则会被杀掉。小于pm.min_spare_servers则会启动进程（注意，pm.max_spare_servers应小于pm.max_children）。pm = ondemand，这种模式和pm = dynamic相反，把内存放在第一位，每个闲置进程在持续闲置了pm.process_idle_timeout秒后就会被杀掉，如果服务器长时间没有请求，就只会有一个php-fpm主进程。弊端是遇到高峰期或者如果pm.process_idle_timeout的值太短的话，容易出现504 Gateway Time-out错误，因此pm = dynamic和pm = ondemand谁更适合视实际情况而定。 解决php-fpm进程占用内存大问题调整管理模式static管理模式适合比较大内存的服务器，而dynamic则适合小内存的服务器，你可以设置一个pm.min_spare_servers和pm.max_spare_servers合理范围，这样进程数会不断变动。ondemand模式则更加适合微小内存，例如512MB或者256MB内存，以及对可用性要求不高的环境。 减少php-fpm进程数如果你的VPS主机的内存被占用耗尽，可以检查一下你的php-fpm进程数，按照php-fpm进程数=内存/2/30来计算，1GB内存适合的php-fpm进程数为10-20之间，具体还得根据你的PHP加载的附加组件有关系。 php-fpm配置示例这里以1GB内存的VPS配置php-fpm为演示，实际操作来看设置数值还得根据服务器本身的性能、PHP等综合考虑。 12345pm = dynamic #dynamic和ondemand适合小内存。pm.max_children = 15 #static模式下生效，dynamic不生效。pm.start_servers = 8 #dynamic模式下开机的进程数量。pm.min_spare_servers = 6 #dynamic模式下最小php-fpm进程数量。pm.max_spare_servers = 15 #dynamic模式下最大php-fpm进程数量。 解决php-fpm进程不释放内存问题上面通过减少php-fpm进程总数来达到减少php-fpm内存占用的问题，实际使用过程中发现php-fpm进程还存长期占用内存而不释放的问题。解决的方法就是减少pm.max_requests数。 最大请求数max_requests，即当一个 PHP-CGI 进程处理的请求数累积到 max_requests 个后，自动重启该进程，这样达到了释放内存的目的了。以1GB内存的VPS主机设置为例（如果你设置的数值没有达到释放内存可以继续调低）： 1pm.max_requests = 500 当php-fpm进程达到了pm.max_requests设定的数值后，就会重启该进程，从而释放内存。下图是我测试后的效果，可以看出php-fpm进程被强制结束并释放了内存。 总结对于大内存以及对并发和可用性要求的话，建议使用static管理模式+最大的pm.max_children。如果是小内存的服务器，建议使用dynamic或者ondemand模式，同时降低pm.start_servers和pm.max_spare_servers进程数。 为什么我调整了参数没有达到应有的效果？根据wzfou.com的经验，php-fpm配置文件参数不能一概而论，必须要结合服务器自身的性能、WEB动态内容以及对可用性的要求来进行调整，内存长期占用最好是再检查一下是否有内存泄露。 2019年10月9日更新，如果你的php-fpm参数调整得过小，有可能出现502错误，解决办法：解决WordPress后台编辑保存菜单出现502错误。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coredns]]></title>
    <url>%2F2019%2F10%2F10%2Fcoredns%2F</url>
    <content type="text"><![CDATA[coredns 启动失败问题coredns 在ubuntu 下启动报错 1plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone ".", see https://coredns.io/plugins/loop#troubleshooting. Query: "HINFO 4547991504243258144.3688648895315093531." 原因： ubuntu系统下的/etc/resolv.conf 中的地址是回环地址127.0.0.53 而在coredns pod的网络和宿主机的网络是不同的。 在ubuntu 宿主机上通过回环地址127.0.0.53 可以指向真实的dns地址，而在coredns pod中在指向的pod 本地地址，无法获取真实的dns地址 解决方法： Add the following to your kubelet config yaml: resolvConf: (or via command line flag –resolv-conf deprecated in 1.10). Your “real” resolv.conf is the one that contains the actual IPs of your upstream servers, and no local/loopback address. This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the “real” resolv.conf, although this can be different depending on your distribution. 启动kubelet 时候加入 –resolv-conf 参数 根据不同的操作系统指定真实的dns 值 ubuntu： –resolv-conf=/run/systemd/resolve/resolv.confcentos： –resolv-conf=/etc/resolv.conf]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>dns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[harbor 能力测试]]></title>
    <url>%2F2019%2F10%2F09%2Fharbor-%E8%83%BD%E5%8A%9B%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[测试环境测试机型： harbor机型：I2 IO优化型， 4C8G测试镜像大小： 1.7G测试工具：harbor perftest 测试集群： 准备k8s集群，通过 k8s node 节点拉取harbor镜像, 准备测试镜像： 123docker pull garima0079/bigimagedocker tag garima0079/bigimage {harbor LB地址}/bigimagedocker push {harbor LB地址}/bigimage 测试工具准备: 123git clone git@github.com:seamounts/perftest.git -d $GOPATH/srccd $GOPATH/src/perftestgo build 准备测试集群node docker daemon 配置: 批量更新 k8s node docker daemon，将 harbor lb地址加入到 insecure-registries 列表 1$GOPATH/src/perftest/cmd/harbor/script/prepare.sh 清除镜像缓存: 批量清除 k8s node 节点镜像缓存 1$GOPATH/src/perftest/cmd/harbor/script/clear-image-cache.sh {harbor LB地址}/bigimage 测试指标镜像无缓存并发拉取大镜像的平均耗时，99值耗时 harbor 实例数和拉取时间的关系 harbor 支持的最大并发 无缓存拉取大镜像测试2 HA harbor测试测试环境：harbor 2个实例，主机I2 IO优化型，4C8G，内网LB，内网KS3 测试数据：执行： $GOPATH/src//perftest harbor pullimage –image 10.21.1.4/hsxue/bigimage –kubeconfig /root/.kube/config, 在集群所有节点并发拉取镜像。 50 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 73.3745 secs. Slowest: 73.3734 secs. Fastest: 0.8629 secs. Average: 49.8971 secs. Response time histogram: 0.8629 [1] |∎ 8.1140 [0] | 15.3650 [0] | 22.6161 [0] | 29.8671 [0] | 37.1182 [0] | 44.3692 [0] | 51.6203 [31] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 58.8713 [17] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 66.1224 [0] | 73.3734 [1] |∎ Latency distribution: 99% in 57.8476 secs. 100 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 127.8106 secs. Slowest: 127.8100 secs. Fastest: 46.4622 secs. Average: 70.0965 secs. Response time histogram: 46.4622 [1] |∎∎ 54.5969 [18] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 62.7317 [18] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 70.8665 [19] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 79.0013 [17] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 87.1361 [12] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 95.2709 [8] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 103.4056 [2] |∎∎∎∎ 111.5404 [3] |∎∎∎∎∎∎ 119.6752 [1] |∎∎ 127.8100 [1] |∎∎ Latency distribution: 99% in 127.8100 secs. 200 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 180.1791 secs. Slowest: 180.1777 secs. Fastest: 53.9234 secs. Average: 112.8468 secs. Response time histogram: 53.9234 [1] |∎ 66.5489 [10] |∎∎∎∎∎∎∎∎∎∎∎∎ 79.1743 [16] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 91.7997 [28] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 104.4252 [30] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 117.0506 [32] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 129.6760 [23] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 142.3015 [21] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 154.9269 [20] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 167.5523 [15] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 180.1777 [4] |∎∎∎∎∎ Latency distribution: 99% in 173.5462 secs. 300 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 246.9771 secs. Slowest: 246.9760 secs. Fastest: 53.7463 secs. Average: 152.4312 secs. Response time histogram: 53.7463 [1] | 73.0693 [10] |∎∎∎∎∎∎∎∎ 92.3923 [22] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 111.7152 [37] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 131.0382 [34] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 150.3612 [34] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 169.6841 [50] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 189.0071 [34] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 208.3301 [42] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 227.6530 [21] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 246.9760 [15] |∎∎∎∎∎∎∎∎∎∎∎∎ Latency distribution: 99% in 237.6006 secs. 400 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 297.7355 secs. Slowest: 297.7326 secs. Fastest: 60.6159 secs. Average: 187.1423 secs. Response time histogram: 60.6159 [1] | 84.3275 [7] |∎∎∎∎∎ 108.0392 [37] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 131.7509 [41] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 155.4626 [40] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 179.1742 [53] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 202.8859 [54] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 226.5976 [53] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 250.3092 [45] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 274.0209 [36] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 297.7326 [31] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ Latency distribution: 99% in 295.2425 secs. 4 HA harbor测试测试环境：harbor 4个实例，主机I2 IO优化型，4C8G，内网LB，内网KS3 测试数据：执行： $GOPATH/src//perftest harbor pullimage –image 10.21.1.4/hsxue/bigimage –kubeconfig /root/.kube/config, 在集群所有节点并发拉取镜像。 50 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 68.5314 secs. Slowest: 68.5308 secs. Fastest: 41.2531 secs. Average: 45.7705 secs. Response time histogram: 41.2531 [1] |∎ 43.9809 [27] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 46.7086 [11] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 49.4364 [4] |∎∎∎∎∎ 52.1642 [2] |∎∎ 54.8919 [1] |∎ 57.6197 [1] |∎ 60.3475 [0] | 63.0752 [0] | 65.8030 [1] |∎ 68.5308 [1] |∎ Latency distribution: 99% in 64.3012 secs. 100 node 并发拉取测试 123456789101112131415161718192021Summary: Total: 115.8375 secs. Slowest: 115.8369 secs. Fastest: 46.4803 secs. Average: 70.5216 secs. Response time histogram: 46.4803 [1] |∎∎ 53.4160 [20] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 60.3516 [12] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 67.2873 [12] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 74.2229 [15] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 81.1586 [11] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 88.0943 [11] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 95.0299 [8] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 101.9656 [7] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 108.9012 [0] | 115.8369 [3] |∎∎∎∎∎∎ Latency distribution: 99% in 115.8369 secs. 300 node 并发拉取测试 12345678910111213141516171819202122Summary: Total: 240.1224 secs. Slowest: 240.1149 secs. Fastest: 53.0441 secs. Average: 138.6291 secs. Response time histogram: 53.0441 [1] | 71.7512 [15] |∎∎∎∎∎∎∎∎∎∎ 90.4583 [27] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 109.1654 [40] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 127.8724 [55] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 146.5795 [38] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 165.2866 [32] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 183.9937 [41] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 202.7008 [27] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 221.4078 [18] |∎∎∎∎∎∎∎∎∎∎∎∎∎ 240.1149 [6] |∎∎∎∎ Latency distribution: 99% in 237.0826 secs. harbor 性能总结2 HA harbor 性能2.1 并发拉取镜像数和拉取延迟关系 拉取 1.7G 大镜像所需要的时间和并发拉取node的数量基本成线性正比的关系，并发度每增加100，拉取时间就相应增加一分钟。 2.2 并发拉取镜像数和cpu 使用率关系 并发拉取量每增加 100，cpu 使用率就增加 10%，当并发达到 500 时，cpu使用率为50%左右 。可知当拉取镜像的并发数达到500以上时，对 harbor 的压力会比较大。 推荐增加harbor 实例个数来缓解压力。 2 HA harbor 和 4HA harbpr 拉取镜像平均时间对比 50 nodes 100 nodes 300 nodes 2HA harbor 49.8971 s 70.0965 s 152.4312 s 4HA harbor 45.7705 s 70.5216 s 138.6291 s 增加 harbor 实例个数，拉取镜像所花费时间没有明显的的减少。时间增加应该和后端存储的性能和网络带宽有关。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows 激活]]></title>
    <url>%2F2019%2F10%2F08%2Fwindows-%E6%BF%80%E6%B4%BB%2F</url>
    <content type="text"><![CDATA[windows kms激活服务windows 系统激活命令 12345set ipk=${ipk}set kms_addr=${kms_addr}cscript //Nologo %windir%\system32\slmgr.vbs /ipk %ipk%cscript //Nologo %windir%\system32\slmgr.vbs -skms %kms_addr%cscript //Nologo %windir%\system32\slmgr.vbs -ato 客户端激活ipk： 操作系统 ipk Windows Server 2012 DC R2 32/64bit W3GGN-FT8W3-Y4M27-J84CP-Q3VJ9 Windows Server 2008 DC R2 32/64bit 74YFP-3QFB3-KQT8W-PMXWJ-7M648 Windows Server 2008 DC R1 32/64bit 7M67G-PC374-GR742-YH8V4-TCBY3 Windows Server 2008 enterprise 32/64bit YQGMW-MPWTJ-34KDK-48M3W-X4Q6V Windows Server 2008 enterprise R2 32/64bit 489J6-VHDMP-X63PK-3K798-CPX3Y Windows7 Professional 32/64bit FJ82H-XT6CR-J8D7P-XQJJ2-GPDD4 Windows7 Enterprise 32/64bit 33PXH-7Y6KF-2VJC9-XBBR8-HVTHH windows_10_enterprise_x64_zh NPPR9-FWDCX-D2C8J-H872K-2YT43 Windows Server 2016 DC R2 64bit CB7KF-BWN84-R7R2Y-793K2-8XDDG kms服务器地址: xxx.xxx.xxx.xxx 验证结果打开cmd 输入slmgr /dlv 查看详细 的 kms 信息]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx-ldap]]></title>
    <url>%2F2019%2F09%2F24%2Fnginx-ldap%2F</url>
    <content type="text"><![CDATA[手动编译安装nginx，添加nginx-auth-ldap模块下载nginx源码包并解压。可在http://nginx.org/en/download.html下载.tar.gz的源码包，如（nginx-1.4.7.tar.gz） 下载后通过tar -xvzf 进行解压，解压后的nginx目录结构如下： 为nginx设置安装目录，并添加nginx-auth-ldap nginx模块 1）安装openldap-devel：yum -y install openldap-devel 2）下载nginx-auth-ldap模块： cd /usr/local/src git clone https://github.com/kvspb/nginx-auth-ldap.git 编译时候加入 --add-module=/usr/local/src/nginx-auth-ldap 3）安装nginx并启用ldap： 切换到解压后的nginx目录中执行命令：./configure --prefix=/opt/nginx --add-module=/usr/local/src/nginx-auth-ldap 参数说明： –prefix 用于指定nginx编译后的安装目录 –add-module 为添加的第三方模块，此次添加了ldap的nginx模块 执行安装命令：make install 配置nginx 文件 在nginx主配置文件（nginx.conf）的http标签中添加如下代码group_attribute People 这个是验证的时候，访问哪个组 12345678910 http { ldap_server openldap { url ldap://172.16.6.13:389/DC=ptmind,DC=com?cn?sub?(objectClass=person); binddn "cn=ldap,dc=ptmind,dc=com"; binddn_passwd "xxxxx"; group_attribute People; group_attribute_is_dn on; require valid_user; }} 启动验证server 12345678910server { listen 8000; server_name localhost; location / { root html; index index.html index.htm; auth_ldap "Forbidden"; auth_ldap_servers openldap; } }]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql记录]]></title>
    <url>%2F2019%2F09%2F19%2Fmysql%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[添加一个字段ALTER TABLE jw_user_role ADD zk_env VARCHAR(16); 修改字段为not null，还要把原来的类型也写出来ALTER TABLE jw_user_role MODIFY zk_env VARCHAR(16) NOT NULL; 更改列名alter table student change physics physisc char(10) not null; 先来看看常用的方法MySql的简单语法，常用，却不容易记住。当然，这些Sql语法在各数据库中基本通用。下面列出： 1.增加一个字段 alter table user add COLUMN new1 VARCHAR(20) DEFAULT NULL; //增加一个字段，默认为空alter table user add COLUMN new2 VARCHAR(20) NOT NULL; //增加一个字段，默认不能为空 2.删除一个字段 alter table user DROP COLUMN new2; //删除一个字段 3.修改一个字段 alter table user MODIFY new1 VARCHAR(10); //修改一个字段的类型alter table user CHANGE new1 new4 int; //修改一个字段的名称，此时一定要重新 主键alter table tabelname add new_field_id int(5) unsigned default 0 not null auto_increment ,add primary key (new_field_id); 增加一个新列alter table t2 add d timestamp;alter table infos add ex tinyint not null default ‘0′; 删除列alter table t2 drop column c; 重命名列alter table t1 change a b integer; 改变列的类型alter table t1 change b b bigint not null;alter table infos change list list tinyint not null default ‘0′; 重命名表alter table t1 rename t2; 加索引mysql> alter table tablename change depno depno int(5) not null;mysql> alter table tablename add index 索引名 (字段名1[，字段名2 …]);mysql> alter table tablename add index emp_name (name); 加主关键字的索引mysql> alter table tablename add primary key(id); 加唯一限制条件的索引mysql> alter table tablename add unique emp_name2(cardnumber); 删除某个索引mysql> alter table tablename drop index emp_name; 增加字段mysql> ALTER TABLE table_name ADD field_name field_type; 修改原字段名称及类型mysql> ALTER TABLE table_name CHANGE old_field_name new_field_name field_type; 删除字段mysql> ALTER TABLE table_name DROP field_name; mysql修改字段长度alter table 表名 modify column 字段名 类型; 例如 数据库中user表 name字段是varchar(30) 可以用 alter table user modify column name varchar(50);]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd资源类型]]></title>
    <url>%2F2019%2F08%2F16%2Fetcd%E8%B5%84%E6%BA%90%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1. ETCD资源类型There are three types of resources in etcd permission resources: users and roles in the user store key-value resources: key-value pairs in the key-value store settings resources: security settings, auth settings, and dynamic etcd cluster settings (election/heartbeat) 2. 权限资源Users：user用来设置身份认证（user：passwd），一个用户可以拥有多个角色，每个角色被分配一定的权限（只读、只写、可读写），用户分为root用户和非root用户。 Roles：角色用来关联权限，角色主要三类：root角色。默认创建root用户时即创建了root角色，该角色拥有所有权限；guest角色，默认自动创建，主要用于非认证使用。普通角色，由root用户创建角色，并分配指定权限。 注意：如果没有指定任何验证方式，即没显示指定以什么用户进行访问，那么默认会设定为 guest 角色。默认情况下 guest 也是具有全局访问权限的。如果不希望未授权就获取或修改etcd的数据，则可收回guest角色的权限或删除该角色，etcdctl role revoke 。 Permissions:权限分为只读、只写、可读写三种权限，权限即对指定目录或key的读写权限。 3. ETCD访问控制3.1. 访问控制相关命令123456789101112131415161718NAME: etcdctl - A simple command line client for etcd.USAGE: etcdctl [global options] command [command options] [arguments...]VERSION: 2.2.0COMMANDS: user user add, grant and revoke subcommands role role add, grant and revoke subcommands auth overall auth controls GLOBAL OPTIONS: --peers, -C a comma-delimited list of machine addresses in the cluster (default: "http://127.0.0.1:4001,http://127.0.0.1:2379") --endpoint a comma-delimited list of machine addresses in the cluster (default: "http://127.0.0.1:4001,http://127.0.0.1:2379") --cert-file identify HTTPS client using this SSL certificate file --key-file identify HTTPS client using this SSL key file --ca-file verify certificates of HTTPS-enabled servers using this CA bundle --username, -u provide username[:password] and prompt if password is not supplied. --timeout '1s' connection timeout per request 3.2. user相关命令1234567891011121314151617[root@localhost etcd]# etcdctl user --helpNAME: etcdctl user - user add, grant and revoke subcommandsUSAGE: etcdctl user command [command options] [arguments...]COMMANDS: add add a new user for the etcd cluster get get details for a user list list all current users remove remove a user for the etcd cluster grant grant roles to an etcd user revoke revoke roles for an etcd user passwd change password for a user help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.2.1. 添加root用户并设置密码etcdctl –endpoints http://172.16.22.36:2379 user add root 3.2.2. 添加非root用户并设置密码etcdctl –endpoints http://172.16.22.36:2379 –username root:123 user add huwh 3.2.3. 查看当前所有用户etcdctl –endpoints http://172.16.22.36:2379 –username root:123 user list 3.2.4. 将用户添加到对应角色etcdctl –endpoints http://172.16.22.36:2379 –username root:123 user grant –roles test1 phpor 3.2.5. 查看用户拥有哪些角色etcdctl –endpoints http://172.16.22.36:2379 –username root:123 user get phpor 3.3. role相关命令12345678910111213141516[root@localhost etcd]# etcdctl role --helpNAME: etcdctl role - role add, grant and revoke subcommandsUSAGE: etcdctl role command [command options] [arguments...]COMMANDS: add add a new role for the etcd cluster get get details for a role list list all roles remove remove a role from the etcd cluster grant grant path matches to an etcd role revoke revoke path matches for an etcd role help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.3.1. 添加角色etcdctl –endpoints http://172.16.22.36:2379 –username root:2379 role add test1 3.3.2. 查看所有角色etcdctl –endpoints http://172.16.22.36:2379 –username root:123 role list 3.3.3. 给角色分配权限12345678910[root@localhost etcd]# etcdctl role grant --helpNAME: grant - grant path matches to an etcd roleUSAGE: command grant [command options] [arguments...]OPTIONS: --path Path granted for the role to access --read Grant read-only access --write Grant write-only access --readwrite Grant read-write access 1、只包含目录etcdctl –endpoints http://172.16.22.36:2379 –username root:123 role grant –readwrite –path /test1 test1 2、包括目录和子目录或文件etcdctl –endpoints http://172.16.22.36:2379 –username root:123 role grant –readwrite –path /test1/* test1 3.3.4. 查看角色所拥有的权限etcdctl –endpoints http://172.16.22.36:2379 –username root:2379 role get test1 3.4. auth相关操作123456789101112[root@localhost etcd]# etcdctl auth --helpNAME: etcdctl auth - overall auth controlsUSAGE: etcdctl auth command [command options] [arguments...]COMMANDS: enable enable auth access controls disable disable auth access controls help, h Shows a list of commands or help for one command OPTIONS: --help, -h show help 3.4.1. 开启认证etcdctl –endpoints http://172.16.22.36:2379 auth enable 4. 访问控制设置步骤 顺序 步骤 命令 1 添加root用户 etcdctl –endpoints http://: user add root 2 开启认证 etcdctl –endpoints http://: auth enable 3 添加非root用户 etcdctl –endpoints http://: –username root: user add 4 添加角色 etcdctl –endpoints http://: –username root: role add 5 给角色授权（只读、只写、可读写） etcdctl –endpoints http://: –username root: role grant –readwrite –path 6 给用户分配角色（即分配了角色对应的权限） etcdctl –endpoints http://: –username root: user grant –roles 5. 访问认证的API调用更多参考 https://coreos.com/etcd/docs/latest/v2/auth_api.html https://coreos.com/etcd/docs/latest/v2/authentication.html]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[容器添加宿主机网卡]]></title>
    <url>%2F2019%2F07%2F01%2Fip-netns%2F</url>
    <content type="text"><![CDATA[实现宿主机网卡添加到Docker容器 添加弹性网卡到宿主机 准备虚拟化网络环境。这里不需要新建的netns（ip netns add netfoo1），而是直接在/var/run/netns下创建软连接，指向运行中的docker容器的netns。 123mkdir -p /var/run/netnsNS_PID=$(docker inspect -f '{{.State.Pid}}' ${CONTAINER_NAME})ln -s /proc/${NS_PID}/ns/net /var/run/netns/${NS_PID} 添加虚拟网卡。这里不需要新建veth pair（ip link add vethfoo1 type veth peer name vethfoo2），而是直接将宿主机上附加的第二块虚拟网卡eth1改名为d-eth0，添加到前面的虚拟网络环境中（/var/run/netns/下的${NS_PID}）。 1ip link set dev eth1 name eth1@eni netns ${NS_PID} 最后，启用虚拟网络环境中新添加的虚拟网卡，并重新设置好添加到容器前的IP和网关。 123456ip netns exec ${NS_PID} ip link set dev eth1@eni upip netns exec ${NS_PID} ip addr add ${CONTAINER_IP}/24 dev eth1@eniip netns exec ${NS_PID} route add -net 0.0.0.0/0 gw ${CONTAINER_GATEWAY}### 或者ip netns exec ${NS_PID} dhclient eth1@eni namespacenamespace是一个独立的网络协议栈，通过namespace，可以将网络设备分隔开，设置独立的路由规则、防火墙规则等。 一个设备只能属于一个namespace。 1man ip-netns 可以通过ip netns [NAMESPACE] [CMD…] 在指定的namespace中操作，例如： //查看名为AAA的ns中的网络设备 1ip netns AAA ip link 基本操作创建ns1: 1ip netns add ns1 查看ns1中的设备: 123ip netns exec ns1 ip link1: lo: mtu 65536 qdisc noop state DOWN mode DEFAULT qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 将网卡eth1添加到ns1中: 1$ip link set eth1 netns ns1 12345$ip netns exec ns1 ip link1: lo: mtu 65536 qdisc noop state DOWN mode DEFAULT qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:003: eth1: mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether 08:00:27:b3:6c:38 brd ff:ff:ff:ff:ff:ff 将网卡eth1重新添加到默认的ns中: 1ip netns exec ns1 ip link set eth1 netns 1 注意必须在ns1中设置，最后一个1表示，进程1所在的namespace。 删除netns： 1ip netns delete ns1 [linux网络虚拟化][3]中给出了一个利用veth连接两个namespace的例子。 利用veth连接两个namespace123456789101112131415161718ip netns add net0ip netns add net1ip link add type vethip link set veth0 netns net0ip link set veth1 netns net1ip netns exec net0 ip link set veth0 upip netns exec net0 ip address add 10.0.1.1/24 dev veth0ip netns exec net1 ip link set veth1 upip netns exec net1 ip address add 10.0.1.2/24 dev veth1ip netns exec net1 ping 10.0.1.1PING 10.0.1.1 (10.0.1.1) 56(84) bytes of data.64 bytes from 10.0.1.1: icmp_seq=1 ttl=64 time=0.036 ms64 bytes from 10.0.1.1: icmp_seq=2 ttl=64 time=0.066 ms 两个namespace连接到bridge 创建三个ns，并利用veth连接: 123456789ip netns add net0ip netns add net1ip netns add bridgeip link add type vethip link set dev veth0 name net0-bridge netns net0 //重新命名ip link set dev veth1 name bridge-net0 netns bridgeip link add type vethip link set dev veth0 name net1-bridge netns net1ip link set dev veth1 name bridge-net1 netns bridge 配置bridge，将另外两个ns的对端veth设备接入bridge: 123456ip netns exec bridge brctl addbr brip netns exec bridge ip link set dev br upip netns exec bridge ip link set dev bridge-net0 upip netns exec bridge ip link set dev bridge-net1 upip netns exec bridge brctl addif br bridge-net0ip netns exec bridge brctl addif br bridge-net1 配置两个ns中的veth设备: 12345ip netns exec net0 ip link set dev net0-bridge upip netns exec net0 ip address add 10.0.1.1/24 dev net0-bridgeip netns exec net1 ip link set dev net1-bridge upip netns exec net1 ip address add 10.0.1.2/24 dev net1-bridge 配置lldpd检查线路链接情况随着虚拟网络环境增加，环境中网卡数量也在不断增加，经常会忘记环境中哪些网卡连接到哪里，通过 lldp [2] 协议，我们可以清楚看到每个网卡连接到了哪些环境中的哪个网卡。 github 上有一个 lldp 在 linux 下的开源实现 [3]，通过在每个环境中起一个 lldp daemon，我们就可以实时查看每个网卡的连接情况 Bridge 上 lldp 的数据 123456789101112131415161718192021222324252627$ lldpcli show neighborsLLDP neighbors:Interface: bridge-net0, via: LLDP, RID: 2, Time: 0 day, 00:06:53 Chassis: ChassisID: mac 82:be:2a:ec:70:69 SysName: localhost SysDescr: net0 Capability: Bridge, off Capability: Router, off Capability: Wlan, off Port: PortID: mac 82:be:2a:ec:70:69 PortDescr: net0-bridgeInterface: bridge-net1, via: LLDP, RID: 1, Time: 0 day, 00:06:53 Chassis: ChassisID: mac b2:34:28:b1:be:49 SysName: localhost SysDescr: net1 Capability: Bridge, off Capability: Router, off Capability: Wlan, off Port: PortID: mac b2:34:28:b1:be:49 PortDescr: net1-bridge]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>netns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcdctl介绍]]></title>
    <url>%2F2019%2F06%2F06%2Fetcdctl%2F</url>
    <content type="text"><![CDATA[etcdctl的v3版本与v2版本使用命令有所不同，本文介绍etcdctl v3版本的命令工具的使用方式。 1. etcdctl的安装etcdctl的二进制文件可以在 github.com/coreos/etcd/releases 选择对应的版本下载，例如可以执行以下install_etcdctl.sh的脚本，修改其中的版本信息。 1234567891011121314#!/bin/bashETCD_VER=v3.3.4ETCD_DIR=etcd-downloadDOWNLOAD_URL=https://github.com/coreos/etcd/releases/download# Downloadmkdir ${ETCD_DIR}cd ${ETCD_DIR}wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz tar -xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz# installcd etcd-${ETCD_VER}-linux-amd64cp etcdctl /usr/local/bin/ 2. etcdctl V3使用etcdctlv3的版本时，需设置环境变量ETCDCTL_API=3。 12345678export ETCDCTL_API=3或者在`/etc/profile`文件中添加环境变量vi /etc/profile...ETCDCTL_API=3...source /etc/profile 查看当前etcdctl的版本信息etcdctl version。 123[root@k8s-dbg-master-1 etcd]# etcdctl versionetcdctl version: 3.3.4API version: 3.3 更多命令帮助可以查询etcdctl —help。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[root@k8s-dbg-master-1 etcd]# etcdctl --helpNAME: etcdctl - A simple command line client for etcd3.USAGE: etcdctlVERSION: 3.3.4API VERSION: 3.3COMMANDS: get Gets the key or a range of keys put Puts the given key into the store del Removes the specified key or range of keys [key, range_end) txn Txn processes all the requests in one transaction compaction Compacts the event history in etcd alarm disarm Disarms all alarms alarm list Lists all alarms defrag Defragments the storage of the etcd members with given endpoints endpoint health Checks the healthiness of endpoints specified in `--endpoints` flag endpoint status Prints out the status of endpoints specified in `--endpoints` flag endpoint hashkv Prints the KV history hash for each endpoint in --endpoints move-leader Transfers leadership to another etcd cluster member. watch Watches events stream on keys or prefixes version Prints the version of etcdctl lease grant Creates leases lease revoke Revokes leases lease timetolive Get lease information lease list List all active leases lease keep-alive Keeps leases alive (renew) member add Adds a member into the cluster member remove Removes a member from the cluster member update Updates a member in the cluster member list Lists all members in the cluster snapshot save Stores an etcd node backend snapshot to a given file snapshot restore Restores an etcd member snapshot to an etcd directory snapshot status Gets backend snapshot status of a given file make-mirror Makes a mirror at the destination etcd cluster migrate Migrates keys in a v2 store to a mvcc store lock Acquires a named lock elect Observes and participates in leader election auth enable Enables authentication auth disable Disables authentication user add Adds a new user user delete Deletes a user user get Gets detailed information of a user user list Lists all users user passwd Changes password of user user grant-role Grants a role to a user user revoke-role Revokes a role from a user role add Adds a new role role delete Deletes a role role get Gets detailed information of a role role list Lists all roles role grant-permission Grants a key to a role role revoke-permission Revokes a key from a role check perf Check the performance of the etcd cluster help Help about any commandOPTIONS: --cacert="" verify certificates of TLS-enabled secure servers using this CA bundle --cert="" identify secure client using this TLS certificate file --command-timeout=5s timeout for short running command (excluding dial timeout) --debug[=false] enable client-side debug logging --dial-timeout=2s dial timeout for client connections -d, --discovery-srv="" domain name to query for SRV records describing cluster endpoints --endpoints=[127.0.0.1:2379] gRPC endpoints --hex[=false] print byte strings as hex encoded strings --insecure-discovery[=true] accept insecure SRV records describing cluster endpoints --insecure-skip-tls-verify[=false] skip server certificate verification --insecure-transport[=true] disable transport security for client connections --keepalive-time=2s keepalive time for client connections --keepalive-timeout=6s keepalive timeout for client connections --key="" identify secure client using this TLS key file --user="" username[:password] for authentication (prompt if password is not supplied) -w, --write-out="simple" set the output format (fields, json, protobuf, simple, table) 3. etcdctl 常用命令3.1. 指定etcd集群123456HOST_1=10.240.0.17HOST_2=10.240.0.18HOST_3=10.240.0.19ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379etcdctl --endpoints=$ENDPOINTS member list 3.2. 增删改查1、增 1etcdctl --endpoints=$ENDPOINTS put foo "Hello World!" 2、查 12etcdctl --endpoints=$ENDPOINTS get fooetcdctl --endpoints=$ENDPOINTS --write-out="json" get foo 基于相同前缀查找 12345etcdctl --endpoints=$ENDPOINTS put web1 value1etcdctl --endpoints=$ENDPOINTS put web2 value2etcdctl --endpoints=$ENDPOINTS put web3 value3etcdctl --endpoints=$ENDPOINTS get web --prefix 列出所有的key 1etcdctl --endpoints=$ENDPOINTS get / --prefix --keys-only 3、删** 123456etcdctl --endpoints=$ENDPOINTS put key myvalueetcdctl --endpoints=$ENDPOINTS del keyetcdctl --endpoints=$ENDPOINTS put k1 value1etcdctl --endpoints=$ENDPOINTS put k2 value2etcdctl --endpoints=$ENDPOINTS del k --prefix 3.3. 集群状态集群状态主要是etcdctl endpoint status 和etcdctl endpoint health两条命令。 123456789101112131415etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status+------------------+------------------+---------+---------+-----------+-----------+------------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+------------------+------------------+---------+---------+-----------+-----------+------------+| 10.240.0.17:2379 | 4917a7ab173fabe7 | 3.0.0 | 45 kB | true | 4 | 16726 || 10.240.0.18:2379 | 59796ba9cd1bcd72 | 3.0.0 | 45 kB | false | 4 | 16726 || 10.240.0.19:2379 | 94df724b66343e6c | 3.0.0 | 45 kB | false | 4 | 16726 |+------------------+------------------+---------+---------+-----------+-----------+------------+etcdctl --endpoints=$ENDPOINTS endpoint health10.240.0.17:2379 is healthy: successfully committed proposal: took = 3.345431ms10.240.0.19:2379 is healthy: successfully committed proposal: took = 3.767967ms10.240.0.18:2379 is healthy: successfully committed proposal: took = 4.025451ms 3.4. 集群成员跟集群成员相关的命令如下：1234member add Adds a member into the clustermember remove Removes a member from the clustermember update Updates a member in the clustermember list Lists all members in the cluster 例如 etcdctl member list列出集群成员的命令。1234567etcdctl --endpoints=http://172.16.5.4:12379 member list -w table+-----------------+---------+-------+------------------------+-----------------------------------------------+| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS |+-----------------+---------+-------+------------------------+-----------------------------------------------+| c856d92a82ba66a | started | etcd0 | http://172.16.5.4:2380 | http://172.16.5.4:2379,http://172.16.5.4:4001 |+-----------------+---------+-------+------------------------+-----------------------------------------------+ 4. etcdctl get使用etcdctl {command} --help可以查看具体命令的帮助信息。 12345678910111213141516171819202122232425262728293031# etcdctl get --helpNAME: get - Gets the key or a range of keysUSAGE: etcdctl get [options] [range_end]OPTIONS: --consistency="l" Linearizable(l) or Serializable(s) --from-key[=false] Get keys that are greater than or equal to the given key using byte compare --keys-only[=false] Get only the keys --limit=0 Maximum number of results --order="" Order of results; ASCEND or DESCEND (ASCEND by default) --prefix[=false] Get keys with matching prefix --print-value-only[=false] Only write values when using the "simple" output format --rev=0 Specify the kv revision --sort-by="" Sort target; CREATE, KEY, MODIFY, VALUE, or VERSIONGLOBAL OPTIONS: --cacert="" verify certificates of TLS-enabled secure servers using this CA bundle --cert="" identify secure client using this TLS certificate file --command-timeout=5s timeout for short running command (excluding dial timeout) --debug[=false] enable client-side debug logging --dial-timeout=2s dial timeout for client connections --endpoints=[127.0.0.1:2379] gRPC endpoints --hex[=false] print byte strings as hex encoded strings --insecure-skip-tls-verify[=false] skip server certificate verification --insecure-transport[=true] disable transport security for client connections --key="" identify secure client using this TLS key file --user="" username[:password] for authentication (prompt if password is not supplied) -w, --write-out="simple" set the output format (fields, json, protobuf, simple, table) 文章参考： https://coreos.com/etcd/docs/latest/demo.html]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JetBrains golang, JetBrains PhpStorm, JetBrains PyCharm 激活]]></title>
    <url>%2F2019%2F05%2F16%2FJetBrains-active%2F</url>
    <content type="text"><![CDATA[JetBrains IDEA 激活流程编辑hosts 文件 wind + R 打开运行输入: 1notepad C:\Windows\System32\drivers\etc\hosts 复制粘贴: 120.0.0.0 account.jetbrains.com0.0.0.0 www.jetbrains.com 打开要激活的软件: JetBrains PhpStorm 2019.1 为例: 点击 “help” –> “Register” 复制粘贴 code 1: 1812LFWMRSH-eyJsaWNlbnNlSWQiOiI4MTJMRldNUlNIIiwibGljZW5zZWVOYW1lIjoi5q2j54mIIOaOiOadgyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wNC0yMSIsInBhaWRVcFRvIjoiMjAyMC0wNC0yMCJ9LHsiY29kZSI6IkFDIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wNC0yMSIsInBhaWRVcFRvIjoiMjAyMC0wNC0yMCJ9LHsiY29kZSI6IkRQTiIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDQtMjEiLCJwYWlkVXBUbyI6IjIwMjAtMDQtMjAifSx7ImNvZGUiOiJQUyIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDQtMjEiLCJwYWlkVXBUbyI6IjIwMjAtMDQtMjAifSx7ImNvZGUiOiJHTyIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDQtMjEiLCJwYWlkVXBUbyI6IjIwMjAtMDQtMjAifSx7ImNvZGUiOiJETSIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDQtMjEiLCJwYWlkVXBUbyI6IjIwMjAtMDQtMjAifSx7ImNvZGUiOiJDTCIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDQtMjEiLCJwYWlkVXBUbyI6IjIwMjAtMDQtMjAifSx7ImNvZGUiOiJSUzAiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiUkMiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiUkQiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiUk0iLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiV1MiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiREIiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiREMiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA0LTIxIiwicGFpZFVwVG8iOiIyMDIwLTA0LTIwIn0seyJjb2RlIjoiUlNVIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wNC0yMSIsInBhaWRVcFRvIjoiMjAyMC0wNC0yMCJ9XSwiaGFzaCI6IjEyNzk2ODc3LzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-ti4tUsQISyJF/zfWxSHCr+IcYrX2w24JO5bUZCPIGKSi+IrgQ0RT2uum9n96o+Eob9Z1iQ9nUZ6FJdpEW5g0Exe6sw8fLrWMoLFhtCIvVgQxEEt+M7Z2xD0esmjP1kPKXZyc/i+NCxA2EO2Sec9uifqklBGP1L3xoENAw2QsIWBfttIe6EPWhbS8TIMMr2vF/S3HrN8To5Hj5lwD/t1GHgFK1uWrhsuifAiKcVzqogybzGiR1h2+yNYTMbKxP7uPCcdYMsIyrBNVRGA3IuEJgyGQTQlFbnVQoVUTGPW2tQxprmC464wMjKi40JHh27WzjOHPwgzxDaigwn4Z0EbSpA==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQAF8uc+YJOHHwOFcPzmbjcxNDuGoOUIP+2h1R75Lecswb7ru2LWWSUMtXVKQzChLNPn/72W0k+oI056tgiwuG7M49LXp4zQVlQnFmWU1wwGvVhq5R63Rpjx1zjGUhcXgayu7+9zMUW596Lbomsg8qVve6euqsrFicYkIIuUu4zYPndJwfe0YkS5nY72SHnNdbPhEnN8wcB2Kz+OIG0lih3yz5EqFhld03bGp222ZQCIghCTVL6QBNadGsiN/lWLl4JdR3lJkZzlpFdiHijoVRdWeSWqM4y0t23c92HXKrgppoSV18XMxrWVdoSM3nuMHwxGhFyde05OdDtLpCv+jlWf5REAHHA201pAU6bJSZINyHDUTB+Beo28rRXSwSh3OUIvYwKNVeoBY+KwOJ7WnuTCUq1meE6GkKc4D/cXmgpOyW/1SmBz3XjVIi/zprZ0zf3qH5mkphtg6ksjKgKjmx1cXfZAAX6wcDBNaCL+Ortep1Dh8xDUbqbBVNBL4jbiL3i3xsfNiyJgaZ5sX7i8tmStEpLbPwvHcByuf59qJhV/bZOl8KqJBETCDJcY6O2aqhTUy+9x93ThKs1GKrRPePrWPluud7ttlgtRveit/pcBrnQcXOl1rHq7ByB8CFAxNotRUYL9IF5n3wJOgkPojMy6jetQA5Ogc8Sm7RG6vg1yow== 复制粘贴 code 2: 1MNQ043JMTU-eyJsaWNlbnNlSWQiOiJNTlEwNDNKTVRVIiwibGljZW5zZWVOYW1lIjoiR1VPIEJJTiIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wNC0wNSIsInBhaWRVcFRvIjoiMjAyMC0wNC0wNCJ9XSwiaGFzaCI6IjEyNjIxNDIwLzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-tltrJHc5lqCKLnza1bcLhMzDkfJeBqhRWbvcrPF9Gqo+X+iFWeKQXoEUOlrG38uSzzmX05ph//PgXgyVfP5RXKsaRMfrv/thoouS5sA0aTemm3z6uRiFirTDj60KSGr5XZoP/WAXO4nuti6SRKZUbr/VSAtRPQRiCJvevq+3gWPDGu2aZ0AemiNLq4qIVWH3wxTN7lK2h5uJssZsyy35Yy9O703c5PFU0fxCj2HRgXq7H/91X+ZNLvvAZAVU9B7bOqnY4ZzNNV/cjY8B5gNo53Lo6s2szEV7DsZ+8e7k8P4Yo81DKxneCBoe4wiZmRaCIT6wK3/27KrtQTFMcGeZMA==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQAF8uc+YJOHHwOFcPzmbjcxNDuGoOUIP+2h1R75Lecswb7ru2LWWSUMtXVKQzChLNPn/72W0k+oI056tgiwuG7M49LXp4zQVlQnFmWU1wwGvVhq5R63Rpjx1zjGUhcXgayu7+9zMUW596Lbomsg8qVve6euqsrFicYkIIuUu4zYPndJwfe0YkS5nY72SHnNdbPhEnN8wcB2Kz+OIG0lih3yz5EqFhld03bGp222ZQCIghCTVL6QBNadGsiN/lWLl4JdR3lJkZzlpFdiHijoVRdWeSWqM4y0t23c92HXKrgppoSV18XMxrWVdoSM3nuMHwxGhFyde05OdDtLpCv+jlWf5REAHHA201pAU6bJSZINyHDUTB+Beo28rRXSwSh3OUIvYwKNVeoBY+KwOJ7WnuTCUq1meE6GkKc4D/cXmgpOyW/1SmBz3XjVIi/zprZ0zf3qH5mkphtg6ksjKgKjmx1cXfZAAX6wcDBNaCL+Ortep1Dh8xDUbqbBVNBL4jbiL3i3xsfNiyJgaZ5sX7i8tmStEpLbPwvHcByuf59qJhV/bZOl8KqJBETCDJcY6O2aqhTUy+9x93ThKs1GKrRPePrWPluud7ttlgtRveit/pcBrnQcXOl1rHq7ByB8CFAxNotRUYL9IF5n3wJOgkPojMy6jetQA5Ogc8Sm7RG6vg1yow== 复制粘贴 code 3: 1C40PF37RR0-eyJsaWNlbnNlSWQiOiJDNDBQRjM3UlIwIiwibGljZW5zZWVOYW1lIjoiemhhbmcgeW9uZyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiJGb3IgZWR1Y2F0aW9uYWwgdXNlIG9ubHkiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwicGFpZFVwVG8iOiIyMDE5LTEyLTEwIn0seyJjb2RlIjoiQUMiLCJwYWlkVXBUbyI6IjIwMTktMTItMTAifSx7ImNvZGUiOiJEUE4iLCJwYWlkVXBUbyI6IjIwMTktMTItMTAifSx7ImNvZGUiOiJQUyIsInBhaWRVcFRvIjoiMjAxOS0xMi0xMCJ9LHsiY29kZSI6IkdPIiwicGFpZFVwVG8iOiIyMDE5LTEyLTEwIn0seyJjb2RlIjoiRE0iLCJwYWlkVXBUbyI6IjIwMTktMTItMTAifSx7ImNvZGUiOiJDTCIsInBhaWRVcFRvIjoiMjAxOS0xMi0xMCJ9LHsiY29kZSI6IlJTMCIsInBhaWRVcFRvIjoiMjAxOS0xMi0xMCJ9LHsiY29kZSI6IlJDIiwicGFpZFVwVG8iOiIyMDE5LTEyLTEwIn0seyJjb2RlIjoiUkQiLCJwYWlkVXBUbyI6IjIwMTktMTItMTAifSx7ImNvZGUiOiJQQyIsInBhaWRVcFRvIjoiMjAxOS0xMi0xMCJ9LHsiY29kZSI6IlJNIiwicGFpZFVwVG8iOiIyMDE5LTEyLTEwIn0seyJjb2RlIjoiV1MiLCJwYWlkVXBUbyI6IjIwMTktMTItMTAifSx7ImNvZGUiOiJEQiIsInBhaWRVcFRvIjoiMjAxOS0xMi0xMCJ9LHsiY29kZSI6IkRDIiwicGFpZFVwVG8iOiIyMDE5LTEyLTEwIn0seyJjb2RlIjoiUlNVIiwicGFpZFVwVG8iOiIyMDE5LTEyLTEwIn1dLCJoYXNoIjoiMTEyMjAxNzEvMCIsImdyYWNlUGVyaW9kRGF5cyI6MCwiYXV0b1Byb2xvbmdhdGVkIjpmYWxzZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlfQ==-qzXEpmSWGn+FL4AjcALxNR9iBbMJGiEiuFItN8L3A6Rt9hAMOuiz4QEjtgl7+MzJ29iUPy8cljVSOkSv6V6Y/dX3KN3pXwg4jQWmoi2zrhDSBSg3c8SrzIObkj1irh+uKezGaAbYQGNQJQFbyY3c1Myvmmp45zDN0iEFiH1jE9CEcpaz+ER9b++FcuUt19aGmXUDLcdRkO2/LI+JkMHqq8sOij3O2K5vPRlSSPCeTIrD8wNS6zGjeCdQKrBi0e1UBJE+3HXtDUF2Gq6rvgVQIqbQVupLZqLsRMOaBURvlFM1P7pHLGnY9U0SgOV9/lGpyQ1K8I6xbUN8vqJZGkTXMA==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQAF8uc+YJOHHwOFcPzmbjcxNDuGoOUIP+2h1R75Lecswb7ru2LWWSUMtXVKQzChLNPn/72W0k+oI056tgiwuG7M49LXp4zQVlQnFmWU1wwGvVhq5R63Rpjx1zjGUhcXgayu7+9zMUW596Lbomsg8qVve6euqsrFicYkIIuUu4zYPndJwfe0YkS5nY72SHnNdbPhEnN8wcB2Kz+OIG0lih3yz5EqFhld03bGp222ZQCIghCTVL6QBNadGsiN/lWLl4JdR3lJkZzlpFdiHijoVRdWeSWqM4y0t23c92HXKrgppoSV18XMxrWVdoSM3nuMHwxGhFyde05OdDtLpCv+jlWf5REAHHA201pAU6bJSZINyHDUTB+Beo28rRXSwSh3OUIvYwKNVeoBY+KwOJ7WnuTCUq1meE6GkKc4D/cXmgpOyW/1SmBz3XjVIi/zprZ0zf3qH5mkphtg6ksjKgKjmx1cXfZAAX6wcDBNaCL+Ortep1Dh8xDUbqbBVNBL4jbiL3i3xsfNiyJgaZ5sX7i8tmStEpLbPwvHcByuf59qJhV/bZOl8KqJBETCDJcY6O2aqhTUy+9x93ThKs1GKrRPePrWPluud7ttlgtRveit/pcBrnQcXOl1rHq7ByB8CFAxNotRUYL9IF5n3wJOgkPojMy6jetQA5Ogc8Sm7RG6vg1yow== 复制粘贴 code 4: 16ZUMD7WWWU-eyJsaWNlbnNlSWQiOiI2WlVNRDdXV1dVIiwibGljZW5zZWVOYW1lIjoiSmV0cyBHcm91cCIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wOS0wMyIsInBhaWRVcFRvIjoiMjAyMC0wOS0wMiJ9LHsiY29kZSI6IkFDIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wOS0wMyIsInBhaWRVcFRvIjoiMjAyMC0wOS0wMiJ9LHsiY29kZSI6IkRQTiIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDktMDMiLCJwYWlkVXBUbyI6IjIwMjAtMDktMDIifSx7ImNvZGUiOiJQUyIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDktMDMiLCJwYWlkVXBUbyI6IjIwMjAtMDktMDIifSx7ImNvZGUiOiJHTyIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDktMDMiLCJwYWlkVXBUbyI6IjIwMjAtMDktMDIifSx7ImNvZGUiOiJETSIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDktMDMiLCJwYWlkVXBUbyI6IjIwMjAtMDktMDIifSx7ImNvZGUiOiJDTCIsImZhbGxiYWNrRGF0ZSI6IjIwMTktMDktMDMiLCJwYWlkVXBUbyI6IjIwMjAtMDktMDIifSx7ImNvZGUiOiJSUzAiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiUkMiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiUkQiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiUk0iLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiV1MiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiREIiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiREMiLCJmYWxsYmFja0RhdGUiOiIyMDE5LTA5LTAzIiwicGFpZFVwVG8iOiIyMDIwLTA5LTAyIn0seyJjb2RlIjoiUlNVIiwiZmFsbGJhY2tEYXRlIjoiMjAxOS0wOS0wMyIsInBhaWRVcFRvIjoiMjAyMC0wOS0wMiJ9XSwiaGFzaCI6IjE0Mjg5NzUwLzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-Gd8RATyTEnHcAydKuC7N1ZdeLaMP9IR+nlPyVxvLsczAUTGKxcuAYbfz/uVtepg8ey4NfJlPNS+AGcGz8x7ImkX9jEV9KElMxPu3tKSdF/WKo6JCONX7UtudYa/9EQum3banxci/qH7jejSrFZSN+YjWQiYTR0Q8gq4/a2RyQTgseZfpImY/nXkOWLwWArr/p+4ddp/bWQN4nLTW+Z4ZaQeLE96Z9viCdn62EKOcR02Hfr9Oju9VYQh1L8pGrTqNey5nUSv/LQUbVwo5qoYbBRos8l6ewkFNGsuC3vtOgGWSgkgChbDjWhW4Nkm4vDM2NFAphMsS1dgyPw3eJ3C+6A==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQAF8uc+YJOHHwOFcPzmbjcxNDuGoOUIP+2h1R75Lecswb7ru2LWWSUMtXVKQzChLNPn/72W0k+oI056tgiwuG7M49LXp4zQVlQnFmWU1wwGvVhq5R63Rpjx1zjGUhcXgayu7+9zMUW596Lbomsg8qVve6euqsrFicYkIIuUu4zYPndJwfe0YkS5nY72SHnNdbPhEnN8wcB2Kz+OIG0lih3yz5EqFhld03bGp222ZQCIghCTVL6QBNadGsiN/lWLl4JdR3lJkZzlpFdiHijoVRdWeSWqM4y0t23c92HXKrgppoSV18XMxrWVdoSM3nuMHwxGhFyde05OdDtLpCv+jlWf5REAHHA201pAU6bJSZINyHDUTB+Beo28rRXSwSh3OUIvYwKNVeoBY+KwOJ7WnuTCUq1meE6GkKc4D/cXmgpOyW/1SmBz3XjVIi/zprZ0zf3qH5mkphtg6ksjKgKjmx1cXfZAAX6wcDBNaCL+Ortep1Dh8xDUbqbBVNBL4jbiL3i3xsfNiyJgaZ5sX7i8tmStEpLbPwvHcByuf59qJhV/bZOl8KqJBETCDJcY6O2aqhTUy+9x93ThKs1GKrRPePrWPluud7ttlgtRveit/pcBrnQcXOl1rHq7ByB8CFAxNotRUYL9IF5n3wJOgkPojMy6jetQA5Ogc8Sm7RG6vg1yow== 注意激活码只在 2019.1 版本以前有效 pycharm-professional-2019.1 下载地址 PhpStorm-2019.1 下载地址 goland-2019.1 下载地址]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>JetBrains</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[debian-docker-ce]]></title>
    <url>%2F2019%2F05%2F10%2Fdebian9-8-docker-ce%2F</url>
    <content type="text"><![CDATA[Install docker-ce for Ubuntu/DebianUninstall old versions 1sudo apt-get remove docker docker-engine docker.io containerd runc Install Docker CE Pre Update the apt package index: 1sudo apt-get update Install packages to allow apt to use a repository over HTTPS: 123456sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker’s official GPG key: 1curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88, by searching for the last 8 characters of the fingerprint. 123456sudo apt-key fingerprint 0EBFCD88pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) sub 4096R/F273FCD8 2017-02-22 Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. Learn about nightly and test channels. 1234sudo add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install Docker CE Update the apt package index. 1sudo apt-get update Install the latest version of Docker CE and containerd, or go to the next step to install a specific version: 1sudo apt-get install docker-ce docker-ce-cli containerd.io To install a specific version of Docker CE, list the available versions in the repo, then select and install: a. List the versions available in your repo: 1234567apt-cache madison docker-ce docker-ce | 5:18.09.1~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages docker-ce | 5:18.09.0~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages docker-ce | 18.06.1~ce~3-0~debian | https://download.docker.com/linux/debian stretch/stable amd64 Packages docker-ce | 18.06.0~ce~3-0~debian | https://download.docker.com/linux/debian stretch/stable amd64 Packages ... b. Install a specific version using the version string from the second column, for example, 5:18.09.1~3-0~debian-stretch . 1sudo apt-get install docker-ce= docker-ce-cli= containerd.io Verify that Docker CE is installed correctly by running the hello-world image. 1sudo docker run hello-world]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet]]></title>
    <url>%2F2019%2F05%2F06%2Fkubelet%2F</url>
    <content type="text"><![CDATA[1. kubelet简介在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点资源。可以把kubelet理解成【Server-Agent】架构中的agent，是Node上的pod管家。 更多kubelet配置参数信息可参考kubelet –help 2. 节点管理节点通过设置kubelet的启动参数“–register-node”，来决定是否向API Server注册自己，默认为true。可以通过kubelet –help或者查看kubernetes源码【cmd/kubelet/app/server.go中】来查看该参数。 kubelet的配置文件 默认配置文件在/etc/kubernetes/kubelet中，其中 –api-servers：用来配置Master节点的IP和端口。 –kubeconfig：用来配置kubeconfig的路径，kubeconfig文件常用来指定证书。 –hostname-override：用来配置该节点在集群中显示的主机名。 –node-status-update-frequency：配置kubelet向Master心跳上报的频率，默认为10s。 3. Pod管理kubelet有几种方式获取自身Node上所需要运行的Pod清单。但本文只讨论通过API Server监听etcd目录，同步Pod列表的方式。 kubelet通过API Server Client使用WatchAndList的方式监听etcd中/registry/nodes/${当前节点名称}和/registry/pods的目录，将获取的信息同步到本地缓存中。 kubelet监听etcd，执行对Pod的操作，对容器的操作则是通过Docker Client执行，例如启动删除容器等。 kubelet创建和修改Pod流程： 为该Pod创建一个数据目录。 从API Server读取该Pod清单。 为该Pod挂载外部卷（External Volume） 下载Pod用到的Secret。 检查运行的Pod，执行Pod中未完成的任务。 先创建一个Pause容器，该容器接管Pod的网络，再创建其他容器。 Pod中容器的处理流程：1）比较容器hash值并做相应处理。2）如果容器被终止了且没有指定重启策略，则不做任何处理。3）调用Docker Client下载容器镜像，调用Docker Client运行容器。 4. 容器健康检查Pod通过探针的方式来检查容器的健康状态，具体可参考Pod详解#Pod健康检查。 5. cAdvisor资源监控kubelet通过cAdvisor获取本节点信息及容器的数据。cAdvisor为谷歌开源的容器资源分析工具，默认集成到kubernetes中。 cAdvisor自动采集CPU,内存，文件系统，网络使用情况，容器中运行的进程，默认端口为4194。可以通过Node IP+Port访问。 更多参考：http://github.com/google/cadvisor]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod调度]]></title>
    <url>%2F2019%2F04%2F25%2Fpod%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[Pod调度在kubernetes集群中，Pod（container）是应用的载体，一般通过RC、Deployment、DaemonSet、Job等对象来完成Pod的调度与自愈功能。 1. RC、Deployment:全自动调度RC的功能即保持集群中始终运行着指定个数的Pod。 在调度策略上主要有： 系统内置调度算法[最优Node] NodeSelector[定向调度] NodeAffinity[亲和性调度] 2. NodeSelector[定向调度]k8s中kube-scheduler负责实现Pod的调度，内部系统通过一系列算法最终计算出最佳的目标节点。如果需要将Pod调度到指定Node上，则可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配来达到目的。 1、kubectl label nodes {node-name} {label-key}={label-value} 2、nodeSelector:{label-key}:{label-value} 如果给多个Node打了相同的标签，则scheduler会根据调度算法从这组Node中选择一个可用的Node来调度。 如果Pod的nodeSelector的标签在Node中没有对应的标签，则该Pod无法被调度成功。 Node标签的使用场景： 对集群中不同类型的Node打上不同的标签，可控制应用运行Node的范围。例如role=frontend;role=backend;role=database。 3. NodeAffinity[亲和性调度]NodeAffinity意为Node亲和性调度策略，NodeSelector为精确匹配，NodeAffinity为条件范围匹配，通过In（属于）、NotIn（不属于）、Exists（存在一个条件）、DoesNotExist（不存在）、Gt（大于）、Lt（小于）等操作符来选择Node，使调度更加灵活。 RequiredDuringSchedulingRequiredDuringExecution：类似于NodeSelector，但在Node不满足条件时，系统将从该Node上移除之前调度上的Pod。 RequiredDuringSchedulingIgnoredDuringExecution：与上一个类似，区别是在Node不满足条件时，系统不一定从该Node上移除之前调度上的Pod。 PreferredDuringSchedulingIgnoredDuringExecution：指定在满足调度条件的Node中，哪些Node应更优先地进行调度。同时在Node不满足条件时，系统不一定从该Node上移除之前调度上的Pod。 如果同时设置了NodeSelector和NodeAffinity，则系统将需要同时满足两者的设置才能进行调度。 4. DaemonSet：特定场景调度DaemonSet是kubernetes1.2版本新增的一种资源对象，用于管理在集群中每个Node上仅运行一份Pod的副本实例。 该用法适用的应用场景： 在每个Node上运行一个GlusterFS存储或者Ceph存储的daemon进程。 在每个Node上运行一个日志采集程序：fluentd或logstach。 在每个Node上运行一个健康程序，采集该Node的运行性能数据，例如：Prometheus Node Exportor、collectd、New Relic agent或Ganglia gmond等。 DaemonSet的Pod调度策略与RC类似，除了使用系统内置算法在每台Node上进行调度，也可以通过NodeSelector或NodeAffinity来指定满足条件的Node范围进行调度。 5. Job：批处理调度kubernetes从1.2版本开始支持批处理类型的应用，可以通过kubernetes Job资源对象来定义并启动一个批处理任务。批处理任务通常并行（或串行）启动多个计算进程去处理一批工作项（work item），处理完后，整个批处理任务结束。 5.1. 批处理的三种模式 批处理按任务实现方式不同分为以下几种模式： Job Template Expansion模式一个Job对象对应一个待处理的Work item，有几个Work item就产生几个独立的Job，通过适用于Work item数量少，每个Work item要处理的数据量比较大的场景。例如有10个文件（Work item）,每个文件（Work item）为100G。 Queue with Pod Per Work Item采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，其中Job会启动N个Pod，每个Pod对应一个Work item。 Queue with Variable Pod Count采用一个任务队列存放Work item，一个Job对象作为消费者去完成这些Work item，其中Job会启动N个Pod，每个Pod对应一个Work item。但Pod的数量是可变的。 5.2. Job的三种类型1）Non-parallel Jobs 通常一个Job只启动一个Pod,除非Pod异常才会重启该Pod,一旦此Pod正常结束，Job将结束。 2）Parallel Jobs with a fixed completion count 并行Job会启动多个Pod，此时需要设定Job的.spec.completions参数为一个正数，当正常结束的Pod数量达到该值则Job结束。 3）Parallel Jobs with a work queue 任务队列方式的并行Job需要一个独立的Queue，Work item都在一个Queue中存放，不能设置Job的.spec.completions参数。 此时Job的特性： 每个Pod能独立判断和决定是否还有任务项需要处理 如果某个Pod正常结束，则Job不会再启动新的Pod 如果一个Pod成功结束，则此时应该不存在其他Pod还在干活的情况，它们应该都处于即将结束、退出的状态 如果所有的Pod都结束了，且至少一个Pod成功结束，则整个Job算是成功结束 参考文章 《Kubernetes权威指南》]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker代理]]></title>
    <url>%2F2019%2F04%2F23%2Fdocker%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Docker：配置HTTP/HTTPS代理 起因我在使用Docker的pull命令拉取ELK官方提供的镜像时，会出现无法连接的情况，并且会出现TLS handshake timeout的错误。在搜索相关文章之后得出结论：国内的网络环境不好，导致连接docker.elastic.co失败或无法连接。于是我第一时间想到了代理的方式，好在Docker支持设置代理来访问其他Registry，下面记录整个配置过程。 准备工作首先，你的机器上需要安装好Docker，当我写这篇文章时，Docker的版本为18.03，对于后续版本，本文章的配置方法可能会失效。 此外，还需要准备一个代理服务器，可以正常访问境外网站（如：Google，YouTuBe等）。我用的是VPS搭建的Shadowsocks代理，本机Shadowsocks客户端开启之后可以直接通过http://127.0.0.1:1080/访问境外网站。 假设你的环境也是Ubuntu（其他环境应该也是类似的）。 开始配置 创建如下路径的目录 1sudo mkdir -p /etc/systemd/system/docker.service.d 进入到上一步创建的目录下，并在该目录下创建一个名为http-proxy.conf的文件（如：/etc/systemd/system/docker.service.d/http-proxy.conf），使用vim编辑文件内容如下 12[Service]Environment="HTTPS_PROXY=http://127.0.0.1:1080/" "NO_PROXY=localhost,127.0.0.1,registry.docker-cn.com,hub-mirror.c.163.com" 刷新配置 1sudo systemctl daemon-reload 重启Docker服务 1sudo systemctl restart docker 查看配置 1systemctl show --property=Environment docker 出现如下信息表示配置成功： 1Environment=HTTPS_PROXY=http://127.0.0.1:1080/ NO_PROXY=localhost,127.0.0.1,registry.docker-cn.com,hub-mirror.c.163.com 验证配置是否生效 123456789# 重新从docker.elastic.co上拉取elasticsearch镜像，此时已经可以正常连接了，只是速度较慢。liuwei@liuwei-Ubuntu:~$ sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:6.2.46.2.4: Pulling from elasticsearch/elasticsearch469cfcc7a4b3: Downloading [==========================> ] 38.87MB/73.17MB8e27facfa9e0: Downloading [===================================> ] 40.05MB/56.33MBcdd15392adc7: Download complete ddcc70fbd933: Downloading [====================> ] 44.31MB/108.9MB3d3fa0383994: Waiting 15d1376ebd55: Waiting 这种方法适用于从一些第三方提供的Registry上拉取镜像时，由于网络原因无法连接。如果从Docker官方的镜像仓库中拉取镜像时，一种比较好的办法就是配置registry-mirrors实现加速，具体方法请自行搜索 参考链接]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pod configmap]]></title>
    <url>%2F2019%2F04%2F18%2Fpod-configmap%2F</url>
    <content type="text"><![CDATA[Pod的配置管理Kubernetes v1.2的版本提供统一的集群配置管理方案–ConfigMap。 1. ConfigMap：容器应用的配置管理使用场景： 生成为容器内的环境变量。 设置容器启动命令的启动参数（需设置为环境变量）。 以Volume的形式挂载为容器内部的文件或目录。 ConfigMap以一个或多个key:value的形式保存在kubernetes系统中供应用使用，既可以表示一个变量的值（例如：apploglevel=info），也可以表示完整配置文件的内容（例如：server.xml=…）。 可以通过yaml配置文件或者使用kubectl create configmap命令的方式创建ConfigMap。 2. 创建ConfigMap2.1. 通过yaml文件方式cm-appvars.yaml 1234567apiVersion: v1kind: ConfigMapmetadata: name: cm-appvarsdata: apploglevel: info appdatadir: /var/data 常用命令 kubectl create -f cm-appvars.yaml kubectl get configmap kubectl describe configmap cm-appvars kubectl get configmap cm-appvars -o yaml 2.2. 通过kubectl命令行方式通过kubectl create configmap创建，使用参数–from-file或–from-literal指定内容，可以在一行中指定多个参数。 1）通过–from-file参数从文件中进行创建，可以指定key的名称，也可以在一个命令行中创建包含多个key的ConfigMap。 kubectl create configmap NAME –from-file=[key=]source –from-file=[key=]source 2）通过–from-file参数从目录中进行创建，该目录下的每个配置文件名被设置为key，文件内容被设置为value。 kubectl create configmap NAME –from-file=config-files-dir 3）通过–from-literal从文本中进行创建，直接将指定的key=value创建为ConfigMap的内容。 kubectl create configmap NAME –from-literal=key1=value1 –from-literal=key2=value2 容器应用对ConfigMap的使用有两种方法： 通过环境变量获取ConfigMap中的内容。 通过Volume挂载的方式将ConfigMap中的内容挂载为容器内部的文件或目录。 2.3. 通过环境变量的方式ConfigMap的yaml文件:cm-appvars.yaml 1234567apiVersion: v1kind: ConfigMapmetadata: name: cm-appvarsdata: apploglevel: info appdatadir: /var/data Pod的yaml文件：cm-test-pod.yaml 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: cm-test-podspec: containers: - name: cm-test image: busybox command: ["/bin/sh","-c","env|grep APP"] env: - name: APPLOGLEVEL valueFrom: configMapKeyRef: name: cm-appvars key: apploglevel - name: APPDATADIR valueFrom: configMapKeyRef: name: cm-appvars key: appdatadir 创建命令： kubectl create -f cm-test-pod.yaml kubectl get pods –show-all kubectl logs cm-test-pod 3. 使用ConfigMap的限制条件 ConfigMap必须在Pod之前创建 ConfigMap也可以定义为属于某个Namespace。只有处于相同Namespace中的Pod可以引用它。 kubelet只支持可以被API Server管理的Pod使用ConfigMap。静态Pod无法引用。 在Pod对ConfigMap进行挂载操作时，容器内只能挂载为“目录”，无法挂载为文件。 参考文章 《Kubernetes权威指南》]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>configmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod volume]]></title>
    <url>%2F2019%2F04%2F17%2Fpod-volume%2F</url>
    <content type="text"><![CDATA[pod volume 使用同一个Pod中的多个容器可以共享Pod级别的存储卷Volume,Volume可以定义为各种类型，多个容器各自进行挂载，将Pod的Volume挂载为容器内部需要的目录。 例如：Pod级别的Volume:”app-logs”,用于tomcat向其中写日志文件，busybox读日志文件。 pod-volumes-applogs.yaml 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: volume-podspec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080 volumeMounts: - name: app-logs mountPath: /usr/local/tomcat/logs - name: busybox image: busybox command: ["sh","-c","tailf /logs/catalina*.log"] volumeMounts: - name: app-logs mountPath: /logs volumes: - name: app-logs emptuDir: {} 查看日志 kubectl logs -c kubectl exec -it -c – tail /usr/local/tomcat/logs/catalina.xx.log 参考文章 《Kubernetes权威指南》]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux进程切换后台和切换pid]]></title>
    <url>%2F2019%2F04%2F15%2FLinux%E8%BF%9B%E7%A8%8B%E5%88%87%E6%8D%A2%E5%90%8E%E5%8F%B0%E5%92%8C%E5%88%87%E6%8D%A2pid%2F</url>
    <content type="text"><![CDATA[Linux中如何让进程(或正在运行的程序)到后台运行?在Linux中，如果要让进程在后台运行，一般情况下，我们在命令后面加上&即可，实际上，这样是将命令放入到一个作业队列中了： $ ./test.sh &[1] 17208 $ jobs -l[1]+ 17208 Running ./test.sh &对于已经在前台执行的命令，也可以重新放到后台执行，首先按ctrl+z暂停已经运行的进程，然后使用bg命令将停止的作业放到后台运行： $ ./test.sh[1]+ Stopped ./test.sh $ bg %1[1]+ ./test.sh & $ jobs -l[1]+ 22794 Running ./test.sh &但是如上方到后台执行的进程，其父进程还是当前终端shell的进程，而一旦父进程退出，则会发送hangup信号给所有子进程，子进程收到hangup以后也会退出。如果我们要在退出shell的时候继续运行进程，则需要使用nohup忽略hangup信号，或者setsid将将父进程设为init进程(进程号为1) $ echo $$21734 $ nohup ./test.sh &[1] 29016 $ ps -ef | grep test515 29710 21734 0 11:47 pts/12 00:00:00 /bin/sh ./test.sh515 29713 21734 0 11:47 pts/12 00:00:00 grep test$ setsid ./test.sh &[1] 409 $ ps -ef | grep test515 410 1 0 11:49 ? 00:00:00 /bin/sh ./test.sh515 413 21734 0 11:49 pts/12 00:00:00 grep test上面的试验演示了使用nohup/setsid加上&使进程在后台运行，同时不受当前shell退出的影响。那么对于已经在后台运行的进程，该怎么办呢？可以使用disown命令： $ ./test.sh &[1] 2539 $ jobs -l[1]+ 2539 Running ./test.sh & $ disown -h %1 $ ps -ef | grep test515 410 1 0 11:49 ? 00:00:00 /bin/sh ./test.sh515 2542 21734 0 11:52 pts/12 00:00:00 grep test另外还有一种方法，即使将进程在一个subshell中执行，其实这和setsid异曲同工。方法很简单，将命令用括号() 括起来即可： $ (./test.sh &) $ ps -ef | grep test515 410 1 0 11:49 ? 00:00:00 /bin/sh ./test.sh515 12483 21734 0 11:59 pts/12 00:00:00 grep test注：本文试验环境为Red Hat Enterprise Linux AS release 4 (Nahant Update 5),shell为/bin/bash，不同的OS和shell可能命令有些不一样。例如AIX的ksh，没有disown，但是可以使用nohup -p PID来获得disown同样的效果。 还有一种更加强大的方式是使用screen，首先创建一个断开模式的虚拟终端，然后用-r选项重新连接这个虚拟终端，在其中执行的任何命令，都能达到nohup的效果，这在有多个命令需要在后台连续执行的时候比较方便： $ screen -dmS screen_test $ screen -listThere is a screen on: 27963.screen_test (Detached)1 Socket in /tmp/uscreens/S-jiangfeng. $ screen -r screen_test]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>pid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod检查]]></title>
    <url>%2F2019%2F04%2F12%2Fpod%E6%A3%80%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[Pod健康检查Pod的健康状态由两类探针来检查：LivenessProbe和ReadinessProbe。 1. 探针类型1. livenessProbe(存活探针) 表明容器是否正在运行。 如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略的影响。 如果容器不提供存活探针，则默认状态为 Success。 2. readinessProbe(就绪探针) 表明容器是否可以正常接受请求。 如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。 初始延迟之前的就绪状态默认为 Failure。 如果容器不提供就绪探针，则默认状态为 Success。 2. Handler探针是kubelet对容器执行定期的诊断，主要通过调用容器配置的三类Handler实现： Handler的类型： ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。 探测结果为以下三种之一： 成功：容器通过了诊断。 失败：容器未通过诊断。 未知：诊断失败，因此不会采取任何行动。 3. 探针使用方式 如果容器异常可以自动崩溃，则不一定要使用探针，可以由Pod的restartPolicy执行重启操作。 存活探针适用于希望容器探测失败后被杀死并重新启动，需要指定restartPolicy 为 Always 或 OnFailure。 就绪探针适用于希望Pod在不能正常接收流量的时候被剔除，并且在就绪探针探测成功后才接收流量。 存活探针由 kubelet 来执行，因此所有的请求都在 kubelet 的网络命名空间中进行。 3.1. LivenessProbe参数 initialDelaySeconds：启动容器后首次进行健康检查的等待时间，单位为秒。 timeoutSeconds:健康检查发送请求后等待响应的时间，如果超时响应kubelet则认为容器非健康，重启该容器，单位为秒。 3.2. LivenessProbe三种实现方式1）ExecAction:在一个容器内部执行一个命令，如果该命令状态返回值为0，则表明容器健康。 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: liveness-execspec: containers: - name: liveness image: tomcagcr.io/google_containers/busybox args: - /bin/sh - -c - echo ok > /tmp/health;sleep 10;rm -fr /tmp/health;sleep 600 livenessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 15 timeoutSeconds: 1 2）TCPSocketAction:通过容器IP地址和端口号执行TCP检查，如果能够建立TCP连接，则表明容器健康。 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-with-healthcheckspec: containers: - name: nginx image: nginx ports: - containnerPort: 80 livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 15 timeoutSeconds: 1 3）HTTPGetAction:通过容器的IP地址、端口号及路径调用HTTP Get方法，如果响应的状态码大于等于200且小于等于400，则认为容器健康。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-with-healthcheckspec: containers: - name: nginx image: nginx ports: - containnerPort: 80 livenessProbe: httpGet: path: /_status/healthz port: 80 initialDelaySeconds: 15 timeoutSeconds: 1 参考文章： https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ 《Kubernetes权威指南》]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod定义]]></title>
    <url>%2F2019%2F04%2F11%2Fpod%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[1. Pod的基本用法1.1. 说明 Pod实际上是容器的集合，在k8s中对运行容器的要求为：容器的主程序需要一直在前台运行，而不是后台运行。应用可以改造成前台运行的方式，例如Go语言的程序，直接运行二进制文件；java语言则运行主类；tomcat程序可以写个运行脚本。或者通过supervisor的进程管理工具，即supervisor在前台运行，应用程序由supervisor管理在后台运行。具体可参考supervisord。 当多个应用之间是紧耦合的关系时，可以将多个应用一起放在一个Pod中，同个Pod中的多个容器之间互相访问可以通过localhost来通信（可以把Pod理解成一个虚拟机，共享网络和存储卷）。 1.2. Pod相关命令 操作 命令 说明 创建 kubectl create -f frontend-localredis-pod.yaml 查询Pod运行状态 kubectl get pods –namespace= 查询Pod详情 kebectl describe pod –namespace= 该命令常用来排查问题，查看Event事件 删除 kubectl delete pod ;kubectl delete pod –all 更新 kubectl replace pod.yaml - 2. Pod的定义文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576apiVersion: v1kind: Podmetadata: name: string namaspace: string labels: - name: string annotations: - name: string spec: containers: - name: string images: string imagePullPolice: [Always | Never | IfNotPresent] command: [string] args: [string] workingDir: string volumeMounts: - name: string mountPath: string readOnly: boolean ports: - name: string containerPort: int hostPort: int protocol: string env: - name: string value: string resources: limits: cpu: string memory: string requests: cpu: string memory: string livenessProbe: exec: command: [string] httpGet: path: string port: int host: string scheme: string httpHeaders: - name: string value: string tcpSocket: port: int initialDelaySeconds: number timeoutSeconds: number periodSeconds: number successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] nodeSelector: object imagePullSecrets: - name: string hostNetwork: false volumes: - name: string emptyDir: {} hostPath: path: string secret: secretName: string items: - key: string path: string configMap: name: string items: - key: string path: string 3. 静态pod静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod。它们不能通过API Server进行管理，无法与ReplicationController、Deployment或DaemonSet进行关联，并且kubelet也无法对其健康检查。 静态Pod总是由kubelet创建，并且总在kubelet所在的Node上运行。 创建静态Pod的方式： 3.1. 通过配置文件方式需要设置kubelet的启动参数“–config”，指定kubelet需要监控的配置文件所在目录，kubelet会定期扫描该目录，并根据该目录的.yaml或.json文件进行创建操作。静态Pod无法通过API Server删除（若删除会变成pending状态），如需删除该Pod则将yaml或json文件从这个目录中删除。 例如： 配置目录为/etc/kubelet.d/，配置启动参数：–config=/etc/kubelet.d/，该目录下放入static-web.yaml。 12345678910111213apiVersion: v1kind: Podmetadata: name: static-web labels: name: static-webspec: containers: - name: static-web image: nginx ports: - name: web containerPort: 80 参考文章 《Kubernetes权威指南》]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod eviction]]></title>
    <url>%2F2019%2F04%2F10%2Fpod-eviction%2F</url>
    <content type="text"><![CDATA[K8S 的 pod evictionK8S 有个特色功能叫 pod eviction，它在某些场景下如节点 NotReady，资源不足时，把 pod 驱逐至其它节点。本文首先介绍该功能，最后谈谈落地经验。 介绍从发起模块的角度，pod eviction 可以分为两类： Kube-controller-manager: 周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，驱逐该节点上所有 pod。 Kubelet: 周期性检查本节点资源，当资源不足时，按照优先级驱逐部分 pod。 Kube-controller-manger 发起的驱逐Kube-controller-manager 周期性检查节点状态，每当节点状态为 NotReady，并且超出 podEvictionTimeout 时间后，就把该节点上的 pod 全部驱逐到其它节点，其中具体驱逐速度还受驱逐速度参数，集群大小等的影响。最常用的 2 个参数如下： –pod-eviction-timeout：NotReady 状态节点超过该时间后，执行驱逐，默认 5 min。 –node-eviction-rate：驱逐速度，默认为 0.1 pod/秒 当某个 zone 故障节点的数目超过一定阈值时，采用二级驱逐速度进行驱逐。 –large-cluster-size-threshold：判断集群是否为大集群，默认为 50，即 50 个节点以上的集群为大集群。 –unhealthy-zone-threshold：故障节点数比例，默认为 55% –secondary-node-eviction-rate：当大集群的故障节点超过 55% 时，采用二级驱逐速率，默认为 0.01 pod／秒。当小集群故障节点超过 55% 时，驱逐速率为 0 pod／秒。 判断是否驱逐的代码如下： 1234567891011121314151617func (nc *NodeController) monitorNodeStatus() error { ...... if currentReadyCondition != nil { // Check eviction timeout against decisionTimestamp if observedReadyCondition.Status == api.ConditionFalse && decisionTimestamp.After(nc.nodeStatusMap[node.Name].readyTransitionTimestamp.Add(nc.podEvictionTimeout)) { if nc.evictPods(node) { glog.V(2).Infof("Evicting pods on node %s: %v is later than %v + %v", node.Name, decisionTimestamp, nc.nodeStatusMap[node.Name].readyTransitionTimestamp, nc.podEvictionTimeout) } } if observedReadyCondition.Status == api.ConditionUnknown && decisionTimestamp.After(nc.nodeStatusMap[node.Name].probeTimestamp.Add(nc.podEvictionTimeout)) { if nc.evictPods(node) { glog.V(2).Infof("Evicting pods on node %s: %v is later than %v + %v", node.Name, decisionTimestamp, nc.nodeStatusMap[node.Name].readyTransitionTimestamp, nc.podEvictionTimeout-gracePeriod) } }} Kubelet 发起的驱逐Kubelet 周期性检查本节点的内存和磁盘资源，当可用资源低于阈值时，则按照优先级驱逐 pod，具体检查的资源如下： memory.available nodefs.available nodefs.inodesFree imagefs.available imagefs.inodesFree 以内存资源为例，当内存资源低于阈值时，驱逐的优先级大体为 BestEffort > Burstable > Guaranteed，具体的顺序可能因实际使用量有所调整。当发生驱逐时，kubelet 支持 soft 和 hard 两种模式，soft 模式表示缓期一段时间后驱逐，hard 模式表示立刻驱逐。 落地经验对于 kubelet 发起的驱逐，往往是资源不足导致，它优先驱逐 BestEffort 类型的容器，这些容器多为离线批处理类业务，对可靠性要求低。驱逐后释放资源，减缓节点压力，弃卒保帅，保护了该节点的其它容器。无论是从其设计出发，还是实际使用情况，该特性非常 nice。 对于由 kube-controller-manager 发起的驱逐，效果需要商榷。正常情况下，计算节点周期上报心跳给 master，如果心跳超时，则认为计算节点 NotReady，当 NotReady 状态达到一定时间后，kube-controller-manager 发起驱逐。然而造成心跳超时的场景非常多，例如： 原生 bug：kubelet 进程彻底阻塞 误操作：误把 kubelet 停止 基础设施异常：如交换机故障演练，NTP 异常，DNS 异常 节点故障：硬件损坏，掉电等 从实际情况看，真正因计算节点故障造成心跳超时概率很低，反而由原生 bug，基础设施异常造成心跳超时的概率更大，造成不必要的驱逐。 理想的情况下，驱逐对无状态且设计良好的业务方影响很小。但是并非所有的业务方都是无状态的，也并非所有的业务方都针对 Kubernetes 优化其业务逻辑。例如，对于有状态的业务，如果没有共享存储，异地重建后的 pod 完全丢失原有数据；即使数据不丢失，对于 Mysql 类的应用，如果出现双写，重则破坏数据。对于关心 IP 层的业务，异地重建后的 pod IP 往往会变化，虽然部分业务方可以利用 service 和 dns 来解决问题，但是引入了额外的模块和复杂性。 除非满足如下需求，不然请尽量关闭 kube-controller-manager 的驱逐功能，即把驱逐的超时时间设置非常长，同时把一级／二级驱逐速度设置为 0。否则，非常容易就搞出大大小小的故障，血泪的教训。 业务方要用正确的姿势使用容器，如数据与逻辑分离，无状态化，增强对异常处理等 分布式存储 可靠的 Service／DNS 服务或者保持异地重建后的 IP 不变 通常情况下，从架构设计的角度来说，管理模块的不可用并不会影响已有的实例。例如：OpenStack 计算节点的服务异常，管理节点不会去变动该节点上已经运行虚拟机。但是 kubernetes 的 pod eviction 则打破了这种设计，出发点很新颖，但是落地时，需要充分的考虑业务方的特点，以及业务方的设计是否充分考虑了 kubernetes 的特色。 参考Configure Out Of Resource Handling记一次 k8s 集群单点故障引发的血案容器化RDS——计算存储分离架构下的“Split-Brain”]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git杂项]]></title>
    <url>%2F2019%2F04%2F10%2Fgit%E6%9D%82%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[gitignore文件的获取与配置可以在 GitHub 官网上搜索 gitignore , 有项目维护着各种语言开发所使用的gitingore文件 gitignore 文件 示例 解释 # 此为注释 表示注释, 将被忽略 *.a * 代表所有, 即忽略所有 .a结尾的文件 /todo 仅仅忽略 RootSRC(项目根目录)/todo, 即忽略指定的文件或文件夹(不包括子目录) todo 忽略 todo/ 目录下的所有文件, 包括子目录 doc/*.txt 会忽略 doc/ 目录内的所有.txt文件, 但不包括子目录的 .gitignore只能忽略那些还没有被track的文件, 如果某些文件已经被纳入了版本管理中, 则修改.gitignore是无效的, 解决方法是先把本地缓存删除(改变成未track状态), 然后再提交 123git rm -r --cached .git add . -Agit commit -m 'update .gitignore']]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod生命周期]]></title>
    <url>%2F2019%2F04%2F09%2Fpod%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[1. Pod phasePod的phase是Pod生命周期中的简单宏观描述，定义在Pod的PodStatus对象的phase 字段中。 phase有以下几种值： 状态值 说明 挂起（Pending） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间。 运行中（Running） 该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded） Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed） Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 未知（Unknown） 因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。 2. Pod 状态Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。 PodCondition包含以下以下字段： lastProbeTime：Pod condition最后一次被探测到的时间戳。 lastTransitionTime：Pod最后一次状态转变的时间戳。 message：状态转化的信息，一般为报错信息，例如：containers with unready status: [c-1]。 reason：最后一次状态形成的原因，一般为报错原因，例如：ContainersNotReady。 status：包含的值有 True、False 和 Unknown。 type：Pod状态的几种类型。 其中type字段包含以下几个值： PodScheduled：Pod已经被调度到运行节点。 Ready：Pod已经可以接收请求提供服务。 Initialized：所有的init container已经成功启动。 Unschedulable：无法调度该Pod，例如节点资源不够。 ContainersReady：Pod中的所有容器已准备就绪。 3. 重启策略Pod通过restartPolicy字段指定重启策略，重启策略类型为：Always、OnFailure 和 Never，默认为 Always。 restartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。 重启策略 说明 Always 当容器失效时，由kubelet自动重启该容器 OnFailure 当容器终止运行且退出码不为0时，由kubelet自动重启该容器 Never 不论容器运行状态如何，kubelet都不会重启该容器 说明： 可以管理Pod的控制器有Replication Controller，Job，DaemonSet，及kubelet（静态Pod）。 RC和DaemonSet：必须设置为Always，需要保证该容器持续运行。 Job：OnFailure或Never，确保容器执行完后不再重启。 kubelet：在Pod失效的时候重启它，不论RestartPolicy设置为什么值，并且不会对Pod进行健康检查。 4. Pod的生命Pod的生命周期一般通过Controler 的方式管理，每种Controller都会包含PodTemplate来指明Pod的相关属性，Controller可以自动对pod的异常状态进行重新调度和恢复，除非通过Controller的方式删除其管理的Pod，不然kubernetes始终运行用户预期状态的Pod。 控制器的分类 使用 Job运行预期会终止的 Pod，例如批量计算。Job 仅适用于重启策略为 OnFailure 或 Never 的 Pod。 对预期不会终止的 Pod 使用 ReplicationController、ReplicaSet和 Deployment，例如 Web 服务器。 ReplicationController 仅适用于具有 restartPolicy 为 Always 的 Pod。 提供特定于机器的系统服务，使用 DaemonSet为每台机器运行一个 Pod 。 如果节点死亡或与集群的其余部分断开连接，则 Kubernetes 将应用一个策略将丢失节点上的所有 Pod 的 phase 设置为 Failed。 5. Pod状态转换常见的状态转换 Pod的容器数 Pod当前状态 发生的事件 Pod结果状态 RestartPolicy=Always RestartPolicy=OnFailure RestartPolicy=Never 包含一个容器 Running 容器成功退出 Running Succeeded Succeeded 包含一个容器 Running 容器失败退出 Running Running Failure 包含两个容器 Running 1个容器失败退出 Running Running Running 包含两个容器 Running 容器被OOM杀掉 Running Running Failure 5.1. 容器运行时内存超出限制 容器以失败状态终止。 记录 OOM 事件。 如果restartPolicy为： Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never: 记录失败事件；Pod phase 仍为 Failed。 5.2. 磁盘故障 杀掉所有容器。 记录适当事件。 Pod phase 变成 Failed。 如果使用控制器来运行，Pod 将在别处重建。 5.3. 运行节点挂掉 节点控制器等待直到超时。 节点控制器将 Pod phase 设置为 Failed。 如果是用控制器来运行，Pod 将在别处重建。 参考文章： https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod介绍]]></title>
    <url>%2F2019%2F04%2F08%2Fpod%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1. Pod是什么(what)1.1. Pod概念 Pod是kubernetes集群中最小的部署和管理的基本单元，协同寻址，协同调度。 Pod是一个或多个容器的集合，是一个或一组服务（进程）的抽象集合。 Pod中可以共享网络和存储（可以简单理解为一个逻辑上的虚拟机，但并不是虚拟机）。 Pod被创建后用一个UID来唯一标识，当Pod生命周期结束，被一个等价Pod替代，UID将重新生成。 1.1.1. Pod与Docker Docker是目前Pod最常用的容器环境，但仍支持其他容器环境。 Pod是一组被模块化的拥有共享命名空间和共享存储卷的容器，但并没有共享PID 命名空间（即同个Pod的不同容器中进程的PID是独立的，互相看不到非自己容器的进程）。 1.1.2. Pod中容器的运行方式 只运行一个单独的容器 即one-container-per-Pod模式，是最常用的模式，可以把这样的Pod看成单独的一个容器去管理。 运行多个强关联的容器 即sidecar模式，Pod 封装了一组紧耦合、共享资源、协同寻址的容器，将这组容器作为一个管理单元。 1.2. Pod管理多个容器Pod是一组紧耦合的容器的集合，Pod内的容器作为一个整体以Pod形式进行协同寻址，协同调度、协同管理。相同Pod内的容器共享网络和存储。 1.2.1. 网络 每个Pod被分配了唯一的IP地址，该Pod内的所有容器共享一个网络空间，包括IP和端口。 同个Pod不同容器之间通过localhost通信，Pod内端口不能冲突。 不同Pod之间的通信则通过IP+端口的形式来访问到Pod内的具体服务（容器）。 1.2.2. 存储 可以在Pod中创建共享存储卷的方式来实现不同容器之间数据共享。 2. 为什么需要Pod(why)2.1. 管理需求Pod 是一种模式的抽象：互相协作的多个进程（容器）共同形成一个完整的服务。以一个或多个容器的方式组合成一个整体，作为管理的基本单元，通过Pod可以方便部署、水平扩展，协同调度等。 2.2. 资源共享和通信Pod作为多个紧耦合的容器的集合，通过共享网络和存储的方式来简化紧耦合容器之间的通信，从这个角度，可以将Pod简单理解为一个逻辑上的“虚拟机”。而不同的Pod之间的通信则通过Pod的IP和端口的方式。 2.3. Pod设计的优势 调度器和控制器的可拔插性。 将Pod 的生存期从 controller 中剥离出来，从而减少相互影响。 高可用–在终止和删除 Pod 前，需要提前生成替代 Pod。 集群级别的功能和 Kubelet（Pod Controller） 级别的功能组合更加清晰。 3. Pod的使用(how)Pod一般是通过各种不同类型的Controller对Pod进行管理和控制，包括自我恢复（例如Pod因异常退出，则会再起一个相同的Pod替代该Pod，而该Pod则会被清除）。也可以不通过Controller单独创建一个Pod，但一般很少这么操作，因为这个Pod是一个孤立的实体，并不会被Controller管理。 3.1. ControllerController是kubernetes中用于对Pod进行管理的控制器，通过该控制器让Pod始终维持在一个用户原本设定或期望的状态。如果节点宕机或者Pod因其他原因死亡，则会在其他节点起一个相同的Pod来替代该Pod。 常用的Controller有： Deployment StatefulSet DaemonSet Controller是通过用户提供的Pod模板来创建和控制Pod。 3.2. Pod模板Pod模板用来定义Pod的各种属性，Controller通过Pod模板来生成对应的Pod。 Pod模板类似一个饼干模具，通过模具已经生成的饼干与原模具已经没有关系，即对原模具的修改不会影响已经生成的饼干，只会对通过修改后的模具生成的饼干有影响。这种方式可以更加方便地控制和管理Pod。 4. Pod的终止用户发起一个删除Pod的请求，系统会先发送TERM信号给每个容器的主进程，如果在宽限期（默认30秒）主进程没有自主终止运行，则系统会发送KILL信号给该进程，接着Pod将被删除。 4.1. Pod终止的流程 用户发送一个删除 Pod 的命令， 并使用默认的宽限期（30s)。 把 API server 上的 pod 的时间更新成 Pod 与宽限期一起被认为 “dead” 之外的时间点。 使用客户端的命令，显示出的Pod的状态为 terminating。 （与第3步同时发生）Kubelet 发现某一个 Pod 由于时间超过第2步的设置而被标志成 terminating 状态时， Kubelet 将启动一个停止进程。 如果 pod 已经被定义成一个 preStop hook，这会在 pod 内部进行调用。如果宽限期已经过期但 preStop 锚依然还在运行，将调用第2步并在原来的宽限期上加一个小的时间窗口（2 秒钟）。 把 Pod 里的进程发送到 TERM信号。 （与第3步同时发生），Pod 被从终端的服务列表里移除，同时也不再被 replication controllers 看做时一组运行中的 pods。 在负载均衡（比如说 service proxy）会将它们从轮询中移除前， Pods 这种慢关闭的方式可以继续为流量提供服务。 当宽期限过期时， 任何还在 Pod 里运行的进程都会被 SIGKILL杀掉。 Kubelet 通过在 API server 把宽期限设置成0(立刻删除)的方式完成删除 Pod的过程。 这时 Pod 在 API 里消失，也不再能被用户看到。 4.2. 强制删除Pod强制删除Pod是指从k8s集群状态和Etcd中立刻删除对应的Pod数据，API Server不会等待kubelet的确认信息。被强制删除后，即可重新创建一个相同名字的Pod。 删除默认的宽限期是30秒，通过将宽限期设置为0的方式可以强制删除Pod。 通过kubectl delete 命令后加--force和--grace-period=0的参数强制删除Pod。 1kubectl delete pod --namespace= --force --grace-period=0 4.3. Pod特权模式特权模式是指让Pod中的进程具有访问宿主机系统设备或使用网络栈操作等的能力，例如编写网络插件和卷插件。 通过将container spec中的SecurityContext设置为privileged即将该容器赋予了特权模式。特权模式的使用要求k8s版本高于v1.1。 参考文章： https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/ https://kubernetes.io/docs/concepts/workloads/pods/pod/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod操作]]></title>
    <url>%2F2019%2F04%2F03%2Fpod%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. Pod伸缩k8s中RC的用来保持集群中始终运行指定数目的实例，通过RC的scale机制可以完成Pod的扩容和缩容（伸缩）。 1.1. 手动伸缩（scale）1kubectl scale rc redis-slave --replicas=3 1.2. 自动伸缩（HPA）Horizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。HPA控制器基于Master的kube-controller-manager服务启动参数–horizontal-pod-autoscaler-sync-period定义是时长（默认30秒），周期性监控目标Pod的CPU使用率，并在满足条件时对ReplicationController或Deployment中的Pod副本数进行调整，以符合用户定义的平均Pod CPU使用率。Pod CPU使用率来源于heapster组件，因此需安装该组件。 可以通过kubectl autoscale命令进行快速创建或者使用yaml配置文件进行创建。创建之前需已存在一个RC或Deployment对象，并且该RC或Deployment中的Pod必须定义resources.requests.cpu的资源请求值，以便heapster采集到该Pod的CPU。 1.2.1. 通过kubectl autoscale创建例如： php-apache-rc.yaml 1234567891011121314151617181920apiVersion: v1kind: ReplicationControllermetadata: name: php-apachespec: replicas: 1 template: metadata: name: php-apache labels: app: php-apache spec: containers: - name: php-apache image: gcr.io/google_containers/hpa-example resources: requests: cpu: 200m ports: - containerPort: 80 创建php-apache的RC 1kubectl create -f php-apache-rc.yaml php-apache-svc.yaml 123456789apiVersion: v1kind: Servicemetadata: name: php-apachespec: ports: - port: 80 selector: app: php-apache 创建php-apache的Service 1kubectl create -f php-apache-svc.yaml 创建HPA控制器 1kubectl autoscale rc php-apache --min=1 --max=10 --cpu-percent=50 1.2.2. 通过yaml配置文件创建hpa-php-apache.yaml 123456789101112apiVersion: v1kind: HorizontalPodAutoscalermetadata: name: php-apachespec: scaleTargetRef: apiVersion: v1 kind: ReplicationController name: php-apache minReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 创建hpa 1kubectl create -f hpa-php-apache.yaml 查看hpa 1kubectl get hpa 2. Pod滚动升级k8s中的滚动升级通过执行kubectl rolling-update命令完成，该命令创建一个新的RC（与旧的RC在同一个命名空间中），然后自动控制旧的RC中的Pod副本数逐渐减少为0，同时新的RC中的Pod副本数从0逐渐增加到附加值，但滚动升级中Pod副本数（包括新Pod和旧Pod）保持原预期值。 2.1. 通过配置文件实现redis-master-controller-v2.yaml 1234567891011121314151617181920212223apiVersion: v1kind: ReplicationControllermetadata: name: redis-master-v2 labels: name: redis-master version: v2spec: replicas: 1 selector: name: redis-master version: v2 template: metadata: labels: name: redis-master version: v2 spec: containers: - name: master image: kubeguide/redis-master:2.0 ports: - containerPort: 6371 注意事项： RC的名字（name）不能与旧RC的名字相同 在selector中应至少有一个Label与旧的RC的Label不同，以标识其为新的RC。例如本例中新增了version的Label。 运行kubectl rolling-update 1kubectl rolling-update redis-master -f redis-master-controller-v2.yaml 2.2. 通过kubectl rolling-update命令实现1kubectl rolling-update redis-master --image=redis-master:2.0 与使用配置文件实现不同在于，该执行结果旧的RC被删除，新的RC仍使用旧的RC的名字。 2.3. 升级回滚kubectl rolling-update加参数–rollback实现回滚操作 1kubectl rolling-update redis-master --image=kubeguide/redis-master:2.0 --rollback]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux bridge]]></title>
    <url>%2F2019%2F04%2F02%2FLinux-bridge%2F</url>
    <content type="text"><![CDATA[Linux bridge 网桥祥解Linux网桥是网桥的软件实现，这是Linux内核的内核部分。与硬件网桥相类似，Linux网桥维护了一个2层转发表（也称为MAC学习表，转发数据库，或者仅仅称为FDB），它跟踪记录了MAC地址与端口的对应关系。当一个网桥在端口N收到一个包时（源MAC地址为X），它在FDB中记录为MAC地址X可以从端口N到达。这样的话，以后当网桥需要转发一个包到地址X时，它就可以从FDB查询知道转发到哪里。构建一个FDB常常称之为“MAC学习”或仅仅称为“学习”过程。 你可以使用以下命令来检查Linux网桥当前转发表或MAC学习表。 12sudo brctl showmacs sudo bridge fdb show #查看所有的mac地址对应的信息 Linux]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bridge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s ResourceQuota 资源配额]]></title>
    <url>%2F2019%2F04%2F01%2Fk8s-ResourceQuota%2F</url>
    <content type="text"><![CDATA[资源配额（ResourceQuota）ResourceQuota对象用来定义某个命名空间下所有资源的使用限额，包括： 计算资源的配额 存储资源的配额 对象数量的配额 如果集群的总容量小于命名空间的配额总额，可能会产生资源竞争。这时会按照先到先得来处理。资源竞争和配额的更新都不会影响已经创建好的资源。 1. 启动资源配额Kubernetes 的众多发行版本默认开启了资源配额的支持。当在apiserver的--admission-control配置中添加ResourceQuota参数后，便启用了。 当一个命名空间中含有ResourceQuota对象时，资源配额将强制执行。 2. 计算资源配额可以在给定的命名空间中限制可以请求的计算资源（compute resources）的总量。 资源名称 描述 cpu 非终止态的所有pod, cpu请求总量不能超出此值。 limits.cpu 非终止态的所有pod， cpu限制总量不能超出此值。 limits.memory 非终止态的所有pod, 内存限制总量不能超出此值。 memory 非终止态的所有pod, 内存请求总量不能超出此值。 requests.cpu 非终止态的所有pod, cpu请求总量不能超出此值。 requests.memory 非终止态的所有pod, 内存请求总量不能超出此值。 3. 存储资源配额可以在给定的命名空间中限制可以请求的存储资源（storage resources）的总量。 资源名称 描述 requests.storage 所有PVC, 存储请求总量不能超出此值。 persistentvolumeclaims 命名空间中可以存在的PVC（persistent volume claims）总数。 .storageclass.storage.k8s.io/requests.storage 和该存储类关联的所有PVC, 存储请求总和不能超出此值。 .storageclass.storage.k8s.io/persistentvolumeclaims 和该存储类关联的所有PVC，命名空间中可以存在的PVC（persistent volume claims）总数。 4. 对象数量的配额 资源名称 描述 congfigmaps 命名空间中可以存在的配置映射的总数。 persistentvolumeclaims 命名空间中可以存在的PVC总数。 pods 命名空间中可以存在的非终止态的pod总数。如果一个pod的status.phase 是 Failed, Succeeded, 则该pod处于终止态。 replicationcontrollers 命名空间中可以存在的rc总数。 resourcequotas 命名空间中可以存在的资源配额（resource quotas）总数。 services 命名空间中可以存在的服务总数量。 services.loadbalancers 命名空间中可以存在的服务的负载均衡的总数量。 services.nodeports 命名空间中可以存在的服务的主机接口的总数量。 secrets 命名空间中可以存在的secrets的总数量。 例如：可以定义pod的限额来避免某用户消耗过多的Pod IPs。 5. 限额的作用域 作用域 描述 Terminating 匹配 spec.activeDeadlineSeconds >= 0 的pod NotTerminating 匹配 spec.activeDeadlineSeconds is nil 的pod BestEffort 匹配具有最佳服务质量的pod NotBestEffort 匹配具有非最佳服务质量的pod 6. request和limit当分配计算资源时，每个容器可以为cpu或者内存指定一个请求值和一个限度值。可以配置限额值来限制它们中的任何一个值。如果指定了requests.cpu 或者 requests.memory的限额值，那么就要求传入的每一个容器显式的指定这些资源的请求。如果指定了limits.cpu或者limits.memory，那么就要求传入的每一个容器显式的指定这些资源的限度。 7. 查看和设置配额1234567891011121314151617181920212223242526272829303132333435# 创建namespace$ kubectl create namespace myspace# 创建resourcequota$ cat]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>ResourceQuota</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openssl生成SSL证书的流程]]></title>
    <url>%2F2019%2F03%2F29%2Fopenssl%2F</url>
    <content type="text"><![CDATA[ssl 是什么SSL证书通过在客户端浏览器和Web服务器之间建立一条SSL安全通道（Secure socketlayer(SSL),SSL安全协议主要用来提供对用户和服务器的认证；对传送的数据进行加密和隐藏；确保数据在传送中不被改变，即数据的完整性，现已成为该领域中全球化的标准。由于SSL技术已建立到所有主要的浏览器和WEB服务器程序中，因此，仅需安装服务器证书就可以激活该功能了）。即通过它可以激活SSL协议，实现数据信息在客户端和服务器之间的加密传输，可以防止数据信息的泄露。保证了双方传递信息的安全性，而且用户可以通过服务器证书验证他所访问的网站是否是真实可靠。 SSL网站不同于一般的Web站点，它使用的是“HTTPS”协议，而不是普通的“HTTP”协议。因此它的URL（统一资源定位器）格式为“https://www.flftuu.com” 什么是x509证书链x509证书一般会用到三类文件，key，csr，crt。Key是私用密钥，openssl格式，通常是rsa算法。csr是证书请求文件，用于申请证书。在制作csr文件的时候，必须使用自己的私钥来签署申请，还可以设定一个密钥。crt是CA认证后的证书文件（windows下面的csr，其实是crt），签署人用自己的key给你签署的凭证。 概念首先要有一个CA根证书，然后用CA根证书来签发用户证书。用户进行证书申请：一般先生成一个私钥，然后用私钥生成证书请求(证书请求里应含有公钥信息)，再利用证书服务器的CA根证书来签发证书。特别说明:（1）自签名证书(一般用于顶级证书、根证书): 证书的名称和认证机构的名称相同.（2）根证书：根证书是CA认证中心给自己颁发的证书,是信任链的起始点。任何安装CA根证书的服务器都意味着对这个CA认证中心是信任的。数字证书则是由证书认证机构（CA）对证书申请者真实身份验证之后，用CA的根证书对申请人的一些基本信息以及申请人的公钥进行签名（相当于加盖发证书机构的公章）后形成的一个数字文件。数字证书包含证书中所标识的实体的公钥（就是说你的证书里有你的公钥），由于证书将公钥与特定的个人匹配，并且该证书的真实性由颁发机构保证（就是说可以让大家相信你的证书是真的），因此，数字证书为如何找到用户的公钥并知道它是否有效这一问题提供了解决方案。 openssl中有如下后缀名的文件.key格式：私有的密钥 .csr格式：证书签名请求（证书请求文件），含有公钥信息，certificate signing request的缩写 .crt格式：证书文件，certificate的缩写 .crl格式：证书吊销列表，Certificate Revocation List的缩写 .pem格式：用于导出，导入证书时候的证书的格式，有证书开头，结尾的格式 CA根证书的生成步骤生成CA私钥（.key）–>生成CA证书请求（.csr）–>自签名得到根证书（.crt）（CA给自已颁发的证书） 123456# Generate CA private keyopenssl genrsa -out ca.key 2048# Generate CSRopenssl req -new -key ca.key -out ca.csr# Generate Self Signed certificate（CA 根证书）openssl x509 -req -days 36500 -in ca.csr -signkey ca.key -out ca.crt 在实际的软件开发工作中，往往服务器就采用这种自签名的方式，因为毕竟找第三方签名机构是要给钱的，也是需要花时间的。 用户证书的生成步骤生成私钥（.key）–>生成证书请求（.csr）–>用CA根证书签名得到证书（.crt） 服务器端用户证书： 创建openssl.cnf 12345678910111213[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = ${SERVICE_DNS}DNS.2 = ${SERVICE_DNS2}IP.1 = ${SERVICE_IP}IP.2 = ${SERVICE_IP2} 替换${SERVICE_DNS} 域名为指定域名替换SERVICE_IP 和 SERVICE_IP2 成服务ip地址， 一般设置公网ip 和内网ip 123456# private keyopenssl genrsa -out server.key 2048# generate csropenssl req -new -key server.key -out server.csr -subj "/CN=server" -config openssl.cnf# generate certificateopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 36500 -extensions v3_req -extfile openssl.cnf 客户端用户证书： 123openssl genrsa -out client.key 2048openssl req -new -key client.key -out client.csr -subj "/CN=client"openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 36500 生成pem格式证书： 有时需要用到pem格式的证书，可以用以下方式合并证书文件（crt）和私钥文件（key）来生成 12$cat client.crt client.key> client.pem$cat server.crt server.key > server.pem 结果：服务端证书：ca.crt, server.key, server.crt, server.pem客户端证书：ca.crt, client.key, client.crt, client.pem]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx虚拟主机+域名重定向]]></title>
    <url>%2F2019%2F03%2F29%2Fnginx%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA-%E5%9F%9F%E5%90%8D%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[域名重定向重定向flftuu.site -> flftuu.com 12345server { listen 80; server_name flftuu.site, www.flftuu.site; rewrite "^/(.*)$" https://www.flftuu.com/$1 permanent;} nginx 虚拟机主机http 选项中包含多个 serverserver 中server_name 设置不通域名实现多虚拟机功能 1234567891011121314151617181920server {# listen 80 default_server;# listen [::]:80 default_server;# server_name _;# root /usr/share/nginx/html;## # Load configuration files for the default server block.# include /etc/nginx/default.d/*.conf;## location / {# }## error_page 404 /404.html;# location = /40x.html {# }## error_page 500 502 503 504 /50x.html;# location = /50x.html {# }} nginx 配置详解在了解具体的Nginx配置项之前我们需要对于Nginx配置文件的构成有所概念，一般来说，Nginx配置文件会由如下几个部分构成： 123456789101112131415161718192021222324252627282930313233# 全局块... # events块events { ...}# http块http { # http全局块 ... # 虚拟主机server块 server { # server全局块 ... # location块 location [PATTERN] { ... } location [PATTERN] { ... } } server { ... } # http全局块 ... } 在上述配置中我们可以看出，Nginx配置文件由以下几个部分构成： 全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。 events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。 http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。 server块：配置虚拟主机的相关参数，一个http中可以有多个server。 location块：配置请求的路由，以及各种页面的处理情况。 123456789101112131415161718192021222324252627282930313233343536373839404142########### 每个指令必须有分号结束。##################user administrator administrators; #配置用户或者组，默认为nobody nobody。#worker_processes 2; #允许生成的进程数，默认为1#pid /nginx/pid/nginx.pid; #指定nginx进程运行文件存放地址error_log log/error.log debug; #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emergevents { accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off #use epoll; #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport worker_connections 1024; #最大连接数，默认为512}http { include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型，默认为text/plain #access_log off; #取消服务日志 log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式 access_log log/access.log myFormat; #combined为日志格式的默认值 sendfile on; #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 keepalive_timeout 65; #连接超时时间，默认为75s，可以在http，server，location块。 # 定义常量 upstream mysvr { server 127.0.0.1:7878; server 192.168.10.121:3333 backup; #热备 } error_page 404 https://www.baidu.com; #错误页 #定义某个负载均衡服务器 server { keepalive_requests 120; #单连接请求上限次数。 listen 4545; #监听端口 server_name 127.0.0.1; #监听地址 location ~*^.+$ { #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。 #root path; #根目录 #index vv.txt; #设置默认页 proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip } }} 虚拟主机与静态站点官方文档 本部分概述如何配置Nginx进行静态内容服务，Nginx的静态内容分发能力还是非常强大的。 1234567891011121314151617181920http { server { listen 80; server_name www.domain1.com; access_log logs/domain1.access.log main; location / { index index.html; root /var/www/domain1.com/htdocs; } } server { listen 80; server_name www.domain2.com; access_log logs/domain2.access.log main; location / { index index.html; root /var/www/domain2.com/htdocs; } }} 虚拟主机配置详解主机与端口123456# 支持多域名配置server_name www.barretlee.com barretlee.com;# 支持泛域名解析server_name *.barretlee.com;# 支持对于域名的正则匹配server_name ~^\.barret\.com$; URI匹配12345678910111213location = / { # 完全匹配 = # 大小写敏感 ~ # 忽略大小写 ~*}location ^~ /images/ { # 前半部分匹配 ^~ # 可以使用正则，如： # location ~* \.(gif|jpg|png)$ { }}location / { # 如果以上都未匹配，会进入这里} 文件路径配置根目录123location / { root /home/barret/test/;} 别名12345678location /blog { alias /home/barret/www/blog/;}location ~ ^/blog/(\d+)/([\w-]+)$ { # /blog/20141202/article-name # -> /blog/20141202-article-name.md alias /home/barret/www/blog/$1-$2.md;} 首页1index /html/index.html /php/index.php; 重定向页面12345678910error_page 404 /404.html;error_page 502 503 /50x.html;error_page 404 =200 /1x1.gif;location / { error_page 404 @fallback;}location @fallback { # 将请求反向代理到上游服务器处理 proxy_pass http://localhost:9000;} try_files123456789try_files $uri $uri.html $uri/index.html @other;location @other { # 尝试寻找匹配 uri 的文件，失败了就会转到上游处理 proxy_pass http://localhost:9000;}location / { # 尝试寻找匹配 uri 的文件，没找到直接返回 502 try_files $uri $uri.html =502;} 缓存配置缓存 Expire:过期时间在Nginx中可以配置缓存的过期时间： 12345location ~* \.(?:ico|css|js|gif|jpe?g|png)$ { expires 30d; add_header Vary Accept-Encoding; access_log off; } 我们也可以添加更复杂的配置项： 12345678910111213location ~* ^.+\.(?:css|cur|js|jpe?g|gif|htc|ico|png|html|xml|otf|ttf|eot|woff|svg)$ { access_log off; expires 30d; ## No need to bleed constant updates. Send the all shebang in one ## fell swoop. tcp_nodelay off; ## Set the OS file cache. open_file_cache max=3000 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; } 反向代理123456789101112131415161718192021222324events{}http{ upstream ggzy { server 127.0.0.1:1398 weight=3; server 127.0.0.1:1399; } # 80端口配置，可配置多个Virtual Host server { listen 80; index index index.htm index.py index.html; server_name app.truelore.cn; location / { proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http//ggzy; } }} NodeJS Application1234const http = require('http');http.createServer((req, res) => { res.end('hello world');}).listen(9000); 任何请求过来都返回 hello world，简版的 Nginx 配置如下， 12345678910111213141516171819202122events { # 这里可不写东西 use epoll;}http { server { listen 127.0.0.1:8888; # 如果请求路径跟文件路径按照如下方式匹配找到了，直接返回 try_files $uri $uri/index.html; location ~* ^/(js|css|image|font)/$ { # 静态资源都在 static 文件夹下 root /home/barret/www/static/; } location /app { # Node.js 在 9000 开了一个监听端口 proxy_pass http://127.0.0.1:9000; } # 上面处理出错或者未找到的，返回对应状态码文件 error_page 404 /404.html; error_page 502 503 504 /50x.html; }} 首先 try_files，尝试直接匹配文件；没找到就匹配静态资源；还没找到就交给 Node 处理；否则就返回 4xx/5xx 的状态码。 Upstream CacheA Guide to Caching with NGINX and NGINX Plus 123456789101112131415161718http { ,,,,, proxy_cache_path /var/cache/nginx/cache levels=1:2 keys_zone=imgcache:100m inactive=1d max_size=10g; server { ........ location ~* ^.+\.(js|ico|gif|jpg|jpeg|png|html|htm)$ { log_not_found off; access_log off; expires 7d; proxy_pass http://img.example.com ; proxy_cache imgcache; proxy_cache_valid 200 302 1d; proxy_cache_valid 404 10m; proxy_cache_valid any 1h; proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504; } }} HTTPSHTTPS 理论详解与实践 https证书生成方式 基本HTTPS配置基本的HTTPS支持配置如下: 12345678910server { listen 192.168.1.11:443; #ssl端口 server_name test.com; #为一个server{......}开启ssl支持 ssl on; #指定PEM格式的证书文件 ssl_certificate /etc/nginx/test.pem; #指定PEM格式的私钥文件 ssl_certificate_key /etc/nginx/test.key; } 在真实的生产环境中，我们的配置如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445server { # 如果需要spdy也可以加上,lnmp1.2及其后版本都默认支持spdy,lnmp1.3 nginx 1.9.5以上版本默认支持http2 listen 443 ssl; # 这里是你的域名 server_name www.vpser.net; index index.html index.htm index.php default.html default.htm default.php; # 网站目录 root /home/wwwroot/www.vpser.net; # 前面生成的证书，改一下里面的域名就行 ssl_certificate /etc/letsencrypt/live/www.vpser.net/fullchain.pem; # 前面生成的密钥，改一下里面的域名就行 ssl_certificate_key /etc/letsencrypt/live/www.vpser.net/privkey.pem; ssl_ciphers "EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5"; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; #这个是伪静态根据自己的需求改成其他或删除 include wordpress.conf; #error_page 404 /404.html; location ~ [^/]\.php(/|$) { # comment try_files $uri =404; to enable pathinfo try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_index index.php; # lnmp 1.0及之前版本替换为include fcgi.conf; include fastcgi.conf; #include pathinfo.conf; } location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$ { expires 30d; } location ~ .*\.(js|css)?$ { expires 12h; } access_log off;} 强制HTTP转到HTTPSNginx Rewrite 12345server { listen 192.168.1.111:80; server_name test.com; rewrite ^(.*)$ https://$host$1 permanent; } Nginx 497错误码利用error_page命令将497状态码的链接重定向到https://test.com这个域名上 1234567891011121314server { listen 192.168.1.11:443; #ssl端口 listen 192.168.1.11:80; #用户习惯用http访问，加上80，后面通过497状态码让它自动跳到443端口 server_name test.com; #为一个server{......}开启ssl支持 ssl on; #指定PEM格式的证书文件 ssl_certificate /etc/nginx/test.pem; #指定PEM格式的私钥文件 ssl_certificate_key /etc/nginx/test.key; #让http请求重定向到https请求 error_page 497 https://$host$uri?$args; } Meta刷新，前端跳转在HTTP正常返回的页面中添加meta属性： 123 1234567891011server { listen 192.168.1.11:80; server_name test.com; location / { #index.html放在虚拟主机监听的根目录下 root /srv/www/http.test.com/; } #将404的页面重定向到https的首页 error_page 404 https://test.com/; }]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go微服务框架go-micro]]></title>
    <url>%2F2019%2F03%2F28%2Fgo-micro%2F</url>
    <content type="text"><![CDATA[微服务架构微服务化项目除了稳定性比较关心的几个问题： 服务间数据传输的效率和安全性。 服务的动态扩充，也就是服务的注册和发现，服务集群化。 微服务功能的可订制化，因为并不是所有的功能都会很符合你的需求，难免需要根据自己的需要二次开发一些功能。 go-micro是go语言下的一个很好的rpc微服务框架，功能很完善： 服务间传输格式为protobuf，效率上没的说，非常的快，也很安全。 go-micro的服务注册和发现是多种多样的。我个人比较喜欢etcdv3的服务服务发现和注册。 主要的功能都有相应的接口，只要实现相应的接口，就可以根据自己的需要订制插件。 通信流程go-micro的通信流程大至如下 Server监听客户端的调用，和Brocker推送过来的信息进行处理。并且Server端需要向Register注册自己的存在或消亡，这样Client才能知道自己的状态。 Register服务的注册的发现。 Client端从Register中得到Server的信息，然后每次调用都根据算法选择一个的Server进行通信，当然通信是要经过编码/解码，选择传输协议等一系列过程的。 Brocker 信息队列进行信息的接收和发布。 go-micro之所以可以高度订制和他的框架结构是分不开的，go-micro由8个关键的interface组成，每一个interface都可以根据自己的需求重新实现，这8个主要的inteface也构成了go-micro的框架结构。 这些接口go-micir都有他自己默认的实现方式，还有一个go-plugins是对这些接口实现的可替换项。你也可以根据需求实现自己的插件。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>go-micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker hub加速器配置 + gcr.io镜像缓存]]></title>
    <url>%2F2019%2F03%2F27%2Fdocker-hub%2F</url>
    <content type="text"><![CDATA[Registry mirror原理Docker Hub的镜像数据分为两部分：index数据和registry数据。前者保存了镜像的一些元数据信息，数据量很小；后者保存了镜像的实际数据，数据量比较大。平时我们使用docker pull命令拉取一个镜像时的过程是：先去index获取镜像的一些元数据，然后再去registry获取镜像数据。 所谓registry mirror就是搭建一个registry，然后将docker hub的registry数据缓存到自己本地的registry。整个过程是：当我们使用docker pull去拉镜像的时候，会先从我们本地的registry mirror去获取镜像数据，如果不存在，registry mirror会先从docker hub的registry拉取数据进行缓存，再传给我们。而且整个过程是流式的，registry mirror并不会等全部缓存完再给我们传，而且边缓存边给客户端传。 对于缓存，我们都知道一致性非常重要。registry mirror与docker官方保持一致的方法是：registry mirror只是缓存了docker hub的registry数据，并不缓存index数据。所以我们pull镜像的时候会先连docker hub的index获取镜像的元数据，如果我们registry mirror里面有该镜像的缓存，且数据与从index处获取到的元数据一致，则从registry mirror拉取；如果我们的registry mirror有该镜像的缓存，但数据与index处获取的元数据不一致，或者根本就没有该镜像的缓存，则先从docker hub的registry缓存或者更新数据。 registry mirror部署 从官方拉取registry的镜像， dockerhub官网 获取registry的默认配置：官方配置文件 1234567891011121314151617181920212223242526272829docker run -it --rm --entrypoint cat registry:2.7.1 /etc/docker/registry/config.yml > config.yml# 文件的内容大概是下面这样：version: 0.1log: fields: service: registrystorage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registryhttp: addr: :5000 headers: X-Content-Type-Options: [nosniff]health: storagedriver: enabled: true interval: 10s threshold: 3# 我们在最后面加上如下配置：proxy: remoteurl: https://registry-1.docker.io username: [username] password: [password]# username和password是可选的，如果配置了的话，那registry mirror除了可以缓存所有的公共镜像外，也可以访问这个用户所有的私有镜像。 启动registry容器： 123456789101112131415161718docker run --restart=always -p 5000:5000 --name v2-mirror -v /data:/var/lib/registry -v $PWD/config.yml:/etc/registry/config.yml registry:2.5.0 /etc/registry/config.yml# 当我们看到如下日志输出的时候就说明已经启动成功了：time="2016-12-19T14:22:35Z" level=warning msg="No HTTP secret provided - generated random secret. This may cause problems with uploads if multiple registries are behind a load-balancer. To provide a shared secret, fill in http.secret in the configuration file or set the REGISTRY_HTTP_SECRET environment variable." go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0time="2016-12-19T14:22:35Z" level=info msg="redis not configured" go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0time="2016-12-19T14:22:35Z" level=info msg="Starting upload purge in 39m0s" go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0time="2016-12-19T14:22:35Z" level=info msg="using inmemory blob descriptor cache" go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0time="2016-12-19T14:22:35Z" level=info msg="Starting cached object TTL expiration scheduler..." go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0time="2016-12-19T14:22:35Z" level=info msg="Registry configured as a proxy cache to https://registry-1.docker.io" go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0time="2016-12-19T14:22:35Z" level=info msg="listening on [::]:5000" go.version=go1.6.3 instance.id=da5468c4-1ee1-4df2-95cf-1336127c87bb version=v2.5.0# 至此，registrymirror就算部署完了。我们也可以用curl验证一下服务是否启动OK：curl -I http://registrycache.example.com:5000/v2/HTTP/1.1 200 OKContent-Length: 2Content-Type: application/json; charset=utf-8Docker-Distribution-Api-Version: registry/2.0Date: Thu, 17 Sep 2015 21:42:02 GMT registry mirror使用要使用registry mirror，我们需要配置一下自己的docker daemon。 对于Mac：在docker的客户端的Preferences——>Advanced——>Registry mirrors里面添加你的地址，然后重启。 对于Ubuntu 14.04：在/etc/default/docker文件中添加DOCKER_OPTS=”$DOCKER_OPTS –registry-mirror=http://registrycache.example.com:5000”，然后重启docker（services docker restart）。 对于Centos7：/etc/docker/daemon.json 中的registry-mirrors 12345678910{ "registry-mirrors": [ "http://mirror.kce.sdns.ksyun.com" ], "disable-legacy-registry": false, "graph": "/home/docker", "insecure-registries": [ "mirror.kce.sdns.ksyun.com" ]} 然后我们pull一个本地不存在的镜像，这时去查看registry mirror服务器的data目录下面已经有了数据。执行如下命令也可以看到效果： 1curl https://mycache.example.com:5000/v2/library/busybox/tags/list 需要说明的是缓存的镜像的有效期默认是一周（168hour），而且如果registry被配置成mirror模式，这个时间是不能通过maintenance部分来改变的： 12345678maintenance: uploadpurging: enabled: true age: 168h interval: 24h dryrun: false readonly: enabled: false 我研究了好久发现怎么改都不能生效，最后发现mirror模式下这个时间竟然在registry的代码里面写死了： 12// todo(richardscothern): from cache control header or configconst repositoryTTL = time.Duration(24 * 7 * time.Hour) 当然如果你想你的registry mirror是https的话，在config.yml的http部分增加tls配置即可： 1234567http: addr: :5000 headers: X-Content-Type-Options: [nosniff] tls: certificate: /etc/registry/domain.crt key: /etc/registry/domain.key gcr.io镜像缓存dockerd 开启远程连接功能远程连接docker daemon，Docker Remote API docker 版本： Docker version 18.09.5, build e8ff056 操作系统： centos7.5 12345678# 编辑启动文件vim /usr/lib/systemd/system/docker.service#加入 -H tcp://0.0.0.0:2376 ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376 --containerd=/run/containerd/containerd.socksystemctl daemon-reload systemctl start docker.service 注：同样这里也是不安全的。 线上环境，安全环境这里介绍，通过自签名证书安全认证构建HTTPS encypted socket。docker推荐2376作为安全端口。当然我们可以随意设置端口，哈哈。 证书的生成： 详细信息，移步官网：Protect the Docker daemon socket。其原理是通过指定tlsverify标志并将Docker的tlscacert标志指向受信任的CA证书来启用TLS。在守护进程模式下，它只允许来自由该CA签名的证书认证的客户端的连接。 在客户端模式下，它将只连接到具有由该CA签名的证书的服务器。 首先，生成CA公钥和私钥： 123456789101112131415161718192021222324$ openssl genrsa -aes256 -out ca-key.pem 4096 # 生成CA私钥Generating RSA private key, 4096 bit long modulus............................................................................................................................................................................................++........++e is 65537 (0x10001)Enter pass phrase for ca-key.pem:Verifying - Enter pass phrase for ca-key.pem:$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem #生成CA公钥，也就是证书Enter pass phrase for ca-key.pem:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:State or Province Name (full name) [Some-State]:QueenslandLocality Name (eg, city) []:BrisbaneOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Docker IncOrganizational Unit Name (eg, section) []:SalesCommon Name (e.g. server FQDN or YOUR name) []:$HOSTEmail Address []:Sven@home.org.au 现在我们有了CA，就可以创建服务器私钥和证书请求文件了，请确保Common Name (i.e., server FQDN or YOUR name)匹配你将要连接的docker主机。 注意，使用你docker宿主机的DNS name替换下面的$HOST 123456$ openssl genrsa -out server-key.pem 4096 # 生成服务器私钥Generating RSA private key, 4096 bit long modulus.....................................................................++.................................................................................................++e is 65537 (0x10001)$ openssl req -subj "/CN=$HOST" -sha256 -new -key server-key.pem -out server.csr # 用私钥生成证书请求文件 现在，我们可以用CA来签署证书了。这里我们可以填写IP地址或则DNS name，如，我们需要允许10.10.10.20和127.0.0.1连接： 123456789101112$ echo subjectAltName = IP:10.10.10.20,IP:127.0.0.1 > extfile.cnf# 将Docker守护程序密钥的扩展使用属性设置为仅用于服务器身份验证：$ echo extendedKeyUsage = serverAuth >> extfile.cnf$ openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \ -CAcreateserial -out server-cert.pem -extfile extfile.cnfSignature oksubject=/CN=your.host.comGetting CA Private KeyEnter pass phrase for ca-key.pem: 客户端证书： 123456$ openssl genrsa -out key.pem 4096 # 客户端私钥Generating RSA private key, 4096 bit long modulus.........................................................++................++e is 65537 (0x10001)$ openssl req -subj '/CN=client' -new -key key.pem -out client.csr # 客户端证书请求文件 用CA为客户端签署证书文件： 123456789# 要使密钥适配客户端身份验证，请创建扩展配置文件：$ echo extendedKeyUsage = clientAuth >> extfile.cnf$ openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \ -CAcreateserial -out cert.pem -extfile extfile.cnfSignature oksubject=/CN=clientGetting CA Private KeyEnter pass phrase for ca-key.pem: 删除证书请求文件： 1rm -v client.csr server.csr 默认的私钥权限太开放了，为了更加的安全，我们需要更改证书的权限，删除写入权限，限制阅读权限（只有你能查看）： 1chmod -v 0400 ca-key.pem key.pem server-key.pem 证书文件删除其写入权限： 1chmod -v 0444 ca.pem server-cert.pem cert.pem 证书的部署： 123456789101112# daemon.json$ sudo vi /etc/docker/daemon.json{ "tlsverify": true, "tlscert": "/etc/docker/server-cert.pem", "tlskey": "/etc/docker/server-key.pem", "tlscacert": "/etc/docker/ca.pem", "registry-mirrors": [ ], "insecure-registries": [ ]} 警告：这些证书的保存非常重要，关系着你的服务器的安全，请妥善保管。 客户端连接普通连接： 123$ docker -H tcp://127.0.0.1:2375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3ed7b8f338ad mongo:3.2 "/entrypoint.sh mo..." 11 days ago Up 3 hours 0.0.0.0:27017->27017/tcp eidb TLS连接： 1234567891011121314151617181920212223242526272829$ docker --tlsverify --tlscacert=~/docker/ca.pem \ --tlscert=~/docker/cert.pem \ --tlskey=~/docker/key.pem \ -H=192.168.99.100:2376 version Client: Version: 1.13.0-rc1 API version: 1.25 Go version: go1.7.3 Git commit: 75fd88b Built: Fri Nov 11 22:32:34 2016 OS/Arch: darwin/amd64Server: Version: 1.13.0-rc1 API version: 1.25 Minimum API version: 1.12 Go version: go1.7.3 Git commit: 75fd88b Built: Fri Nov 11 22:32:34 2016 OS/Arch: linux/amd64 Experimental: false # curl 连接测试$ curl https://192.168.99.100:2376/images/json \ --cert ~/.docker/cert.pem \ --key ~/.docker/key.pem \ --cacert ~/.docker/ca.pem 为了不每次都指定证书，我们可以指定默认连接： 1234567$ mkdir -pv ~/.docker$ cp -v ~/{ca,cert,key}.pem ~/.docker$ export DOCKER_HOST=tcp://192.168.99.100:2376 DOCKER_TLS_VERIFY=1 # 这里只是临时指定，永久请写入*profile里面$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3ed7b8f338ad mongo:3.2 "/entrypoint.sh mo..." 11 days ago Up 3 hours 0.0.0.0:27017->27017/tcp eidb 当然也可以指定证书文件路径： 1export DOCKER_CERT_PATH="/etc/docker" gcrsync 项目架构介绍 gcrsync 使用方法1docker run --rm -d --name google-containers -v $PWD/etc/:/usr/local/etc/ -v /etc/docker/:/etc/docker/ flftuu/gcrsync:v0.0.1 monitor --sync --namespace google-containers 参数注解 -v $PWD/etc/:/usr/local/etc/ 挂载当前目录etc目录 etc 目录下必须包括: docker.json 文件 存储gcr镜像的hub服务 可支持（docker hub， vmware， registerv2) gcr.json gcr.io 仓库配置信息 可以设置本地代理proxy git.json git 项目配置信息 -v /etc/docker/:/etc/docker/ 挂载docker client 配置目录 docker 目录必须包括： ca.pem ，cert.pem， key.pem 文件 monitor 开启监控功能 –sync 开启同步功能 –namespace 指定gcr中的要同步的namespace 默认为google-containers 配置文件模板docker.json 12345678910111213{ "docker_api_version": "1.39", "docker_host": "tcp://10.0.0.14:2376", "docker_tls_verify": "true", "docker_cert_path": "/etc/docker", "backend": { "type": "vmware", "addr": "https://gcr.flftuu.com", "user": "用户名", "password": "密码", "insecure": true }} git.json 12345678{ "github_token": "zwei:pass", "github_repo": "zwei/gcrsync-change-log.git", "git_email": "zwei@flftuu.com", "git_user": "zwei", "git_addr": "http://gitlab.com/", "commit_msg": "GcrSync Auto Synchronized."} gcr.json 1234{ "proxy": "", //开启本地代理 http://127.0.0.1:1080 "http_timeout": 30} ca.pem ，cert.pem， key.pem 文件根据dockerd 服务开启远程功能时候使用的证书而定]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>hub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kube-Scheduler简介]]></title>
    <url>%2F2019%2F03%2F27%2FKube-Scheduler%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1. Scheduler简介Scheduler负责Pod调度。在整个系统中起”承上启下”作用，承上：负责接收Controller Manager创建的新的Pod，为其选择一个合适的Node；启下：Node上的kubelet接管Pod的生命周期。 Scheduler： 1）通过调度算法为待调度Pod列表的每个Pod从Node列表中选择一个最适合的Node，并将信息写入etcd中 2）kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载Image，并启动容器。 2. 调度流程1、预选调度过程，即遍历所有目标Node,筛选出符合要求的候选节点，kubernetes内置了多种预选策略（xxx Predicates）供用户选择 2、确定最优节点，在第一步的基础上采用优选策略（xxx Priority）计算出每个候选节点的积分，取最高积分。 调度流程通过插件式加载的“调度算法提供者”（AlgorithmProvider）具体实现，一个调度算法提供者就是包括一组预选策略与一组优选策略的结构体。 3. 预选策略说明：返回true表示该节点满足该Pod的调度条件；返回false表示该节点不满足该Pod的调度条件。 3.1. NoDiskConflict判断备选Pod的数据卷是否与该Node上已存在Pod挂载的数据卷冲突，如果是则返回false，否则返回true。 3.2. PodFitsResources判断备选节点的资源是否满足备选Pod的需求，即节点的剩余资源满不满足该Pod的资源使用。 计算备选Pod和节点中已用资源（该节点所有Pod的使用资源）的总和。 获取备选节点的状态信息，包括节点资源信息。 如果（备选Pod+节点已用资源>该节点总资源）则返回false，即剩余资源不满足该Pod使用；否则返回true。 3.3. PodSelectorMatches判断节点是否包含备选Pod的标签选择器指定的标签，即通过标签来选择Node。 如果Pod中没有指定spec.nodeSelector，则返回true。 否则获得备选节点的标签信息，判断该节点的标签信息中是否包含该Pod的spec.nodeSelector中指定的标签，如果包含返回true，否则返回false。 3.4. PodFitsHost判断备选Pod的spec.nodeName所指定的节点名称与备选节点名称是否一致，如果一致返回true，否则返回false。 3.5. CheckNodeLabelPresence检查备选节点中是否有Scheduler配置的标签，如果有返回true，否则返回false。 3.6. CheckServiceAffinity判断备选节点是否包含Scheduler配置的标签，如果有返回true，否则返回false。 3.7. PodFitsPorts判断备选Pod所用的端口列表中的端口是否在备选节点中已被占用，如果被占用返回false，否则返回true。 4. 优选策略4.1. LeastRequestedPriority优先从备选节点列表中选择资源消耗最小的节点（CPU+内存）。 4.2. CalculateNodeLabelPriority优先选择含有指定Label的节点。 4.3. BalancedResourceAllocation优先从备选节点列表中选择各项资源使用率最均衡的节点。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 安装Docker-CE]]></title>
    <url>%2F2019%2F03%2F26%2Fdocker-install%2F</url>
    <content type="text"><![CDATA[1 卸载老版本Docker1234sudo yum remove docker \ docker-common \ docker-selinux \ docker-engine 2 设置仓库123sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 123sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo docker 3 安装docker1sudo yum install docker-ce 4 启动Docker1sudo systemctl start docker 5 卸载Docker CE12sudo yum remove docker-cesudo rm -rf /var/lib/docker]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Controller Manager简介]]></title>
    <url>%2F2019%2F03%2F25%2FController-Manager%2F</url>
    <content type="text"><![CDATA[1. Kubernetes Controller Manager简介Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。 每个Controller通过API Server提供的接口实时监控整个集群的每个资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将系统状态修复到“期望状态”。 2. Replication Controller为了区分，将资源对象Replication Controller简称RC,而本文中是指Controller Manager中的Replication Controller，称为副本控制器。副本控制器的作用即保证集群中一个RC所关联的Pod副本数始终保持预设值。 只有当Pod的重启策略是Always的时候（RestartPolicy=Always），副本控制器才会管理该Pod的操作（创建、销毁、重启等）。 RC中的Pod模板就像一个模具，模具制造出来的东西一旦离开模具，它们之间就再没关系了。一旦Pod被创建，无论模板如何变化，也不会影响到已经创建的Pod。 Pod可以通过修改label来脱离RC的管控，该方法可以用于将Pod从集群中迁移，数据修复等调试。 删除一个RC不会影响它所创建的Pod，如果要删除Pod需要将RC的副本数属性设置为0。 不要越过RC创建Pod，因为RC可以实现自动化控制Pod，提高容灾能力。 2.1. Replication Controller的职责 确保集群中有且仅有N个Pod实例，N是RC中定义的Pod副本数量。 通过调整RC中的spec.replicas属性值来实现系统扩容或缩容。 通过改变RC中的Pod模板来实现系统的滚动升级。 2.2. Replication Controller使用场景 使用场景 说明 使用命令 重新调度 当发生节点故障或Pod被意外终止运行时，可以重新调度保证集群中仍然运行指定的副本数。 弹性伸缩 通过手动或自动扩容代理修复副本控制器的spec.replicas属性，可以实现弹性伸缩。 kubectl scale 滚动更新 创建一个新的RC文件，通过kubectl 命令或API执行，则会新增一个新的副本同时删除旧的副本，当旧副本为0时，删除旧的RC。 kubectl rolling-update 滚动升级，具体可参考kubectl rolling-update –help,官方文档：https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/ 3. Node Controllerkubelet在启动时会通过API Server注册自身的节点信息，并定时向API Server汇报状态信息，API Server接收到信息后将信息更新到etcd中。 Node Controller通过API Server实时获取Node的相关信息，实现管理和监控集群中的各个Node节点的相关控制功能。流程如下 1、Controller Manager在启动时如果设置了–cluster-cidr参数，那么为每个没有设置Spec.PodCIDR的Node节点生成一个CIDR地址，并用该CIDR地址设置节点的Spec.PodCIDR属性，防止不同的节点的CIDR地址发生冲突。 2、具体流程见以上流程图。 3、逐个读取节点信息，如果节点状态变成非“就绪”状态，则将节点加入待删除队列，否则将节点从该队列删除。 4. ResourceQuota Controller资源配额管理确保指定的资源对象在任何时候都不会超量占用系统物理资源。 支持三个层次的资源配置管理： 1）容器级别：对CPU和Memory进行限制 2）Pod级别：对一个Pod内所有容器的可用资源进行限制 3）Namespace级别：包括 Pod数量 Replication Controller数量 Service数量 ResourceQuota数量 Secret数量 可持有的PV（Persistent Volume）数量 说明： k8s配额管理是通过Admission Control（准入控制）来控制的； Admission Control提供两种配额约束方式：LimitRanger和ResourceQuota； LimitRanger作用于Pod和Container； ResourceQuota作用于Namespace上，限定一个Namespace里的各类资源的使用总额。 ResourceQuota Controller流程图： 5. Namespace Controller用户通过API Server可以创建新的Namespace并保存在etcd中，Namespace Controller定时通过API Server读取这些Namespace信息。 如果Namespace被API标记为优雅删除（即设置删除期限，DeletionTimestamp）,则将该Namespace状态设置为“Terminating”,并保存到etcd中。同时Namespace Controller删除该Namespace下的ServiceAccount、RC、Pod等资源对象。 6. Endpoint ControllerService、Endpoint、Pod的关系： Endpoints表示了一个Service对应的所有Pod副本的访问地址，而Endpoints Controller负责生成和维护所有Endpoints对象的控制器。它负责监听Service和对应的Pod副本的变化。 如果监测到Service被删除，则删除和该Service同名的Endpoints对象； 如果监测到新的Service被创建或修改，则根据该Service信息获得相关的Pod列表，然后创建或更新Service对应的Endpoints对象。 如果监测到Pod的事件，则更新它对应的Service的Endpoints对象。 kube-proxy进程获取每个Service的Endpoints，实现Service的负载均衡功能。 7. Service ControllerService Controller是属于kubernetes集群与外部的云平台之间的一个接口控制器。Service Controller监听Service变化，如果是一个LoadBalancer类型的Service，则确保外部的云平台上对该Service对应的LoadBalancer实例被相应地创建、删除及更新路由转发表。 Controller ManagerController Manager 由 kube-controller-manager 和 cloud-controller-manager 组成，是 Kubernetes 的大脑，它通过 apiserver 监控整个集群的状态，并确保集群处于预期的工作状态。 kube-controller-manager 由一系列的控制器组成 Replication Controller Node Controller CronJob Controller Daemon Controller Deployment Controller Endpoint Controller Garbage Collector Namespace Controller Job Controller Pod AutoScaler RelicaSet Service Controller ServiceAccount Controller StatefulSet Controller Volume Controller Resource quota Controller cloud-controller-manager 在 Kubernetes 启用 Cloud Provider 的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器，如 Node Controller Route Controller Service Controller 从 v1.6 开始，cloud provider 已经经历了几次重大重构，以便在不修改 Kubernetes 核心代码的同时构建自定义的云服务商支持。参考 这里 查看如何为云提供商构建新的 Cloud Provider。 MetricsController manager metrics 提供了控制器内部逻辑的性能度量，如 Go 语言运行时度量、etcd 请求延时、云服务商 API 请求延时、云存储请求延时等。Controller manager metrics 默认监听在 kube-controller-manager 的 10252 端口，提供 Prometheus 格式的性能度量数据，可以通过 http://localhost:10252/metrics 来访问。 1234567891011121314151617$ curl http://localhost:10252/metrics...# HELP etcd_request_cache_add_latencies_summary Latency in microseconds of adding an object to etcd cache# TYPE etcd_request_cache_add_latencies_summary summaryetcd_request_cache_add_latencies_summary{quantile="0.5"} NaNetcd_request_cache_add_latencies_summary{quantile="0.9"} NaNetcd_request_cache_add_latencies_summary{quantile="0.99"} NaNetcd_request_cache_add_latencies_summary_sum 0etcd_request_cache_add_latencies_summary_count 0# HELP etcd_request_cache_get_latencies_summary Latency in microseconds of getting an object from etcd cache# TYPE etcd_request_cache_get_latencies_summary summaryetcd_request_cache_get_latencies_summary{quantile="0.5"} NaNetcd_request_cache_get_latencies_summary{quantile="0.9"} NaNetcd_request_cache_get_latencies_summary{quantile="0.99"} NaNetcd_request_cache_get_latencies_summary_sum 0etcd_request_cache_get_latencies_summary_count 0... kube-controller-manager 启动示例123456789101112131415161718kube-controller-manager \ --enable-dynamic-provisioning=true \ --feature-gates=AllAlpha=true \ --horizontal-pod-autoscaler-sync-period=10s \ --horizontal-pod-autoscaler-use-rest-clients=true \ --node-monitor-grace-period=10s \ --address=127.0.0.1 \ --leader-elect=true \ --kubeconfig=/etc/kubernetes/controller-manager.conf \ --cluster-signing-key-file=/etc/kubernetes/pki/ca.key \ --use-service-account-credentials=true \ --controllers=*,bootstrapsigner,tokencleaner \ --root-ca-file=/etc/kubernetes/pki/ca.crt \ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt \ --allocate-node-cidrs=true \ --cluster-cidr=10.244.0.0/16 \ --node-cidr-mask-size=24 控制器kube-controller-managerkube-controller-manager 由一系列的控制器组成，这些控制器可以划分为三组 必须启动的控制器 EndpointController ReplicationController： PodGCController ResourceQuotaController NamespaceController ServiceAccountController GarbageCollectorController DaemonSetController JobController DeploymentController ReplicaSetController HPAController DisruptionController StatefulSetController CronJobController CSRSigningController CSRApprovingController TTLController 默认启动的可选控制器，可通过选项设置是否开启 TokenController NodeController ServiceController RouteController PVBinderController AttachDetachController 默认禁止的可选控制器，可通过选项设置是否开启 BootstrapSignerController TokenCleanerController cloud-controller-managercloud-controller-manager 在 Kubernetes 启用 Cloud Provider 的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器 CloudNodeController RouteController ServiceController 高可用在启动时设置 --leader-elect=true 后，controller manager 会使用多节点选主的方式选择主节点。只有主节点才会调用 StartControllers() 启动所有控制器，而其他从节点则仅执行选主算法。 多节点选主的实现方法见 leaderelection.go。它实现了两种资源锁（Endpoint 或 ConfigMap，kube-controller-manager 和 cloud-controller-manager 都使用 Endpoint 锁），通过更新资源的 Annotation（control-plane.alpha.kubernetes.io/leader），来确定主从关系。 高性能从 Kubernetes 1.7 开始，所有需要监控资源变化情况的调用均推荐使用 Informer。Informer 提供了基于事件通知的只读缓存机制，可以注册资源变化的回调函数，并可以极大减少 API 的调用。 Informer 的使用方法可以参考 这里。 Node EvictionNode 控制器在节点异常后，会按照默认的速率（--node-eviction-rate=0.1，即每10秒一个节点的速率）进行 Node 的驱逐。Node 控制器按照 Zone 将节点划分为不同的组，再跟进 Zone 的状态进行速率调整： Normal：所有节点都 Ready，默认速率驱逐。 PartialDisruption：即超过33% 的节点 NotReady 的状态。当异常节点比例大于 --unhealthy-zone-threshold=0.55 时开始减慢速率： 小集群（即节点数量小于 --large-cluster-size-threshold=50）：停止驱逐 大集群，减慢速率为 --secondary-node-eviction-rate=0.01 FullDisruption：所有节点都 NotReady，返回使用默认速率驱逐。但当所有 Zone 都处在 FullDisruption 时，停止驱逐。 #什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 使用 DaemonSet 的一些典型用法： 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。在每个 Node 上运行日志收集 daemon，例如fluentd、logstash。在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。一个简单的用法是，在所有的 Node 上都存在一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法可能是，对单独的每种类型的 daemon 使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有不同的内存、CPU要求。 编写 DaemonSet Spec必需字段和其它所有 Kubernetes 配置一样，DaemonSet 需要 apiVersion、kind 和 metadata字段。有关配置文件的通用信息，详见文档 deploying applications、配置容器 和 资源管理 。 DaemonSet 也需要一个 .spec 配置段。 Pod 模板.spec 唯一必需的字段是 .spec.template。 .spec.template 是一个 Pod 模板。 它与 Pod 具有相同的 schema，除了它是嵌套的，而且不具有 apiVersion 或 kind 字段。 Pod 除了必须字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 pod selector）。 在 DaemonSet 中的 Pod 模板必需具有一个值为 Always 的 RestartPolicy，或者未指定它的值，默认是 Always。 Pod Selector.spec.selector 字段表示 Pod Selector，它与 Job 或其它资源的 .sper.selector 的原理是相同的。 spec.selector 表示一个对象，它由如下两个字段组成： matchLabels - 与 ReplicationController 的 .spec.selector 的原理相同。matchExpressions - 允许构建更加复杂的 Selector，可以通过指定 key、value 列表，以及与 key 和 value 列表的相关的操作符。当上述两个字段都指定时，结果表示的是 AND 关系。 如果指定了 .spec.selector，必须与 .spec.template.metadata.labels 相匹配。如果没有指定，它们默认是等价的。如果与它们配置的不匹配，则会被 API 拒绝。 如果 Pod 的 label 与 selector 匹配，或者直接基于其它的 DaemonSet、或者 Controller（例如 ReplicationController），也不可以创建任何 Pod。 否则 DaemonSet Controller 将认为那些 Pod 是它创建的。Kubernetes 不会阻止这样做。一个场景是，可能希望在一个具有不同值的、用来测试用的 Node 上手动创建 Pod。 仅在相同的 Node 上运行 Pod如果指定了 .spec.template.spec.nodeSelector，DaemonSet Controller 将在能够匹配上 Node Selector 的 Node 上创建 Pod。 类似这种情况，可以指定 .spec.template.spec.affinity，然后 DaemonSet Controller 将在能够匹配上 Node Affinity 的 Node 上创建 Pod。 如果根本就没有指定，则 DaemonSet Controller 将在所有 Node 上创建 Pod。 如果调度 Daemon Pod正常情况下，Pod 运行在哪个机器上是由 Kubernetes 调度器进行选择的。然而，由 Daemon Controller 创建的 Pod 已经确定了在哪个机器上（Pod 创建时指定了 .spec.nodeName），因此： DaemonSet Controller 并不关心一个 Node 的 unschedulable 字段。DaemonSet Controller 可以创建 Pod，即使调度器还没有被启动，这对集群启动是非常有帮助的。Daemon Pod 关心 Taint 和 Toleration，它们会为没有指定 tolerationSeconds 的 node.alpha.kubernetes.io/notReady 和 node.alpha.kubernetes.io/unreachable 的 Taint，而创建具有 NoExecute 的 Toleration。这确保了当 alpha 特性的 TaintBasedEvictions 被启用，当 Node 出现故障，比如网络分区，这时它们将不会被清除掉（当 TaintBasedEvictions 特性没有启用，在这些场景下也不会被清除，但会因为 NodeController 的硬编码行为而被清除，Toleration 是不会的）。 与 Daemon Pod 通信与 DaemonSet 中的 Pod 进行通信，几种可能的模式如下： Push：配置 DaemonSet 中的 Pod 向其它 Service 发送更新，例如统计数据库。它们没有客户端。NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过 Node IP 访问到 Pod。客户端能通过某种方法知道 Node IP 列表，并且基于此也可以知道端口。DNS：创建具有相同 Pod Selector 的 Headless Service，然后通过使用 endpoints 资源或从 DNS 检索到多个 A 记录来发现 DaemonSet。Service：创建具有相同 Pod Selector 的 Service，并使用该 Service 访问到某个随机 Node 上的 daemon。（没有办法访问到特定 Node） 更新 DaemonSet如果修改了 Node Label，DaemonSet 将立刻向新匹配上的 Node 添加 Pod，同时删除新近无法匹配上的 Node 上的 Pod。 可以修改 DaemonSet 创建的 Pod。然而，不允许对 Pod 的所有字段进行更新。当下次 Node（即使具有相同的名称）被创建时，DaemonSet Controller 还会使用最初的模板。 可以删除一个 DaemonSet。如果使用 kubectl 并指定 –cascade=false 选项，则 Pod 将被保留在 Node 上。然后可以创建具有不同模板的新 DaemonSet。具有不同模板的新 DaemonSet 将鞥能够通过 Label 匹配识别所有已经存在的 Pod。它不会修改或删除它们，即使是错误匹配了 Pod 模板。通过删除 Pod 或者 删除 Node，可以强制创建新的 Pod。 在 Kubernetes 1.6 或以后版本，可以在 DaemonSet 上 执行滚动升级。 未来的 Kubernetes 版本将支持 Node 的可控更新。 DaemonSet 的可替代选择init 脚本很可能通过直接在一个 Node 上启动 daemon 进程（例如，使用 init、upstartd、或 systemd）。这非常好，然而基于 DaemonSet 来运行这些进程有如下一些好处： 像对待应用程序一样，具备为 daemon 提供监控和管理日志的能力。为 daemon 和应用城西使用相同的配置语言和工具（如 Pod 模板、kubectl）。Kubernetes 未来版本可能会支持对 DaemonSet 创建 Pod 与 Node升级工作流进行集成。在资源受限的容器中运行 daemon，能够增加 daemon 和应用容器的隔离性。然而这也实现了在容器中运行 daemon，但却不能在 Pod 中运行（例如，直接基于 Docker 启动）。裸 Pod可能要直接创建 Pod，同时指定其运行在特定的 Node 上。 然而，DaemonSet 替换了由于任何原因被删除或终止的 Pod，例如 Node 失败、例行节点维护，比如内和升级。由于这个原因，我们应该使用 DaemonSet 而不是单独创建 Pod。 静态 Pod很可能，通过在一个指定目录下编写文件来创建 Pod，该目录受 Kubelet 所监视。这些 Pod 被称为 静态 Pod。 不像 DaemonSet，静态 Pod 不受 kubectl 和 其它 Kubernetes API 客户端管理。静态 Pod 不依赖于 apiserver，这使得它们在集群启动的情况下非常有用。 而且，未来静态 Pod 可能会被废弃掉。 Replication ControllerDaemonSet 与 Replication Controller 非常类似，它们都能创建 Pod，这些 Pod 都具有不期望被终止的进程（例如，Web 服务器、存储服务器）。 为无状态的 Service 使用 Replication Controller，像 frontend，实现对副本的数量进行扩缩容、平滑升级，比之于精确控制 Pod 运行在某个主机上要重要得多。需要 Pod 副本总是运行在全部或特定主机上，并需要先于其他 Pod 启动，当这被认为非常重要时，应该使用 Daemon Controller。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cAdvisor]]></title>
    <url>%2F2019%2F03%2F22%2FcAdvisor%2F</url>
    <content type="text"><![CDATA[1. cAdvisor简介​ cAdvisor对Node机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，cAdvisor集成在Kubelet中，当kubelet启动时会自动启动cAdvisor，即一个cAdvisor仅对一台Node机器进行监控。kubelet的启动参数–cadvisor-port可以定义cAdvisor对外提供服务的端口，默认为4194。可以通过浏览器访问。项目主页：http://github.com/google/cadvisor。 2. cAdvisor结构图 3. Metrics 分类 字段 描述 cpu cpu_usage_total cpu_usage_system cpu_usage_user cpu_usage_per_cpu load_average Smoothed average of number of runnable threads x 1000 memory memory_usage Memory Usage memory_working_set Working set size network rx_bytes Cumulative count of bytes received rx_errors Cumulative count of receive errors encountered tx_bytes Cumulative count of bytes transmitted tx_errors Cumulative count of transmit errors encountered filesystem fs_device Filesystem device fs_limit Filesystem limit fs_usage Filesystem usage 4. cAdvisor源码4.1. cAdvisor入口函数cadvisor.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344func main() { defer glog.Flush() flag.Parse() if *versionFlag { fmt.Printf("cAdvisor version %s (%s)/n", version.Info["version"], version.Info["revision"]) os.Exit(0) } setMaxProcs() memoryStorage, err := NewMemoryStorage() if err != nil { glog.Fatalf("Failed to initialize storage driver: %s", err) } sysFs, err := sysfs.NewRealSysFs() if err != nil { glog.Fatalf("Failed to create a system interface: %s", err) } collectorHttpClient := createCollectorHttpClient(*collectorCert, *collectorKey) containerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, &collectorHttpClient) if err != nil { glog.Fatalf("Failed to create a Container Manager: %s", err) } mux := http.NewServeMux() if *enableProfiling { mux.HandleFunc("/debug/pprof/", pprof.Index) mux.HandleFunc("/debug/pprof/cmdline", pprof.Cmdline) mux.HandleFunc("/debug/pprof/profile", pprof.Profile) mux.HandleFunc("/debug/pprof/symbol", pprof.Symbol) } // Register all HTTP handlers. err = cadvisorhttp.RegisterHandlers(mux, containerManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm) if err != nil { glog.Fatalf("Failed to register HTTP handlers: %v", err) } cadvisorhttp.RegisterPrometheusHandler(mux, containerManager, *prometheusEndpoint, nil) // Start the manager. if err := containerManager.Start(); err != nil { glog.Fatalf("Failed to start container manager: %v", err) } // Install signal handler. installSignalHandler(containerManager) glog.Infof("Starting cAdvisor version: %s-%s on port %d", version.Info["version"], version.Info["revision"], *argPort) addr := fmt.Sprintf("%s:%d", *argIp, *argPort) glog.Fatal(http.ListenAndServe(addr, mux))} 核心代码： 123456memoryStorage, err := NewMemoryStorage()sysFs, err := sysfs.NewRealSysFs()#创建containerManagercontainerManager, err := manager.New(memoryStorage, sysFs, *maxHousekeepingInterval, *allowDynamicHousekeeping, ignoreMetrics.MetricSet, &collectorHttpClient)#启动containerManagererr := containerManager.Start() 4.2. cAdvisor Client的使用1234import "github.com/google/cadvisor/client"func main(){ client, err := client.NewClient("http://192.168.19.30:4194/") //http://:/} 4.2.1 client定义cadvisor/client/client.go 12345678910111213// Client represents the base URL for a cAdvisor client.type Client struct { baseUrl string}// NewClient returns a new v1.3 client with the specified base URL.func NewClient(url string) (*Client, error) { if !strings.HasSuffix(url, "/") { url += "/" } return &Client{ baseUrl: fmt.Sprintf("%sapi/v1.3/", url), }, nil} 4.2.2. client方法1）MachineInfo 123456789101112// MachineInfo returns the JSON machine information for this client.// A non-nil error result indicates a problem with obtaining// the JSON machine information data.func (self *Client) MachineInfo() (minfo *v1.MachineInfo, err error) { u := self.machineInfoUrl() ret := new(v1.MachineInfo) if err = self.httpGetJsonData(ret, nil, u, "machine info"); err != nil { return } minfo = ret return} 2）ContainerInfo 1234567891011// ContainerInfo returns the JSON container information for the specified// container and request.func (self *Client) ContainerInfo(name string, query *v1.ContainerInfoRequest) (cinfo *v1.ContainerInfo, err error) { u := self.containerInfoUrl(name) ret := new(v1.ContainerInfo) if err = self.httpGetJsonData(ret, query, u, fmt.Sprintf("container info for %q", name)); err != nil { return } cinfo = ret return} 3）DockerContainer 1234567891011121314151617// Returns the JSON container information for the specified// Docker container and request.func (self *Client) DockerContainer(name string, query *v1.ContainerInfoRequest) (cinfo v1.ContainerInfo, err error) { u := self.dockerInfoUrl(name) ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(&ret, query, u, fmt.Sprintf("Docker container info for %q", name)); err != nil { return } if len(ret) != 1 { err = fmt.Errorf("expected to only receive 1 Docker container: %+v", ret) return } for _, cont := range ret { cinfo = cont } return} 4）AllDockerContainers 12345678910111213// Returns the JSON container information for all Docker containers.func (self *Client) AllDockerContainers(query *v1.ContainerInfoRequest) (cinfo []v1.ContainerInfo, err error) { u := self.dockerInfoUrl("/") ret := make(map[string]v1.ContainerInfo) if err = self.httpGetJsonData(&ret, query, u, "all Docker containers info"); err != nil { return } cinfo = make([]v1.ContainerInfo, 0, len(ret)) for _, cont := range ret { cinfo = append(cinfo, cont) } return}]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>cAdvisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerfile]]></title>
    <url>%2F2019%2F03%2F21%2Fdockerfile%2F</url>
    <content type="text"><![CDATA[1. Dockerfile的说明dockerfile指令忽略大小写，建议大写，#作为注释，每行只支持一条指令，指令可以带多个参数。 dockerfile指令分为构建指令和设置指令。 构建指令：用于构建image，其指定的操作不会在运行image的容器中执行。 设置指令：用于设置image的属性，其指定的操作会在运行image的容器中执行。 2. Dockerfile指令说明2.1. FROM（指定基础镜像）[构建指令]该命令用来指定基础镜像，在基础镜像的基础上修改数据从而构建新的镜像。基础镜像可以是本地仓库也可以是远程仓库。 指令有两种格式： FROM image 【默认为latest版本】 FROM image:tag 【指定版本】 2.2. MAINTAINER（镜像创建者信息）[构建指令]将镜像制作者（维护者）的信息写入image中，执行docker inspect时会输出该信息。 格式：MAINTAINER name 2.3. RUN（安装软件用）[构建指令]RUN可以运行任何被基础镜像支持的命令（即在基础镜像上执行一个进程），可以使用多条RUN指令，指令较长可以使用\来换行。 指令有两种格式： RUN command (the command is run in a shell - /bin/sh -c) RUN [“executable”, “param1”, “param2” … ] (exec form) 指定使用其他终端实现，使用exec执行。 例子：RUN[“/bin/bash”,”-c”,”echo hello”] 2.4. CMD（设置container启动时执行的操作）[设置指令]用于容器启动时的指定操作，可以是自定义脚本或命令，只执行一次，多个默认执行最后一个。 指令有三种格式： CMD [“executable”,”param1”,”param2”] (like an exec, this is the preferred form) 运行一个可执行文件并提供参数。 CMD command param1 param2 (as a shell) 直接执行shell命令，默认以/bin/sh -c执行。 CMD [“param1”,”param2”] (as default parameters to ENTRYPOINT) 和ENTRYPOINT配合使用，只作为完整命令的参数部分。 2.5. ENTRYPOINT（设置container启动时执行的操作）[设置指令]指定容器启动时执行的命令，若多次设置只执行最后一次。 ENTRYPOINT翻译为“进入点”，它的功能可以让容器表现得像一个可执行程序一样。 例子：ENTRYPOINT [“/bin/echo”] ，那么docker build出来的镜像以后的容器功能就像一个/bin/echo程序，docker run -it imageecho “this is a test”，就会输出对应的字符串。这个imageecho镜像对应的容器表现出来的功能就像一个echo程序一样。 指令有两种格式： ENTRYPOINT [“executable”, “param1”, “param2”] (like an exec, the preferred form) 和CMD配合使用，CMD则作为完整命令的参数部分，ENTRYPOINT以JSON格式指定执行的命令部分。CMD可以为ENTRYPOINT提供可变参数，不需要变动的参数可以写在ENTRYPOINT里面。 例子： ENTRYPOINT [“/usr/bin/ls”,”-a”] CMD [“-l”] ENTRYPOINT command param1 param2 (as a shell) 独自使用，即和CMD类似，如果CMD也是个完整命令[CMD command param1 param2 (as a shell) ]，那么会相互覆盖，只执行最后一个CMD或ENTRYPOINT。 例子：ENTRYPOINT ls -l 2.6. USER（设置container容器启动的登录用户）[设置指令]设置启动容器的用户，默认为root用户。 格式：USER daemon 2.7. EXPOSE（指定容器需要映射到宿主机的端口）[设置指令]该指令会将容器中的端口映射为宿主机中的端口[确保宿主机的端口号没有被使用]。通过宿主机IP和映射后的端口即可访问容器[避免每次运行容器时IP随机生成不固定的问题]。前提是EXPOSE设置映射端口，运行容器时加上-p参数指定EXPOSE设置的端口。EXPOSE可以设置多个端口号，相应地运行容器配套多次使用-p参数。可以通过docker port +容器需要映射的端口号和容器ID来参考宿主机的映射端口。 格式：EXPOSE port [port…] 2.8. ENV（用于设置环境变量）[构建指令]在image中设置环境变量[以键值对的形式]，设置之后RUN命令可以使用该环境变量，在容器启动后也可以通过docker inspect查看环境变量或者通过 docker run –env key=value设置或修改环境变量。 格式：ENV key value 例子：ENV JAVA_HOME /path/to/java/dirent 2.9. ADD（从src复制文件到container的dest路径）[构建指令]复制指定的src到容器中的dest，其中src是相对被构建的源目录的相对路径，可以是文件或目录的路径，也可以是一个远程的文件url。dest 是container中的绝对路径。所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0。 如果src是一个目录，那么会将该目录下的所有文件添加到container中，不包括目录； 如果src文件是可识别的压缩格式，则docker会帮忙解压缩（注意压缩格式）； 如果src是文件且dest中不使用斜杠结束，则会将dest视为文件，src的内容会写入dest； 如果src是文件且dest中使用斜杠结束，则会src文件拷贝到dest目录下。 格式：ADD src dest 2.10. COPY（复制文件）复制本地主机的src为容器中的dest，目标路径不存在时会自动创建。 格式：COPY src dest 2.11. VOLUME（指定挂载点）[设置指令]创建一个可以从本地主机或其他容器挂载的挂载点，使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用也可以被其他容器使用。 格式：VOLUME [“mountpoint“] 其他容器使用共享数据卷：docker run -t -i -rm -volumes-from container1 image2 bash [container1为第一个容器的ID，image2为第二个容器运行image的名字。] 2.12. WORKDIR（切换目录）[设置指令]相当于cd命令，可以多次切换目录，为RUN,CMD,ENTRYPOINT配置工作目录。可以使用多个WORKDIR的命令，后续命令如果是相对路径则是在上一级路径的基础上执行[类似cd的功能]。 格式：WORKDIR /path/to/workdir 2.13. ONBUILD（在子镜像中执行）当所创建的镜像作为其他新创建镜像的基础镜像时执行的操作命令，即在创建本镜像时不运行，当作为别人的基础镜像时再在构建时运行（可认为基础镜像为父镜像，而该命令即在它的子镜像构建时运行，相当于在子镜像构建时多加了一些命令）。 格式：ONBUILD Dockerfile关键字 3. docker build123456789101112131415161718192021222324252627282930313233343536373839Usage: docker build [OPTIONS] PATH | URL | -Build a new image from the source code at PATH-c, --cpu-shares=0 CPU shares (relative weight)--cgroup-parent= Optional parent cgroup for the container--cpu-period=0 Limit the CPU CFS (Completely Fair Scheduler) period--cpu-quota=0 Limit the CPU CFS (Completely Fair Scheduler) quota--cpuset-cpus= CPUs in which to allow execution (0-3, 0,1)--cpuset-mems= MEMs in which to allow execution (0-3, 0,1)--disable-content-trust=true Skip image verification-f, --file= Name of the Dockerfile (Default is 'PATH/Dockerfile')--force-rm=false Always remove intermediate containers--help=false Print usage-m, --memory= Memory limit--memory-swap= Total memory (memory + swap), '-1' to disable swap--no-cache=false Do not use cache when building the image--pull=false Always attempt to pull a newer version of the image-q, --quiet=false Suppress the verbose output generated by the containers--rm=true Remove intermediate containers after a successful build-t, --tag= Repository name (and optionally a tag) for the image--ulimit=[] Ulimit options]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker架构]]></title>
    <url>%2F2019%2F03%2F20%2Fdocker%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1. Docker的总架构图docker是一个C/S模式的架构，后端是一个松耦合架构，模块各司其职。 用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。 Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求； Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。 Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储； 当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境； 当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。 libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。 2. Docker各模块组件分析2.1. Docker Client[发起请求] Docker Client是和Docker Daemon建立通信的客户端。用户使用的可执行文件为docker（类似可执行脚本的命令），docker命令后接参数的形式来实现一个完整的请求命令（例如docker images，docker为命令不可变，images为参数可变）。 Docker Client可以通过以下三种方式和Docker Daemon建立通信：tcp://host:port，unix://path_to_socket和fd://socketfd。 Docker Client发送容器管理请求后，由Docker Daemon接受并处理请求，当Docker Client接收到返回的请求相应并简单处理后，Docker Client一次完整的生命周期就结束了。[一次完整的请求：发送请求→处理请求→返回结果]，与传统的C/S架构请求流程并无不同。 2.2. Docker Daemon[后台守护进程]Docker Daemon的架构图 2.2.1. Docker Server[调度分发请求]Docker Server的架构图 Docker Server相当于C/S架构的服务端。功能为接受并调度分发Docker Client发送的请求。接受请求后，Server通过路由与分发调度，找到相应的Handler来执行请求。 在Docker的启动过程中，通过包gorilla/mux，创建了一个mux.Router，提供请求的路由功能。在Golang中，gorilla/mux是一个强大的URL路由器以及调度分发器。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。 创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。 在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。 2.2.2. Engine Engine是Docker架构中的运行引擎，同时也Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。 在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。举例说明，Engine的handler对象中有一项为：{“create”: daemon.ContainerCreate,}，则说明当名为”create”的job在运行时，执行的是daemon.ContainerCreate的handler。 2.2.3. Job 一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。例如：在容器内部运行一个进程，这是一个job；创建一个新的容器，这是一个job。Docker Server的运行过程也是一个job，名为serveapi。 Job的设计者，把Job设计得与Unix进程相仿。比如说：Job有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。 2.3. Docker Registry[镜像注册中心] Docker Registry是一个存储容器镜像的仓库（注册中心），可理解为云端镜像仓库，按repository来分类，docker pull 按照[repository]:[tag]来精确定义一个image。 在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为”search”，”pull” 与 “push”。 可分为公有仓库（docker hub）和私有仓库。 2.4. Graph[docker内部数据库]Graph的架构图 2.4.1. Repository 已下载镜像的保管者（包括下载镜像和dockerfile构建的镜像）。 一个repository表示某类镜像的仓库（例如Ubuntu），同一个repository内的镜像用tag来区分（表示同一类镜像的不同标签或版本）。一个registry包含多个repository，一个repository包含同类型的多个image。 镜像的存储类型有aufs，devicemapper,Btrfs，Vfs等。其中centos系统使用devicemapper的存储类型。 同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。 2.4.2. GraphDB 已下载容器镜像之间关系的记录者。 GraphDB是一个构建在SQLite之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录 2.5. Driver[执行部分]Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。即Graph负责镜像的存储，Driver负责容器的执行。 2.5.1. graphdrivergraphdriver架构图 graphdriver主要用于完成容器镜像的管理，包括存储与获取。 存储：docker pull下载的镜像由graphdriver存储到本地的指定目录（Graph中）。 获取：docker run（create）用镜像来创建容器的时候由graphdriver到本地Graph中获取镜像。 2.5.2. networkdrivernetworkdriver的架构图 networkdriver的用途是完成Docker容器网络环境的配置，其中包括 Docker启动时为Docker环境创建网桥； Docker容器创建时为其创建专属虚拟网卡设备； Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。 2.5.3. execdriverexecdriver的架构图 execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。 现在execdriver默认使用native驱动，不依赖于LXC。 2.6. libcontainer[函数库]libcontainer的架构图 libcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。 Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。 libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。 2.7. docker container[服务交付的最终形式]container架构 Docker container（Docker容器）是Docker架构中服务交付的最终体现形式。 Docker按照用户的需求与指令，订制相应的Docker容器： 用户通过指定容器镜像，使得Docker容器可以自定义rootfs等文件系统； 用户通过指定计算资源的配额，使得Docker容器使用指定的计算资源； 用户通过配置网络及其安全策略，使得Docker容器拥有独立且安全的网络环境； 用户通过指定运行的命令，使得Docker容器执行指定的工作。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[API Server简介]]></title>
    <url>%2F2019%2F03%2F19%2FAPI-Server%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1. API Server简介k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。 kubernetes API Server的功能： 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; 是资源配额控制的入口； 拥有完备的集群安全机制. kube-apiserver工作原理图 2. 如何访问kubernetes APIk8s通过kube-apiserver这个进程提供服务，该进程运行在单个k8s-master节点上。默认有两个端口。 2.1. 本地端口 该端口用于接收HTTP请求； 该端口默认值为8080，可以通过API Server的启动参数“–insecure-port”的值来修改默认值； 默认的IP地址为“localhost”，可以通过启动参数“–insecure-bind-address”的值来修改该IP地址； 非认证或授权的HTTP请求通过该端口访问API Server。 2.2. 安全端口 该端口默认值为6443，可通过启动参数“–secure-port”的值来修改默认值； 默认IP地址为非本地（Non-Localhost）网络端口，通过启动参数“–bind-address”设置该值； 该端口用于接收HTTPS请求； 用于基于Tocken文件或客户端证书及HTTP Base的认证； 用于基于策略的授权； 默认不启动HTTPS安全访问控制。 2.3. 访问方式Kubernetes REST API可参考https://kubernetes.io/docs/api-reference/v1.6/ 2.3.1. curl1234curl localhost:8080/apicurl localhost:8080/api/v1/podscurl localhost:8080/api/v1/servicescurl localhost:8080/api/v1/replicationcontrollers 2.3.2. Kubectl ProxyKubectl Proxy代理程序既能作为API Server的反向代理，也能作为普通客户端访问API Server的代理。通过master节点的8080端口来启动该代理程序。 kubectl proxy –port=8080 & 具体见kubectl proxy –help 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@node5 ~]# kubectl proxy --helpTo proxy all of the kubernetes api and nothing else, use:kubectl proxy --api-prefix=/To proxy only part of the kubernetes api and also some static files:kubectl proxy --www=/my/files --www-prefix=/static/ --api-prefix=/api/The above lets you 'curl localhost:8001/api/v1/pods'.To proxy the entire kubernetes api at a different root, use:kubectl proxy --api-prefix=/custom/The above lets you 'curl localhost:8001/custom/api/v1/pods'Usage: kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags]Examples:# Run a proxy to kubernetes apiserver on port 8011, serving static content from ./local/www/$ kubectl proxy --port=8011 --www=./local/www/# Run a proxy to kubernetes apiserver on an arbitrary local port.# The chosen port for the server will be output to stdout.$ kubectl proxy --port=0# Run a proxy to kubernetes apiserver, changing the api prefix to k8s-api# This makes e.g. the pods api available at localhost:8011/k8s-api/v1/pods/$ kubectl proxy --api-prefix=/k8s-apiFlags: --accept-hosts="^localhost$,^127//.0//.0//.1$,^//[::1//]$": Regular expression for hosts that the proxy should accept. --accept-paths="^/.*": Regular expression for paths that the proxy should accept. --api-prefix="/": Prefix to serve the proxied API under. --disable-filter[=false]: If true, disable request filtering in the proxy. This is dangerous, and can leave you vulnerable to XSRF attacks, when used with an accessible port. -p, --port=8001: The port on which to run the proxy. Set to 0 to pick a random port. --reject-methods="POST,PUT,PATCH": Regular expression for HTTP methods that the proxy should reject. --reject-paths="^/api/.*/exec,^/api/.*/run": Regular expression for paths that the proxy should reject. -u, --unix-socket="": Unix socket on which to run the proxy. -w, --www="": Also serve static files from the given directory under the specified prefix. -P, --www-prefix="/static/": Prefix to serve static files under, if static file directory is specified. Global Flags: --alsologtostderr[=false]: log to standard error as well as files --api-version="": The API version to use when talking to the server --certificate-authority="": Path to a cert. file for the certificate authority. --client-certificate="": Path to a client key file for TLS. --client-key="": Path to a client key file for TLS. --cluster="": The name of the kubeconfig cluster to use --context="": The name of the kubeconfig context to use --insecure-skip-tls-verify[=false]: If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure. --kubeconfig="": Path to the kubeconfig file to use for CLI requests. --log-backtrace-at=:0: when logging hits line file:N, emit a stack trace --log-dir="": If non-empty, write log files in this directory --log-flush-frequency=5s: Maximum number of seconds between log flushes --logtostderr[=true]: log to standard error instead of files --match-server-version[=false]: Require server version to match client version --namespace="": If present, the namespace scope for this CLI request. --password="": Password for basic authentication to the API server. -s, --server="": The address and port of the Kubernetes API server --stderrthreshold=2: logs at or above this threshold go to stderr --token="": Bearer token for authentication to the API server. --user="": The name of the kubeconfig user to use --username="": Username for basic authentication to the API server. --v=0: log level for V logs --vmodule=: comma-separated list of pattern=N settings for file-filtered logging 2.3.3. kubectl客户端命令行工具kubectl客户端，通过命令行参数转换为对API Server的REST API调用，并将调用结果输出。 命令格式：kubectl [command] [options] 具体可参考k8s常用命令 2.3.4. 编程方式调用使用场景： 1、运行在Pod里的用户进程调用kubernetes API,通常用来实现分布式集群搭建的目标。 2、开发基于kubernetes的管理平台，比如调用kubernetes API来完成Pod、Service、RC等资源对象的图形化创建和管理界面。可以使用kubernetes提供的Client Library。 具体可参考https://github.com/kubernetes/client-go。 3. 通过API Server访问Node、Pod和Servicek8s API Server最主要的REST接口是资源对象的增删改查，另外还有一类特殊的REST接口—k8s Proxy API接口，这类接口的作用是代理REST请求，即kubernetes API Server把收到的REST请求转发到某个Node上的kubelet守护进程的REST端口上，由该kubelet进程负责响应。 3.1. Node相关接口关于Node相关的接口的REST路径为：/api/v1/proxy/nodes/{name}，其中{name}为节点的名称或IP地址。 123/api/v1/proxy/nodes/{name}/pods/ #列出指定节点内所有Pod的信息/api/v1/proxy/nodes/{name}/stats/ #列出指定节点内物理资源的统计信息/api/v1/prxoy/nodes/{name}/spec/ #列出指定节点的概要信息 这里获取的Pod信息来自Node而非etcd数据库，两者时间点可能存在偏差。如果在kubelet进程启动时加–enable-debugging-handles=true参数，那么kubernetes Proxy API还会增加以下接口： 12345678/api/v1/proxy/nodes/{name}/run #在节点上运行某个容器/api/v1/proxy/nodes/{name}/exec #在节点上的某个容器中运行某条命令/api/v1/proxy/nodes/{name}/attach #在节点上attach某个容器/api/v1/proxy/nodes/{name}/portForward #实现节点上的Pod端口转发/api/v1/proxy/nodes/{name}/logs #列出节点的各类日志信息/api/v1/proxy/nodes/{name}/metrics #列出和该节点相关的Metrics信息/api/v1/proxy/nodes/{name}/runningpods #列出节点内运行中的Pod信息/api/v1/proxy/nodes/{name}/debug/pprof #列出节点内当前web服务的状态，包括CPU和内存的使用情况 3.2. Pod相关接口12345/api/v1/proxy/namespaces/{namespace}/pods/{name}/{path:*} #访问pod的某个服务接口/api/v1/proxy/namespaces/{namespace}/pods/{name} #访问Pod#以下写法不同，功能一样/api/v1/namespaces/{namespace}/pods/{name}/proxy/{path:*} #访问pod的某个服务接口/api/v1/namespaces/{namespace}/pods/{name}/proxy #访问Pod 3.3. Service相关接口1/api/v1/proxy/namespaces/{namespace}/services/{name} Pod的proxy接口的作用：在kubernetes集群之外访问某个pod容器的服务（HTTP服务），可以用Proxy API实现，这种场景多用于管理目的，比如逐一排查Service的Pod副本，检查哪些Pod的服务存在异常问题。 4. 集群功能模块之间的通信kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信，集群内各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，通过API Server提供的REST接口（GET/LIST/WATCH方法）来实现，从而实现各模块之间的信息交互。 4.1. kubelet与API Server交互每个Node节点上的kubelet定期就会调用API Server的REST接口报告自身状态，API Server接收这些信息后，将节点状态信息更新到etcd中。kubelet也通过API Server的Watch接口监听Pod信息，从而对Node机器上的POD进行管理。 监听信息 kubelet动作 新的POD副本被调度绑定到本节点 执行POD对应的容器的创建和启动逻辑 POD对象被删除 删除本节点上相应的POD容器 修改POD信息 修改本节点的POD容器 4.2. kube-controller-manager与API Server交互kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口，实时监控Node的信息，并做相应处理。 4.3. kube-scheduler与API Server交互Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，它会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑。调度成功后将Pod绑定到目标节点上。 4.4. 特别说明为了缓解各模块对API Server的访问压力，各功能模块都采用缓存机制来缓存数据，各功能模块定时从API Server获取指定的资源对象信息（LIST/WATCH方法），然后将信息保存到本地缓存，功能模块在某些情况下不直接访问API Server，而是通过访问缓存数据来间接访问API Server。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开放分布式系统追踪和Jaeger(Uber)]]></title>
    <url>%2F2019%2F03%2F19%2F%E5%BC%80%E6%94%BE%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[开放分布式系统追踪分布式运维系统的挑战开放和工程团队因为系统组件水平扩展、开放团队小型化、敏捷开放、CD（持续集成）、解耦等各种需求，正在使用现代的微服务架构替换老旧的单机系统。 也就是说，当一个生产系统面对真正的高并发，或者解耦成大量微服务时，以前很容易实现的重点任务变得困难了。过程中需要面临一系列问题：用户体验优化、后台真实错误原因分析，分布式系统内各组件的调用情况等。 面向DevOps诊断与分析系统 常见诊断与分析系统如：Zipkin，Jaeger(Uber)，Appdash，盘古，云图，X-ray Prometheus，ELK等，可按关注的领域分为三类； Zipkin，Jaeger，Appdash，盘古，云图 ，X-ray专注于tracing，Prometheus专注于metrics，ELK 更专注于logging领域； 开放分布式追踪 OpenTracingTracing 与 OpenTracing分布式追踪系统核心步骤: 当代分布式跟踪系统（例如，Zipkin, Dapper, HTrace, X-Trace等）旨在解决这些问题，但是他们使用不兼容的API来实现各自的应用需求。尽管这些分布式追踪系统有着相似的API语法，但各种语言的开放人员依然很难将他们各自的系统（使用不同的语言和技术）和特定的分布式追踪系统进行整合；为了解决不同的分布式追踪系统 API 不兼容的问题，诞生了OpenTracing标准规范 OpenTracing概念与术语 术语 说明 Trace 一个trace代表一个潜在的，分布式的，存在并行数据或并行执行轨迹（潜在的分布式、并行）的系统。一个trace可以认为是多个span的有向无环图（DAG） Span 一个span代表系统中具有开始时间和执行时长的逻辑运行单元。span之间通过嵌套或者顺序排列建立逻辑因果关系 OperationName Trace执行操作的名称 Inter-Span References ChildOf 父子关系 ，FollowFrom 引用 SpanContext SpanContext代表跨越进程边界，传递到下级span的状态，并用于封装Baggage Baggage Baggage是存储在SpanContext中的一个键值对(SpanContext)集合。它会在一条追踪链路上的所有span内全局传输 Tag Span中的键值对，tag不会被子Span集成，也不会传输；span的tag可以用来记录业务相关的数据，并存储于追踪系统中 123456789101112131415#一个tracer过程中，各span的关系 [Span A] ←←←(the root span) | +------+------+ | |[Span B] [Span C] ←←←(Span C 是 Span A 的孩子节点, ChildOf) | |[Span D] +---+-------+ | | [Span E] [Span F] >>> [Span G] >>> [Span H] ↑ ↑ ↑ (Span G 在 Span F 后被调用, FollowsFrom) 12345678# 上述tracer与span的时间轴关系––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–> time [Span A···················································] [Span B··············································] [Span D··········································] [Span C········································] [Span E·······] [Span F··] [Span G··] [Span H··] Jaeger(Uber)Jaeger 是 Uber 推出的一款开源分布式追踪系统，兼容 OpenTracing API，用于分布式微服务系统； 主要有以下功能： 分布式上下文传递 分布式事务流程监控 故障分析排查 服务依赖分析 服务性能优化 Jaeger组件及架构 jaeger-query 提供trace数据访问接口 jaeger-client 提供不同编程语言对OpenTracing协议的实现，并负责jaerger-agent发送消息 目前支持python，go，java, c++ jaeger-ui 用于trace数据展示 jaeger-agent 监听client发送的span消息，并转发给jaeger-collector 每个微服务所在节点，单独部署 jaeger-collector 采集agent发送的trace消息，并对消息进行验证，索引，转化，存储 目前collector支持使用 Cassandra,，Elasticsearch ，Kafka 存储数据]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>op</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod限额]]></title>
    <url>%2F2019%2F03%2F19%2Fpod%E9%99%90%E9%A2%9D%2F</url>
    <content type="text"><![CDATA[Pod限额（LimitRange） ResourceQuota对象是限制某个namespace下所有Pod(容器)的资源限额 LimitRange对象是限制某个namespace单个Pod(容器)的资源限额 LimitRange对象用来定义某个命名空间下某种资源对象的使用限额，其中资源对象包括：Pod、Container、PersistentVolumeClaim。 1. 为namespace配置CPU和内存的默认值如果在一个拥有默认内存或CPU限额的命名空间中创建一个容器，并且这个容器未指定它自己的内存或CPU的limit， 它会被分配这个默认的内存或CPU的limit。既没有设置pod的limit和request才会分配默认的内存或CPU的request。 1.1. namespace的内存默认值12345678910111213141516171819202122232425262728293031323334353637383940414243# 创建namespace$ kubectl create namespace default-mem-example# 创建LimitRange$ cat memory-defaults.yamlapiVersion: v1kind: LimitRangemetadata: name: mem-limit-rangespec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults.yaml --namespace=default-mem-example# 创建Pod,未指定内存的limit和request$ cat memory-defaults-pod.yamlapiVersion: v1kind: Podmetadata: name: default-mem-demospec: containers: - name: default-mem-demo-ctr image: nginx $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults-pod.yaml --namespace=default-mem-example# 查看Pod$ kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-examplecontainers:- image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi 1.2. namespace的CPU默认值12345678910111213141516171819202122232425262728293031323334353637383940414243# 创建namespace$ kubectl create namespace default-cpu-example# 创建LimitRange$ cat cpu-defaults.yaml apiVersion: v1kind: LimitRangemetadata: name: cpu-limit-rangespec: limits: - default: cpu: 1 defaultRequest: cpu: 0.5 type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-defaults.yaml --namespace=default-cpu-example # 创建Pod，未指定CPU的limit和request$ cat cpu-defaults-pod.yamlapiVersion: v1kind: Podmetadata: name: default-cpu-demospec: containers: - name: default-cpu-demo-ctr image: nginx$ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-defaults-pod.yaml --namespace=default-cpu-example# 查看Pod$ kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-examplecontainers:- image: nginx imagePullPolicy: Always name: default-cpu-demo-ctr resources: limits: cpu: "1" requests: cpu: 500m 1.3 说明 如果没有指定pod的request和limit，则创建的pod会使用LimitRange对象定义的默认值（request和limit） 如果指定pod的limit但未指定request，则创建的pod的request值会取limit的值，而不会取LimitRange对象定义的request默认值。 如果指定pod的request但未指定limit，则创建的pod的limit值会取LimitRange对象定义的limit默认值。 默认Limit和request的动机 如果命名空间具有资源配额（ResourceQuota）, 它为内存限额（CPU限额）设置默认值是有意义的。 以下是资源配额对命名空间施加的两个限制： 在命名空间运行的每一个容器必须有它自己的内存限额（CPU限额）。 在命名空间中所有的容器使用的内存总量（CPU总量）不能超出指定的限额。 如果一个容器没有指定它自己的内存限额（CPU限额），它将被赋予默认的限额值，然后它才可以在被配额限制的命名空间中运行。 2. 为namespace配置CPU和内存的最大最小值2.1. 内存的最大最小值创建LimitRange 12345678910111213141516171819202122232425262728293031323334# 创建namespace$ kubectl create namespace constraints-mem-example# 创建LimitRange$ cat memory-constraints.yamlapiVersion: v1kind: LimitRangemetadata: name: mem-min-max-demo-lrspec: limits: - max: memory: 1Gi min: memory: 500Mi type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints.yaml --namespace=constraints-mem-example# 查看LimitRange$ kubectl get limitrange cpu-min-max-demo --namespace=constraints-mem-example --output=yaml... limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container...# LimitRange设置了最大最小值，但没有设置默认值，也会被自动设置默认值。 创建符合要求的Pod 123456789101112131415161718192021222324252627# 创建符合要求的Pod$ cat memory-constraints-pod.yamlapiVersion: v1kind: Podmetadata: name: constraints-mem-demospec: containers: - name: constraints-mem-demo-ctr image: nginx resources: limits: memory: "800Mi" requests: memory: "600Mi" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod.yaml --namespace=constraints-mem-example# 查看Pod$ kubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example...resources: limits: memory: 800Mi requests: memory: 600Mi... 创建超过最大内存limit的pod 1234567891011121314151617181920$ cat memory-constraints-pod-2.yamlapiVersion: v1kind: Podmetadata: name: constraints-mem-demo-2spec: containers: - name: constraints-mem-demo-2-ctr image: nginx resources: limits: memory: "1.5Gi" # 超过最大值 1Gi requests: memory: "800Mi" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-2.yaml --namespace=constraints-mem-example# Pod创建失败，因为容器指定的limit过大Error from server (Forbidden): error when creating "docs/tasks/administer-cluster/memory-constraints-pod-2.yaml":pods "constraints-mem-demo-2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi. 创建小于最小内存request的Pod 1234567891011121314151617181920$ cat memory-constraints-pod-3.yamlapiVersion: v1kind: Podmetadata: name: constraints-mem-demo-3spec: containers: - name: constraints-mem-demo-3-ctr image: nginx resources: limits: memory: "800Mi" requests: memory: "100Mi" # 小于最小值500Mi $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-3.yaml --namespace=constraints-mem-example # Pod创建失败，因为容器指定的内存request过小Error from server (Forbidden): error when creating "docs/tasks/administer-cluster/memory-constraints-pod-3.yaml":pods "constraints-mem-demo-3" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi. 创建没有指定任何内存limit和request的pod 123456789101112131415161718192021$ cat memory-constraints-pod-4.yamlapiVersion: v1kind: Podmetadata: name: constraints-mem-demo-4spec: containers: - name: constraints-mem-demo-4-ctr image: nginx$ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-constraints-pod-4.yaml --namespace=constraints-mem-example# 查看Pod$ kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml...resources: limits: memory: 1Gi requests: memory: 1Gi... 容器没有指定自己的 CPU 请求和限制，所以它将从 LimitRange 获取默认的 CPU 请求和限制值。 2.2. CPU的最大最小值创建LimitRange 123456789101112131415161718192021222324252627282930313233# 创建namespace$ kubectl create namespace constraints-cpu-example# 创建LimitRange$ cat cpu-constraints.yamlapiVersion: v1kind: LimitRangemetadata: name: cpu-min-max-demo-lrspec: limits: - max: cpu: "800m" min: cpu: "200m" type: Container $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints.yaml --namespace=constraints-cpu-example# 查看LimitRange$ kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example...limits:- default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container... 创建符合要求的Pod 1234567891011121314151617181920212223242526$ cat cpu-constraints-pod.yamlapiVersion: v1kind: Podmetadata: name: constraints-cpu-demospec: containers: - name: constraints-cpu-demo-ctr image: nginx resources: limits: cpu: "800m" requests: cpu: "500m" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod.yaml --namespace=constraints-cpu-example# 查看Pod$ kubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example...resources: limits: cpu: 800m requests: cpu: 500m... 创建超过最大CPU limit的Pod 1234567891011121314151617181920$ cat cpu-constraints-pod-2.yamlapiVersion: v1kind: Podmetadata: name: constraints-cpu-demo-2spec: containers: - name: constraints-cpu-demo-2-ctr image: nginx resources: limits: cpu: "1.5" requests: cpu: "500m" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example# Pod创建失败，因为容器指定的CPU limit过大Error from server (Forbidden): error when creating "docs/tasks/administer-cluster/cpu-constraints-pod-2.yaml":pods "constraints-cpu-demo-2" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m. 创建小于最小CPU request的Pod 1234567891011121314151617181920$ cat cpu-constraints-pod-3.yamlapiVersion: v1kind: Podmetadata: name: constraints-cpu-demo-4spec: containers: - name: constraints-cpu-demo-4-ctr image: nginx resources: limits: cpu: "800m" requests: cpu: "100m" $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example# Pod创建失败，因为容器指定的CPU request过小Error from server (Forbidden): error when creating "docs/tasks/administer-cluster/cpu-constraints-pod-3.yaml":pods "constraints-cpu-demo-4" is forbidden: minimum cpu usage per Container is 200m, but request is 100m. 创建没有指定任何CPU limit和request的pod 123456789101112131415161718192021$ cat cpu-constraints-pod-4.yamlapiVersion: v1kind: Podmetadata: name: constraints-cpu-demo-4spec: containers: - name: constraints-cpu-demo-4-ctr image: vish/stress $ kubectl create -f https://k8s.io/docs/tasks/administer-cluster/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example # 查看Podkubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml...resources: limits: cpu: 800m requests: cpu: 800m... 容器没有指定自己的 CPU 请求和限制，所以它将从 LimitRange 获取默认的 CPU 请求和限制值。 2.3. 说明LimitRange 在 namespace 中施加的最小和最大内存（CPU）限制只有在创建和更新 Pod 时才会被应用。改变 LimitRange 不会对之前创建的 Pod 造成影响。 Kubernetes 都会执行下列步骤： 如果容器没有指定自己的内存（CPU）请求（request）和限制（limit），系统将会为其分配默认值。 验证容器的内存（CPU）请求大于等于最小值。 验证容器的内存（CPU）限制小于等于最大值。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knative介绍]]></title>
    <url>%2F2019%2F03%2F18%2FKnative%2F</url>
    <content type="text"><![CDATA[Knative 是什么Knative 基于Kubernetes的平台，用来构建、部署和管理现代serverless工作负载。该框架试图将开发云原生应用在三个领域的最佳实践结合起来，这三个领域指的是构建容器（和函数）、为工作负载提供服务（和动态扩展）以及事件。Knative是由谷歌与Pivotal、IBM、Red Hat 和SAP紧密协作开发的。 Knative组件Build描述: Knative Build 扩展了 Kubernetes，并利用现有 Kubernetes 原语为你提供从源码运行集群容器构建的能力。 例如，你可以编写一个 Kubernetes-native 的 build 来从代码库获取源代码，将其构建到容器镜像中，然后运行该容器镜像。 虽然 Knative build 针对构建、测试和部署源代码进行了优化，但是您仍然需要负责开发相应的组件： 从代码库中检索并获取源代码； 在共享文件系统中运行多个顺序作业，比如： 安装依赖； 运行单元和集成测试； 构建容器镜像； 推送容器镜像到镜像仓库，或部署容器镜像到集群； Knative Build 的目标是提高一个标准的、可移植的、可重用的、性能优化的方法，用户定义和运行集群容器镜像的构建。通过提供在 Kubernetes 上运行构建“枯燥而困难”的任务，Knative 使您不必开发和复制这些基于 Kubernetes 的开发过程。 尽管在现在，Knative build 并没有提供一套完整的 CI/CD 解决方案，但是它提供了一个较为低级的构建块，它是有意设计的，以便在更大的系统中实现基础和利用。 Build对象是Kubernetes集群的CRD（自定义资源）: 123456789101112131415161718192021222324252627apiVersion: build.knative.dev/v1alpha1kind: Buildmetadata: name: example-build-namespec: serviceAccountName: build-auth-example source: git: url: https://github.com/example/build-example.git revision: master steps: - name: ubuntu-example image: ubuntu args: ["ubuntu-build-example", "SECRETS-example.md"] steps: - image: gcr.io/example-builders/build-example args: ['echo', 'hello-example', 'build'] steps: - name: dockerfile-pushexample image: gcr.io/example-builders/push-example args: ["push", "${IMAGE}"] volumeMounts: - name: docker-socket-example mountPath: /var/run/docker.sock volumes: - name: example-volume emptyDir: {} Build对象: 一个Build对象可以包含多个steps，每个step指定一个Builder。（step可以近似理解为 init container） 一个Build对象是一种创建出来完成任何任务的容器镜像，无论这是流程中的单个步骤，还是整个流程本身。 Build对象中的steps可以推送到repository。 使用BuildTemplate可定义可重用模板。 Build对象中的source可以被定义为挂载到Kubernete volumes，source支持git repository、Google Cloud Storage、任意容器镜像。 使用ServiceAccount+k8s Secrets进行认证。 Serving描述: Knative Serving 建立在 Kubernetes 和 Istio 之上，以支持部署和服务无服务函数和应用。Knative Serving 项目提供了包含以下中间件原语的功能： 无服务容器的快速部署； 自动扩容，自动缩容乃至到0； 基于 Istio 组件的路由和网络编程； 已部署代码和配置的时间点快照； resources: Service: service.serving.knative.dev Route: route.serving.knative.dev Configuration: configuration.serving.knative.dev Revision: revision.serving.knative.dev resource关系图: Eventing设计目标:Knative Eventing 是用来解决什么问题的？ 在开发期间，Services 都是松耦合的，并且独立的部署在各种平台上 (Kubernetes, VMs, SaaS or FaaS)。 生产者能在消费者监听之前生成事件，消费者也能在生产者产生事件之前订阅感兴趣的事件。 Services 能在以下两种场景下被连接 (be connected to) 以创建新的应用： 不修改生产者或消费者。 有能力从特定的生产者中选择特定的事件子集。 从架构上分离出三个抽象组件: Buses Buses 提供了基于 Kafka 或 NATS 等消息总线的 k8s-native 的抽象。说白了就是 pubsub 机制。事件被发布到一个订阅通道，之后被路由到订阅的各方。 Knative Eventing 目前支持三种 buses：Stub, Kafka, GCP PubSub。 Sources Sources 提供了一个类似的抽象层，用于从 Kubernetes外部提供数据源，并将它们路由到集群，以提要的形式表示。 目前提供三种源：K8sevents, GitHub, GCP PubSub。 Flows Flow是在事件中最顶层的用户面对的概念;它描述了从外部事件源到对事件作出反应的目的地的所需路径。 抽象架构图:]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>serverless</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd raft协议]]></title>
    <url>%2F2019%2F03%2F15%2Fetcd-raft%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[etcd raft选举机制etcd 是一个分布式的k/V存储系统。核心使用了RAFT分布式一致性协议。一致性这个概念，它是指多个服务器在状态达成一致，但是在一个分布式系统中，因为各种意外可能，有的服务器可能会崩溃或变得不可靠，它就不能和其他服务器达成一致状态。这样就需要一种Consensus协议，一致性协议是为了确保容错性，也就是即使系统中有一两个服务器当机，也不会影响其处理过程。 为了以容错方式达成一致，我们不可能要求所有服务器100%都达成一致状态，只要超过半数的大多数服务器达成一致就可以了，假设有N台服务器，N/2 +1 就超过半数，代表大多数了。 raft协议核心要点： Leader选举（Leader Election） 日志同步 （Log Replication） leader收到client的更新请求后，会讲更新的内容同步给所有follower。 集群状态的正确性 （Safety） 保证日志的一致性 保证选举的正确性 服务器状态： leader 处理所有客户端交互，日志复制等，一个任期只有一个。 follower 完全被动的选民，是只读的。 candidate 候选人，可以被选举为新领导。 状态之间的转换图： 任期（terms） 如上图，蓝色代表 Election 模式，绿色代表 Operation 模式 在每个任期内最多一个leader 有些可能没有leader 每一个服务会维护当前的任期值 每一个rpc请求中都会携带term值 如果一个peer实例拥有老的term值，则更新为最新的term值并状态变为follower 一旦一个服务选举为leader，就会进入 operation 模式 Leader选举etcd服务启动后，会进入 follower 状态，leader 心跳超时后会进入选举状态。选举总体流程图如下： 选举流程分解 初始状态都是Follower S1 超时, 变为Candidate，开始选举, 发起投票请求 S1 变为Leader S2 和 S3 同意投票给S1 Leader S1开始接受客户端写请求 Leader接受到客户端写请求后，会将数据更新写入到log中 如果S2和S3收到客户端写请求，会将请求转发到Leader S1 Leader会异步的将更新的log同步到Follower S2和S3 超过多数的Follower将数据成功同步到log后，Leader会将该条数据更新为Committed状态，Committed index会随着增长。 选举的正确性 在每一任期内，最多允许一个服务被选举为leader 在一个任期内，一个服务只能投一票 只有获得大多数投票才能作为leader 如果有多个candidate，最终一定会有一个被选举为leader 如果多个candidate同时发起了选举，导致都没有获得大多数选票时，每一个candidate会随机等待一段时间后重新发起新一轮投票（一般是随机等待150-300ms） 日志的一致性 客户端写入数据到 leader： leader 将数据写入到 log leader将更新的数据广播到所有的followers 多数follower成功写入log后，leader会将该数据提交到状态机 leader 把数据提交后，返回给client结果 在下一个心跳中，leader 通知follower更新已经提交的数据 Crashed/slow followers ？ leader会一直重试同步数据到follower，直到成功]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s联邦集群]]></title>
    <url>%2F2019%2F03%2F14%2Fk8s%E8%81%94%E9%82%A6%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[Federation 概述Federation(集群联邦)是kubernetes社区中的多云管理项目，可以方便地跨地区跨服务商管理多个Kubernetes集群。其最初在1.3版本中被引入，后从主库迁移到独立repo(v1版本)，由于v1版本存在的若干问题，现已切换到v2版本。v1、v2版本虽然架构有较大差异，但共同目标都是使管理多个集群更为简单，主要实现了以下俩个模型： 跨集群的资源同步与伸缩(Sync and Scale resources across clusters)：提供在多个集群中保持资源同步与伸缩的功能，例如确保一个Deployment可以运行在多个集群中，并根据负载情况在集群间合理伸缩。跨级群的服务发现(Cross cluster discovery)：提供自动配置DNS服务的功能，实现应用跨集群服务发现的能力，例如在某一集群中可以访问另一集群的应用。 Federation v1Federation v1的架构跟k8s集群的架构非常类似，整体架构如下： 主要包含四个组件： federation-apiserver：类似kube-apiserver，兼容k8s API，只是对联邦处理的特定资源做了过滤（部分资源联邦不支持，故使用apiserver来过滤）。 federation-controller-manager：提供多个集群间资源调度及状态通同步，工作原理类似kube-controller-manager。 kubefed：Federation CLI工具，用来将子集群加入到联邦中。 etcd：存储federation层面的资源对象，供federation control plane同步状态。 Federation v2Federation v2版本在v1的基础上，进一步简练、增强，主要功能仍然是跨地区跨服务商管理多个Kubernetes集群。其通过当下大热的CRD模型定义了独立的API，同时仍通过ControllerManager模型来同步、调度资源，通过kubefed2来将子集群加入联邦。CRD与ControllerManager组成的Control Plane模型（去除了v1中独立APIServer、Etcd），使其可以部署在任意的k8s集群中，同时还可将该集群也join到联邦控制面作为子集群，整体定义模型如下： Federation v2 采用 CRD 模式运行在 Kubernetes 集群中，扩展了 k8s 的资源管理能力。首先，它向原有 k8s 集群注册一系列 CRD 资源，这些 CRD 定义了联邦系统所支持的 k8s 资源；然后，它通过开启一个 ControllerManager 来管理这些 CRD 资源，实现跨集群资源调度。CRD 资源和 ControllerManager 共同构成了 Federation v2 的 Control 目前 Federation v2 主要定义了 4 种 CRD 资源： Cluster configuration: 主要定义了子集群注册时的配置信息，其中主要引用了Cluster-Registry[3]这个子项目来定义cluster的配置信息。用户只需执行kubefed2 join将安装好的集群加入联邦，federation-controller-manager会自动读取新加入集群的context信息，生成cluster configuration信息，并持久化到etcd中，供后续消费。 Type configuration: 主要定义了federation可以处理哪些资源对象(在v1版本中靠独立APIServer来过滤)，例如使federation处理deployment，就创建一个deployment type configuration。Type configuration中又包含了三种类型的CRD资源： Template：定义了federation要处理的资源对象，含有该对象的全部信息，例如depoyment的template中就直接引用了k8s的deployment。 Placement：定义要将资源对象运行在哪些子集群中，如不定义该对象，则资源不会运行在任一集群。在v1版本中资源是会默认下发到每一个集群中。 Override：对于同一资源对象，在不同服务商的集群配置中可能有会有差异。例如deployment对象，其中volume可能不同云厂商实现有所不同，所以需要差异化配置volume字段，Overide就提供了差异化修改template中字段的能力（当前仅支持部分字段，后续会支持全部字段差异化修改)。 Schedule: 主要定义应用在集群中的调度分布，该类型主要涉及Deployment与Replicaset俩种（该配置在v1中写在对象的annotations中）。用户可以定义对象在每个集群中分布的最多、最少实例数，并且还能在集群中做到应用实例数的均衡分布。值得注意的是，如果调度结果跟用户自定义的override冲突时，该调度算法享有优先权。例如用户override中定义为5个实例，实际调度结果只有3个，那么自定义的override中5个实例将被改为3个。 MultiClusterDNS: 如字段名，该资源主要在做多集群间的服务发现，其下主要包含ServiceDNSRecord、IngressDNSRecord、DNSEndpoint这几个资源对象。整个工作流程为： 用户首先创建Service资源，需要创建Service Template、Placement、Override（可选）三个对象，使Service分布到各子集群。 创建ServiceDNSRecord/IngressDNSRecord资源，federation-controller会根据该资源的配置，收集各子集群对应的service信息，最后生成由域名与IP组合而成的DNSEndpoint资源并持久化到etcd中。 将federation-controller创建的DNSEndpoint资源中的域名与IP自动配置到DNS服务商的服务器上，可通过external-dns项目自动配置。 这样，就可以实现不同集群中应用的服务发现，其实质是将各集群服务的IP与对应域名配置到公网的DNS服务器上，以通过公网域名实现跨集群服务发现。 总结Federation v2中的CRD配置复杂，但是能够根据负载情况，调度并调节各集群的资源分布，同时能够提供跨集群的应用故障转移，以及跨集群的服务发现。实现了方便的跨地区、跨服务商配置并管理多个k8s集群，以及多集群资源的统一管理]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>federation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大规模集群配置优化]]></title>
    <url>%2F2019%2F03%2F12%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[节点配额和内核参数调整对于公有云上的 Kubernetes 集群，规模大了之后很容器碰到配额问题，需要提前在云平台上增大配额。这些需要增大的配额包括 虚拟机个数 vCPU 个数 内网 IP 地址个数 公网 IP 地址个数 安全组条数 路由表条数 持久化存储大小 参考gce随着node节点的增加master节点的配置： 1-5 nodes: n1-standard-1 6-10 nodes: n1-standard-2 11-100 nodes: n1-standard-4 101-250 nodes: n1-standard-8 251-500 nodes: n1-standard-16 more than 500 nodes: n1-standard-32 参考阿里云配置： 节点规模 master规格 1-5个节点 4C8G(不建议2C4G) 6-20个节点 4C16G 21-100个节点 8C32G 100-200个节点 16C64G 增大内核选项配置 /etc/sysctl.conf： 12345678910111213141516171819202122232425262728fs.file-max=1000000# max-file 表示系统级别的能够打开的文件句柄的数量， 一般如果遇到文件句柄达到上限时，会碰到# "Too many open files"或者Socket/File: Can’t open so many files等错误。# 配置arp cache 大小net.ipv4.neigh.default.gc_thresh1=1024# 存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。net.ipv4.neigh.default.gc_thresh2=4096# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。net.ipv4.neigh.default.gc_thresh3=8192# 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。# 以上三个参数，当内核维护的arp表过于庞大时候，可以考虑优化net.netfilter.nf_conntrack_max=10485760# 允许的最大跟踪连接条目，是在内核内存中netfilter可以同时处理的“任务”（连接跟踪条目）net.netfilter.nf_conntrack_tcp_timeout_established=300net.netfilter.nf_conntrack_buckets=655360# 哈希表大小（只读）（64位系统、8G内存默认 65536，16G翻倍，如此类推）net.core.netdev_max_backlog=10000# 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。fs.inotify.max_user_instances=524288# 默认值: 128 指定了每一个real user ID可创建的inotify instatnces的数量上限fs.inotify.max_user_watches=524288# 默认值: 8192 指定了每个inotify instance相关联的watches的上限 参考资料： 关于conntrack的详细说明：https://testerhome.com/topics/7509 Etcd 搭建高可用的etcd集群, 集群规模增大时可以自动增加etcd节点 目前的解决方案是使用etcd operator来搭建etcd 集群，operator是CoreOS推出的旨在 简化复杂有状态应用管理的框架，它是一个感知应用状态的控制器，通过扩展Kubernetes API来自动创建、管理和配置应用实例。 etcd operator 有如下特性： ceate/destroy: 自动部署和删除 etcd 集群，不需要人额外干预配置。 resize：可以动态实现 etcd 集群的扩缩容。 backup： 支持etcd集群的数据备份和恢复重建。 upgrade： 可以实现在升级etcd集群时不中断服务。 配置etcd使用ssd固态盘存储 设置 –quota-backend-bytes 增大etcd的存储限制。默认值是 2G 需要配置单独的 Etcd 集群存储 kube-apiserver 的 event。 参考资料： etcd operator 镜像拉取相关配置Docker配置 设置 max-concurrent-downloads=10 配置docker daemon 并行拉取镜像，提高镜像拉取效率。 使用 SSD 存储。 预加载 pause 镜像，比如 docker image save -o /opt/preloaded_docker_images.tar 和 docker image load -i /opt/preloaded_docker_images.tar 启动pod时都会拉取pause镜像，为了减小拉取pause镜像网络带宽，可以每个node预加载pause镜像。 Kubelet配置 设置 –serialize-image-pulls=false 该选项配置串行拉取镜像，默认值时true，配置为false可以增加并发度。但是如果docker daemon 版本小于 1.9，且使用 aufs 存储则不能改动该选项。 设置 –image-pull-progress-deadline=30 配置镜像拉取超时。默认值时1分，对于大镜像拉取需要适量增大超时时间。 Kubelet 单节点允许运行的最大 Pod 数：–max-pods=110（默认是 110，可以根据实际需要设置） 镜像registry p2p分发 使用 Dragonfly 文件分发系统来实现镜像的分布式p2p分发。减少 镜像仓库的负载。 参考资料： Dragonfly kube-api-server 配置node节点数量 >= 3000， 推荐设置如下配置： –max-requests-inflight=3000 –max-mutating-requests-inflight=1000 node节点数量在 1000 – 3000， 推荐设置如下配置： –max-requests-inflight=1500 –max-mutating-requests-inflight=500 内存配置选项和node数量的关系，单位是MB： –target-ram-mb=node_nums * 60 pod配置在运行 Pod 的时候也需要注意遵循一些最佳实践，比如： 为容器设置资源请求和限制，尤其是一些基础插件服务spec.containers[].resources.limits.cpuspec.containers[].resources.limits.memoryspec.containers[].resources.requests.cpuspec.containers[].resources.requests.memoryspec.containers[].resources.limits.ephemeral-storagespec.containers[].resources.requests.ephemeral-storage在k8s中，会根据pod的limit 和 requests的配置将pod划分为不同的qos类别：- Guaranteed - Burstable - BestEffort 当机器可用资源不够时，kubelet会根据qos级别划分迁移驱逐pod。被驱逐的优先级：BestEffort > Burstable > Guaranteed 对关键应用使用 nodeAffinity、podAffinity 和 podAntiAffinity 等保护，使其调度分散到不同的node上。比如kube-dns 配置： 尽量使用控制器来管理容器（如 Deployment、StatefulSet、DaemonSet、Job 等） Kube-scheduler 配置 设置 –kube-api-qps=100 默认值是50 Kube-controller-manager 配置 设置 –kube-api-qps=100 默认值是20 设置 –kube-api-burst=100 默认值是30]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ETCD集群在线升级]]></title>
    <url>%2F2019%2F03%2F08%2Fetcd%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[将etcd从3.2升级到3.3在一般情况下，从etcd 3.2升级到3.3可以是零停机，滚动升级： 一个接一个，停止etcd v3.2进程并用etcd v3.3进程替换它们（一个升级之后，等待集群状态正常之后再升级下一个） 运行所有v3.3进程后，群集可以使用v3.3中的新功能 具体操作： 登陆到master01，master02，master03，将/etc/kubernetes/manifests/中的etcd.yaml复制到临时目录，注意：一定做好etcd.yaml的备份 首先修改master01，修改临时目录下etcd.yaml，直接修改版本号，以及启动命令 将修改完的etcd.yaml复制到/etc/kubernetes/manifests/，执行kubectl get po -n kube-system|grep etcd查看etcd的状态，等待etcd的三个节点都正常之后，用etcdctl查看集群状态 etcdctl member list 三个etcd的member都正常之后，查看etcd日志，显示升级成功。 依次在master02，master03上执行相同的操作以及操作检查。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSI存储接口解释]]></title>
    <url>%2F2019%2F03%2F06%2FCSI%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[什么是CSIContainer Storage Interface是面向容器编排系统的存储接口规范，定义了容器编排系统和各种存储之间如何交互。 CSI并不是kubernetes专有的技术或者规范，整个容器社区（包括Mesos、Docker、Kubernetes等）都在推进和定义该规范。理论上，一个遵从CSI规范的实现，能够运行在所有的容器编排系统上。 kubernetes存储插件的发展Kubernetes 里的存储插件可以分为 In-tree 和 Out-of-tree 这两大类。 In-tree 存储插件的代码是在 Kubernetes 核心代码库里，跟随 Kubernetes 打包发布，从维护性上来说并不是很好维护。 Out-of-tree 存储插件的代码独立于 Kubernetes，也可以独立构建发布，比较易于维护。kubernetes社区推荐开发和使用 Out-of-tree 存储插件。 Out-of-tree 存储插件现在分为 FlexVolume 和 CSI 两大类。 FlexVolume 插件是 Kubernetes1.2 开始支持的，以二进制可执行文件的方式运行在节点的指定路径上，一个名为flexvolume的in-tree插件会在需要时和FlexVolume插件交互。 kubernetes从1.9版本引入CSI，1.13版本变为GA状态。CSI 插件部署起来比较简单，支持容器化部署，功能比较强大，除了存储卷管理功能外，还有快照管理功能。CSI 的存储标准方案在持续快速发展中，功能还在不断扩展。` 概述CSI插件主要有2个独立的组件： Node Plugin（节点插件）：对主机上的存储卷进行相应的操作。假设一个kubernetes集群有3个计算节点，那么这3个计算节点上都需要部署Node Plugin。 Controller Plugin（控制插件）：从存储服务端角度对存储卷进行管理和操作。可以运行在集群中的任何节点上（甚至是master节点）。 但只能有一个副本在运行。 另外还有一个Identify组件，用来获取插件的信息，检查插件的状态。 接口规范Indentity接口 GetPluginInfo：返回插件的名字和版本 GetPluginCapabilities：返回插件的功能点，是否支持存储卷创建、删除等功能，是否支持存储卷挂载的功能 Probe：返回插件的健康状态（是否在运行中） Controller接口 CreateVolume：创建一个存储卷（如EBS盘） DeleteVolume：删除一个已创建的存储卷（如EBS盘） ControllerPublishVolume：将一个已创建的存储卷，挂载（attach）到指定的节点上 ControllerUnpublishVolume：从指定节点上，卸载（detach）指定的存储卷 ValidateVolumeCapabilities：返回存储卷的功能点，如是否支持挂载到多个节点上，是否支持多个节点同时读写 ListVolumes：返回所有存储卷的列表 GetCapacity：返回存储资源池的可用空间大小 ControllerGetCapabilities：返回controller插件的功能点，如是否支持GetCapacity接口，是否支持snapshot功能等 Node接口 NodeStageVolume：如果存储卷没有格式化，首先要格式化。然后把存储卷mount到一个临时的目录（这个目录通常是节点上的一个全局目录）。再通过NodePublishVolume将存储卷mount到pod的目录中。mount过程分为2步，原因是为了支持多个pod共享同一个volume（如NFS）。 NodeUnstageVolume：NodeStageVolume的逆操作，将一个存储卷从临时目录umount掉 NodePublishVolume：将存储卷从临时目录mount到目标目录（pod目录） NodeUnpublishVolume：将存储卷从pod目录umount掉 NodeGetId：返回插件运行的节点的ID NodeGetCapabilities：返回Node插件的功能点，如是否支持stage/unstage功能 存储卷的生命周期当用户向kubernetes发起创建存储卷的请求时，kubernetes会调用 Controller 插件的 CreateVolume 接口，发起创建存储卷的请求。 CreateVolume 接口会调用云平台 API，创建一个块设备并将设备的信息返回给kubernetes。此时存储卷处于CREATED状态。 之后存储卷要被用户的应用所使用，kubernetes会调度容器到某台主机上，并调用 CSI 插件的 ControllerPublishVolume 接口，将存储卷挂载至这台主机上。 ControllerPublishVolume 会调用云平台 API 实现相关操作。当主机上能够看到这个块设备以后，这个状态处于Node Ready状态。 接下来，kubernetes会调用 CSI 插件的 NodeStageVolume 接口对存储卷进行分区格式化操作。完成之后状态称之为 Volume Ready 状态。 最后，kubernetes调用NodePublishVolume将存储卷mount到容器的指定目录下，完成之后进入Published状态。 存储卷的卸载，是上述过程的逆过程，不再赘述。 部署架构图推荐使用statefulset部署controller plugin，使用daemonset部署Node plugin]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>volume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AutoScaler设计文档]]></title>
    <url>%2F2019%2F03%2F06%2FAutoScaler%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[背景Kubernetes服务简化了K8S集群的创建、升级和手动扩缩容。然而使用Kubernetes集群经常问到的一个问题是，我应该保持多大的节点规模来满足应用需求呢？ Autoscaler的出现解决了这个问题，它可以自动的根据部署的应用所请求的资源量来动态的伸缩集群 名词解释 cluster-autoscaler (简称CA)是用来弹性伸缩kubernetes集群的。我们在使用kubernetes集群经常问到的一个问题是，我应该保持多大的节点规模来满足应用需求呢？ cluster-autoscaler的出现解决了这个问题，它可以自动的根据部署的应用所请求的资源量来动态的伸缩集群。 架构 Autoscaler：核心模块，负责整体扩缩容功能 Estimator：负责评估计算扩容节点 Simulator：负责模拟调度，计算缩容节点 CA Cloud-Provider：与云交互进行节点的增删操作。社区目前仅支持AWS和GCE，其他云厂商需要自己实现CloudProvider和NodeGroup相关接口。 CA的架构如下图所示： 设计思路实现流程设计 用户登陆创建自动伸缩集群 创建kubernetes集群成功后，调用伸缩组的OpenAPI，创建启动配置 创建自动伸缩组 启用自动伸缩组 集群节点绑定伸缩组 创建集群自动伸缩的Deployment（CA） CA根据Pending Pod数伸缩节点 实现实现CloudProvider和NodeGroup相关接口 123456789101112131415161718192021222324252627282930313233343536type CloudProvider interface { // Name returns name of the cloud provider. Name() string // NodeGroups returns all node groups configured for this cloud provider. NodeGroups() []NodeGroup // NodeGroupForNode returns the node group for the given node, nil if the node // should not be processed by cluster autoscaler, or non-nil error if such // occurred. Must be implemented. NodeGroupForNode(*apiv1.Node) (NodeGroup, error) // Pricing returns pricing model for this cloud provider or error if not available. // Implementation optional. Pricing() (PricingModel, errors.AutoscalerError) // GetAvailableMachineTypes get all machine types that can be requested from the cloud provider. // Implementation optional. GetAvailableMachineTypes() ([]string, error) // NewNodeGroup builds a theoretical node group based on the node definition provided. The node group is not automatically // created on the cloud provider side. The node group is not returned by NodeGroups() until it is created. // Implementation optional. NewNodeGroup(machineType string, labels map[string]string, systemLabels map[string]string, taints []apiv1.Taint, extraResources map[string]resource.Quantity) (NodeGroup, error) // GetResourceLimiter returns struct containing limits (max, min) for resources (cores, memory etc.). GetResourceLimiter() (*ResourceLimiter, error) // Cleanup cleans up open resources before the cloud provider is destroyed, i.e. go routines etc. Cleanup() error // Refresh is called before every main loop and can be used to dynamically update cloud provider state. // In particular the list of node groups returned by NodeGroups can change as a result of CloudProvider.Refresh(). Refresh() error} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263type NodeGroup interface { // MaxSize returns maximum size of the node group. MaxSize() int // MinSize returns minimum size of the node group. MinSize() int // TargetSize returns the current target size of the node group. It is possible that the // number of nodes in Kubernetes is different at the moment but should be equal // to Size() once everything stabilizes (new nodes finish startup and registration or // removed nodes are deleted completely). Implementation required. TargetSize() (int, error) // IncreaseSize increases the size of the node group. To delete a node you need // to explicitly name it and use DeleteNode. This function should wait until // node group size is updated. Implementation required. IncreaseSize(delta int) error // DeleteNodes deletes nodes from this node group. Error is returned either on // failure or if the given node doesn't belong to this node group. This function // should wait until node group size is updated. Implementation required. DeleteNodes([]*apiv1.Node) error // DecreaseTargetSize decreases the target size of the node group. This function // doesn't permit to delete any existing node and can be used only to reduce the // request for new nodes that have not been yet fulfilled. Delta should be negative. // It is assumed that cloud provider will not delete the existing nodes when there // is an option to just decrease the target. Implementation required. DecreaseTargetSize(delta int) error // Id returns an unique identifier of the node group. Id() string // Debug returns a string containing all information regarding this node group. Debug() string // Nodes returns a list of all nodes that belong to this node group. Nodes() ([]string, error) // TemplateNodeInfo returns a schedulercache.NodeInfo structure of an empty // (as if just started) node. This will be used in scale-up simulations to // predict what would a new node look like if a node group was expanded. The returned // NodeInfo is expected to have a fully populated Node object, with all of the labels, // capacity and allocatable information as well as all pods that are started on // the node by default, using manifest (most likely only kube-proxy). Implementation optional. TemplateNodeInfo() (*schedulercache.NodeInfo, error) // Exist checks if the node group really exists on the cloud provider side. Allows to tell the // theoretical node group from the real one. Implementation required. Exist() bool // Create creates the node group on the cloud provider side. Implementation optional. Create() error // Delete deletes the node group on the cloud provider side. // This will be executed only for autoprovisioned node groups, once their size drops to 0. // Implementation optional. Delete() error // Autoprovisioned returns true if the node group is autoprovisioned. An autoprovisioned group // was created by CA and can be deleted when scaled to 0. Autoprovisioned() bool} CA功能接口创建CA123456789101112131415161718192021222324252627282930313233apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscalerspec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler spec: serviceAccountName: admin containers: - image: hub.kce.ksyun.com/ksyun/cluster-autoscaler:v1.1.0 name: cluster-autoscaler resources: requests: cpu: 100m memory: 300Mi command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=alicloud - --skip-nodes-with-local-storage=false - --nodes=${MIN}:${MAX}:${AUTO_SCALER_GROUP} - --nodes=${MIN2}:${MAX2}:${AUTO_SCALER_GROUP2} 参数 ${MIN}: ${MAX}: ${AUTO_SCALER_GROUP} MIN: 伸缩组节点最小值 MAX: 伸缩组节点最大值 AUTO_SCALER_GROUP: 伸缩组ID 修改CA123456789101112131415161718192021222324252627282930313233apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscalerspec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler spec: serviceAccountName: admin containers: - image: hub.kce.ksyun.com/ksyun/cluster-autoscaler:v1.1.0 name: cluster-autoscaler resources: requests: cpu: 100m memory: 300Mi command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=alicloud - --skip-nodes-with-local-storage=false - --nodes=1:100:${AUTO_SCALER_GROUP} - --nodes=1:3:${AUTO_SCALER_GROUP2} 参数：${MIN}:${MAX}:${AUTO_SCALER_GROUP} MIN: 伸缩组节点最小值 MAX: 伸缩组节点最大值 AUTO_SCALER_GROUP: 伸缩组ID 删除CA12345678910111213141516171819202122232425262728293031apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscalerspec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler spec: serviceAccountName: admin containers: - image: hub.kce.ksyun.com/ksyun/cluster-autoscaler:v1.1.0 name: cluster-autoscaler resources: requests: cpu: 100m memory: 300Mi command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=alicloud - --skip-nodes-with-local-storage=false]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AutoScaler源码梳理]]></title>
    <url>%2F2019%2F03%2F06%2FAutoScaler%E6%BA%90%E7%A0%81%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Cluster AutoScaler源码梳理功能总结: 删除nodegroup中15分钟没有注册到kube的node 获取未调度的Pod列表，计算需要资源（limit之内） 获取comingnode（在nodegroup内，但是未注册kube集群，启动中）+node模版，计算（base，binpacking）的需要的node数 根据expander(random,most-pods,price,least-waste)进行scaleup 更新nodegroup的currentsize cluster ca 入口函数: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485func main() { leaderElection := defaultLeaderElectionConfiguration() //默认进行选举 leaderElection.LeaderElect = true bindFlags(&leaderElection, pflag.CommandLine) kube_flag.InitFlags() //初始化健康检查 超过15分钟失败 healthCheck := metrics.NewHealthCheck(*maxInactivityTimeFlag, *maxFailingTimeFlag) glog.V(1).Infof("Cluster Autoscaler %s", ClusterAutoscalerVersion) correctEstimator := false //评估算法 basic binpacking for _, availableEstimator := range estimator.AvailableEstimators { if *estimatorFlag == availableEstimator { correctEstimator = true } } if !correctEstimator { glog.Fatalf("Unrecognized estimator: %v", *estimatorFlag) } //注册监控 //注册健康检查 go func() { http.Handle("/metrics", prometheus.Handler()) http.Handle("/health-check", healthCheck) err := http.ListenAndServe(*address, nil) glog.Fatalf("Failed to start metrics: %v", err) }() if !leaderElection.LeaderElect { run(healthCheck) } else { //默认 id, err := os.Hostname() if err != nil { glog.Fatalf("Unable to get hostname: %v", err) } kubeClient := createKubeClient(getKubeConfig()) // Validate that the client is ok. _, err = kubeClient.CoreV1().Nodes().List(metav1.ListOptions{}) if err != nil { glog.Fatalf("Failed to get nodes from apiserver: %v", err) } lock, err := resourcelock.New( leaderElection.ResourceLock, *namespace, "cluster-autoscaler", kubeClient.CoreV1(), resourcelock.ResourceLockConfig{ Identity: id, EventRecorder: kube_util.CreateEventRecorder(kubeClient), }, ) if err != nil { glog.Fatalf("Unable to create leader election lock: %v", err) } kube_leaderelection.RunOrDie(kube_leaderelection.LeaderElectionConfig{ Lock: lock, LeaseDuration: leaderElection.LeaseDuration.Duration, RenewDeadline: leaderElection.RenewDeadline.Duration, RetryPeriod: leaderElection.RetryPeriod.Duration, Callbacks: kube_leaderelection.LeaderCallbacks{ OnStartedLeading: func(_ 1 { var buffer bytes.Buffer for i, ng := range targetNodeGroups { if i > 0 { buffer.WriteString(", ") } buffer.WriteString(ng.Id()) } glog.V(0).Infof("Splitting scale-up between %v similar node groups: {%v}", len(targetNodeGroups), buffer.String()) } } scaleUpInfos, typedErr := nodegroupset.BalanceScaleUpBetweenGroups( targetNodeGroups, newNodes) if typedErr != nil { return nil, typedErr } glog.V(0).Infof("Final scale-up plan: %v", scaleUpInfos) for _, info := range scaleUpInfos { typedErr := executeScaleUp(context, clusterStateRegistry, info, gpu.GetGpuTypeForMetrics(nodeInfo.Node(), nil)) if typedErr != nil { return nil, typedErr } } clusterStateRegistry.Recalculate() glog.V(0).Info("kce cloud provider end scale up ***********************") return &status.ScaleUpStatus{ ScaledUp: true, ScaleUpInfos: scaleUpInfos, PodsRemainUnschedulable: getRemainingPods(podsRemainUnschedulable), PodsTriggeredScaleUp: bestOption.Pods, PodsAwaitEvaluation: getPodsAwaitingEvaluation(unschedulablePods, podsRemainUnschedulable, bestOption.Pods)}, nil } return &status.ScaleUpStatus{ScaledUp: false, PodsRemainUnschedulable: getRemainingPods(podsRemainUnschedulable)}, nil}]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flannel优化方案]]></title>
    <url>%2F2019%2F03%2F05%2Fflannel%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[flannel cni组件 优化flannel 使用主机路由 网络架构优化ksyun-vpc 原有网络模型 ksyun-vpc 优化活网络模型 优化方向说明： 原有网络方式是在没有个节点上多有生成一个cni0 的网桥， 让后创建一个pod 并把pod 的veth对加入到cni0网桥，然后在从cni0 中（走默认路由）在转到eth0网卡 去掉cni0 网桥， 创建pod 时候不添加到cni0网桥中 需要修改模块 flanneld-cni 插件 仿照caclio-cni 去实现 flanneld-cni 和 caclio-cni 源码可上github 官网找到]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s调度权重]]></title>
    <url>%2F2019%2F03%2F01%2Fk8s%E8%B0%83%E5%BA%A6%E6%9D%83%E9%87%8D%2F</url>
    <content type="text"><![CDATA[k8s 调度算法算法需要经过两个阶段，分别是过滤和打分，首先过滤掉一部分，保证剩余的节点都是可调度的，接着在打分阶段选出最高分节点，该节点就是scheduler的输出节点。 过滤规则 计算权重 过滤规则链接地址 计算权重默认注册权重函数13个 策略名称 权重值Weight 备注 SelectorSpreadPriority 1 default InterPodAffinityPriority 1 default LeastRequestedPriority 1 default BalancedResourceAllocation 1 default NodePreferAvoidPodsPriority 10000 default NodeAffinityPriority 1 default TaintTolerationPriority 1 default ImageLocalityPriority 1 default ServiceSpreadingPriority 1 default EqualPriority 1 default MostRequestedPriority 1 default RequestedToCapacityRatioPriority 1 default ResourceLimitsPriority 1 开启ResourceLimitsPriorityFunction ClusterAutoscalerProvider 调度器把LeastRequestedPriority 替换成MostRequestedPriority 策略 打分源码12345678910// PriorityConfig is a config used for a priority function.type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int} Name 打分函数名称 Map 加分方法 Reduce 减分方法 Function 通用方法，将废弃 Weight 打分函数的权重值 打分调度顺序 Function > Map > Reduce优先计算所有的通用性方法Fuction， 再计算打分函数的加分方法，最后计算所有打分函数的加分方法 SelectorSpreadPriority 加分方法：CalculateSpreadPriorityMap 减分方法: CalculateSpreadPriorityReduce 加分方法：CalculateSpreadPriorityMap 获取正在调度pod 有关的svc，sts，rs，rc 获取该node 上的所有pod 存在的pod 与正在调度的pod 是同一svc或者sts，rs，rc 则改node Score分数 +1 减分方法: CalculateSpreadPriorityReduce 获取加分方法计算出所有node 的分数 获取所有node 中的 最高分数 如果其中有node 设置region和zone， 则获取每一个”regino::zone” 其中最高的分数 获取分数常数值 MaxPriority = 10, zoneWeighting float64 = 2.0 / 3.0 fScore = MaxPriorityFloat64 (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) 注释：–> 常熟10 (最大值 - 当前node的值) / 最大值 score 值，通过上面计算方法则表示 在加分方法中分数越高的node 最后分数越低 如果其中有region和zone设置, 则计算公式 zoneScore = MaxPriorityFloat64 (float64(maxCountByZone-countsByZone[zoneID]) / maxCountByZoneFloat64)， fScore = (fScore (1.0 - zoneWeighting)) + (zoneWeighting zoneScore) 注释–> (常熟10 (zone最大值 - 当前zone的值) / zone最大值) 2/3 + node的fscore 1/3 score值，通过上面计算法则表示 在加分方法中分数越高的node 最后分数越低 和 不同zone的node 分数越高 总结： 相同service／rc的pods越分散，得分越高， 不通zone的node 分数越高 InterPodAffinityPriority 通用方法 CalculateInterPodAffinityPriority 通用方法 CalculateInterPodAffinityPriority 获取调度pod 上的PodAffinity 和PodAntiAffinity 需调度pod上有 PodAffinity 和PodAntiAffinity设置 获取该node上所有pod信息 总结： 通过迭代 weightedPodAffinityTerm 的元素计算和，并且如果对该节点满足相应的PodAffinityTerm，则将 “weight” 加到和中，具有最高和的节点是最优选的。 LeastRequestedPriority 加分方法 leastResourcePriority.PriorityMap 加分方法 leastResourcePriority.PriorityMapBalancedResourceAllocation 加分方法 balancedResourcePriority.PriorityMap 加分方法 balancedResourcePriority.PriorityMapNodePreferAvoidPodsPriority 加分方法 priorities.CalculateNodePreferAvoidPodsPriorityMap 加分方法 priorities.CalculateNodePreferAvoidPodsPriorityMapNodeAffinityPriority 加分方法 priorities.CalculateNodeAffinityPriorityMap 减分方法 priorities.CalculateNodeAffinityPriorityReduce TaintTolerationPriority 加分方法 priorities.ComputeTaintTolerationPriorityMap 减分方法 priorities.ComputeTaintTolerationPriorityReduce ImageLocalityPriority 加分方法 riorities.ImageLocalityPriorityMap ServiceSpreadingPriority 通 SelectorSpreadPriority 相同，以被代替 EqualPriority 加分方法 core.EqualPriorityMap MostRequestedPriority 加分方法 priorities.MostRequestedPriorityMap RequestedToCapacityRatioPriority 加分方法 priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap ResourceLimitsPriority 加分方法 priorities.ResourceLimitsPriorityMap]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNI网络架构解析]]></title>
    <url>%2F2019%2F03%2F01%2FCNI%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[CNI 架构源码解析kubelet cni 源码解析123# kubelet 使用cni 参数设置--cni-bin-dir=/opt/cni/bin--cni-conf-dir=/etc/cni/net.d /opt/cni/bin 目录中插件 bridge cnitool dhcp flannel host-local ipvlan loopback macvlan noop portmap ptp tuning vlan cni 配置文件解析 10-flannel.conflist12345678910111213141516171819{ "cniVersion": "0.3.1", "name": "cni0", "plugins": [ { "delegate": { "forceAddress": true, "isDefaultGateway": true }, "type": "flannel" }, { "capabilities": { "portMappings": true }, "type": "portmap" } ]} 123456789101112131415161718192021222324252627282930313233343536373839// kubernetes/pkg/kubelet/dockershim/network/cni/cni.gofunc (plugin *cniNetworkPlugin) addToNetwork(network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations map[string]string) (cnitypes.Result, error) { rt, err := plugin.buildCNIRuntimeConf(podName, podNamespace, podSandboxID, podNetnsPath, annotations) if err != nil { glog.Errorf("Error adding network when building cni runtime conf: %v", err) return nil, err } netConf, cniNet := network.NetworkConfig, network.CNIConfig glog.V(4).Infof("About to add CNI network %v (type=%v)", netConf.Name, netConf.Plugins[0].Network.Type) // 调用cni 架构接口AddNetworkList res, err := cniNet.AddNetworkList(netConf, rt) if err != nil { glog.Errorf("Error adding network: %v", err) return nil, err } return res, nil}func (plugin *cniNetworkPlugin) deleteFromNetwork(network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations map[string]string) error { rt, err := plugin.buildCNIRuntimeConf(podName, podNamespace, podSandboxID, podNetnsPath, annotations) if err != nil { glog.Errorf("Error deleting network when building cni runtime conf: %v", err) return err } netConf, cniNet := network.NetworkConfig, network.CNIConfig glog.V(4).Infof("About to del CNI network %v (type=%v)", netConf.Name, netConf.Plugins[0].Network.Type) // 调用cni 架构接口 DelNetworkList err = cniNet.DelNetworkList(netConf, rt) // The pod may not get deleted successfully at the first time. // Ignore "no such file or directory" error in case the network has already been deleted in previous attempts. if err != nil && !strings.Contains(err.Error(), "no such file or directory") { glog.Errorf("Error deleting network: %v", err) return err } return nil} containernetworking/cni 源码解析12345678// github.com/containernetworking/cni/libcni/api.gotype CNI interface { AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork(net *NetworkConfig, rt *RuntimeConf) error} 12345678910111213141516171819{ "cniVersion": "0.3.1", "name": "cni0", "plugins": [ { "delegate": { "forceAddress": true, "isDefaultGateway": true }, "type": "flannel" }, { "capabilities": { "portMappings": true }, "type": "portmap" } ]} 12345678910111213141516171819202122// AddNetworkList executes a sequence of plugins with the ADD commandfunc (c *CNIConfig) AddNetworkList(list *NetworkConfigList, rt *RuntimeConf) (types.Result, error) { var prevResult types.Result for _, net := range list.Plugins { pluginPath, err := invoke.FindInPath(net.Network.Type, c.Path) if err != nil { return nil, err } newConf, err := buildOneConfig(list, net, prevResult, rt) if err != nil { return nil, err } prevResult, err = invoke.ExecPluginWithResult(pluginPath, newConf.Bytes, c.args("ADD", rt)) if err != nil { return nil, err } } return prevResult, nil} AddNetworkList 从上至下调用10-flannel.conflist 中plugins 中type 字段的命令 AddNetworkList 调用 /opt/cni/bin/flannel 命令 AddNetworkList 调用 /opt/cni/bin/portmap 命令 12345678910111213141516171819202122// DelNetworkList executes a sequence of plugins with the DEL commandfunc (c *CNIConfig) DelNetworkList(list *NetworkConfigList, rt *RuntimeConf) error { for i := len(list.Plugins) - 1; i >= 0; i-- { net := list.Plugins[i] pluginPath, err := invoke.FindInPath(net.Network.Type, c.Path) if err != nil { return err } newConf, err := buildOneConfig(list, net, nil, rt) if err != nil { return err } if err := invoke.ExecPluginWithoutResult(pluginPath, newConf.Bytes, c.args("DEL", rt)); err != nil { return err } } return nil} DelNetworkList 从下至上调用10-flannel.conflist 中plugins 中type 字段的命令 DelNetworkList 调用 /opt/cni/bin/portmap 命令 DelNetworkList 调用 /opt/cni/bin/flannel 命令 /opt/cni/bin/flannel 命令 源码解析1234// github.com/containernetworking/cni/plugins/plugins/meta/flannel/flannel.gofunc main() { skel.PluginMain(cmdAdd, cmdDel, version.All)} flannel 命令接受ADD, DEL， VERSION 命令行参数 cni 插件接口只需实现ADD， DEL, VERSION 三方法即可 /opt/cni/bin/flanel 命令所支持配置文件字段如下： 123456# /run/flannel/subnet.env 文件为flanneld 服务启动时候生成cat /run/flannel/subnet.envFLANNEL_NETWORK=172.28.0.0/14FLANNEL_SUBNET=172.28.2.1/24FLANNEL_MTU=1500FLANNEL_IPMASQ=true 123456789101112131415161718192021222324252627282930{ "delegate": { "cniVersion": "3.0.1", "type": "bridge", "ipMasq": true, "mtu": 1472, "forceAddress": true, "isDefaultGateway": true, "ipam": { "type": "host-local", "subnet": "172.28.2.1/24", "routes": [{"Dst": "172.28.0.0/14", "Gw": ""}] } }, "cniVersion": "3.0.1", "name": "mycni0", "type": "flannel", "capabilities": {}, "ipam": { "type": "host-local", }, "subnetFile": "/run/flannel/subnet.env", "dataDir": "/var/lib/cni/flannel", "dns": { "nameservers": [], "domain": "", "search": [], "options": [] } } add 命令源码解析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105// NetConf describes a network.type NetConf struct { CNIVersion string `json:"cniVersion,omitempty"` Name string `json:"name,omitempty"` Type string `json:"type,omitempty"` Capabilities map[string]bool `json:"capabilities,omitempty"` IPAM IPAM `json:"ipam,omitempty"` DNS DNS `json:"dns"` RawPrevResult map[string]interface{} `json:"prevResult,omitempty"` PrevResult Result `json:"-"`}type NetConf struct { types.NetConf SubnetFile string `json:"subnetFile"` DataDir string `json:"dataDir"` Delegate map[string]interface{} `json:"delegate"`}func cmdAdd(args *skel.CmdArgs) error { n, err := loadFlannelNetConf(args.StdinData) if err != nil { return err } fenv, err := loadFlannelSubnetEnv(n.SubnetFile) if err != nil { return err } if n.Delegate == nil { n.Delegate = make(map[string]interface{}) } else { if hasKey(n.Delegate, "type") && !isString(n.Delegate["type"]) { return fmt.Errorf("'delegate' dictionary, if present, must have (string) 'type' field") } if hasKey(n.Delegate, "name") { return fmt.Errorf("'delegate' dictionary must not have 'name' field, it'll be set by flannel") } if hasKey(n.Delegate, "ipam") { return fmt.Errorf("'delegate' dictionary must not have 'ipam' field, it'll be set by flannel") } } n.Delegate["name"] = n.Name if !hasKey(n.Delegate, "type") { n.Delegate["type"] = "bridge" } if !hasKey(n.Delegate, "ipMasq") { // if flannel is not doing ipmasq, we should ipmasq := !*fenv.ipmasq n.Delegate["ipMasq"] = ipmasq } if !hasKey(n.Delegate, "mtu") { mtu := fenv.mtu n.Delegate["mtu"] = mtu } if n.Delegate["type"].(string) == "bridge" { if !hasKey(n.Delegate, "isGateway") { n.Delegate["isGateway"] = true } } if n.CNIVersion != "" { n.Delegate["cniVersion"] = n.CNIVersion } n.Delegate["ipam"] = map[string]interface{}{ "type": "host-local", "subnet": fenv.sn.String(), "routes": []types.Route{ types.Route{ Dst: *fenv.nw, }, }, } return delegateAdd(args.ContainerID, n.DataDir, n.Delegate)}func delegateAdd(cid, dataDir string, netconf map[string]interface{}) error { netconfBytes, err := json.Marshal(netconf) if err != nil { return fmt.Errorf("error serializing delegate netconf: %v", err) } // save the rendered netconf for cmdDel if err = saveScratchNetConf(cid, dataDir, netconfBytes); err != nil { return err } result, err := invoke.DelegateAdd(netconf["type"].(string), netconfBytes) if err != nil { return err } return result.Print()} 代码解释 保存配置文件中的 delegate 字段信息到 /var/lib/cni/flannel/ 1234567891011121314151617181920cat 577ed940b4972515b0270636b82c8b0d8151043d93b0c14cf98edfd01ef0da8b | python -m json.tool{ "cniVersion": "0.3.1", "forceAddress": true, "ipMasq": false, "ipam": { "routes": [ { "dst": "172.28.0.0/14" } ], "subnet": "172.28.2.0/24", "type": "host-local" }, "isDefaultGateway": true, "isGateway": true, "mtu": 1500, "name": "cni0", "type": "bridge"} 调用 /opt/cni/bin/bridge 命令 1234567891011type NetConf struct { types.NetConf BrName string `json:"bridge"` //网桥名 IsGW bool `json:"isGateway"` //是否将网桥配置为网关 IsDefaultGW bool `json:"isDefaultGateway"` // IsGw ForceAddress bool `json:"forceAddress"` //如果网桥已存在且已配置了其他IP，通过此参数决定是否将其他ip除去 IPMasq bool `json:"ipMasq"` //如果true，配置私有网段到外部网段的masquerade规则 MTU int `json:"mtu"` // 设置网桥，veth 对 mtu HairpinMode bool `json:"hairpinMode"` // 如果true， 设置veth对网卡为发卡模式 PromiscMode bool `json:"promiscMode"` // 如果true, 设置网桥为混杂模式} 注意：hairpin mode 介绍http://chenchun.github.io/network/2017/10/09/hairpin 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160func cmdAdd(args *skel.CmdArgs) error { var success bool = false n, cniVersion, err := loadNetConf(args.StdinData) if err != nil { return err } isLayer3 := n.IPAM.Type != "" if n.IsDefaultGW { n.IsGW = true } if n.HairpinMode && n.PromiscMode { return fmt.Errorf("cannot set hairpin mode and promiscous mode at the same time.") } br, brInterface, err := setupBridge(n) if err != nil { return err } netns, err := ns.GetNS(args.Netns) if err != nil { return fmt.Errorf("failed to open netns %q: %v", args.Netns, err) } defer netns.Close() hostInterface, containerInterface, err := setupVeth(netns, br, args.IfName, n.MTU, n.HairpinMode) if err != nil { return err } // Assume L2 interface only result := &current.Result{CNIVersion: cniVersion, Interfaces: []*current.Interface{brInterface, hostInterface, containerInterface}} if isLayer3 { // run the IPAM plugin and get back the config to apply r, err := ipam.ExecAdd(n.IPAM.Type, args.StdinData) if err != nil { return err } // release IP in case of failure defer func() { if !success { os.Setenv("CNI_COMMAND", "DEL") ipam.ExecDel(n.IPAM.Type, args.StdinData) os.Setenv("CNI_COMMAND", "ADD") } }() // Convert whatever the IPAM result was into the current Result type ipamResult, err := current.NewResultFromResult(r) if err != nil { return err } result.IPs = ipamResult.IPs result.Routes = ipamResult.Routes if len(result.IPs) == 0 { return errors.New("IPAM plugin returned missing IP config") } // Gather gateway information for each IP family gwsV4, gwsV6, err := calcGateways(result, n) if err != nil { return err } // Configure the container hardware address and IP address(es) if err := netns.Do(func(_ ns.NetNS) error { contVeth, err := net.InterfaceByName(args.IfName) if err != nil { return err } // Disable IPv6 DAD just in case hairpin mode is enabled on the // bridge. Hairpin mode causes echos of neighbor solicitation // packets, which causes DAD failures. for _, ipc := range result.IPs { if ipc.Version == "6" && (n.HairpinMode || n.PromiscMode) { if err := disableIPV6DAD(args.IfName); err != nil { return err } break } } // Add the IP to the interface if err := ipam.ConfigureIface(args.IfName, result); err != nil { return err } // Send a gratuitous arp for _, ipc := range result.IPs { if ipc.Version == "4" { _ = arping.GratuitousArpOverIface(ipc.Address.IP, *contVeth) } } return nil }); err != nil { return err } if n.IsGW { var firstV4Addr net.IP // Set the IP address(es) on the bridge and enable forwarding for _, gws := range []*gwInfo{gwsV4, gwsV6} { for _, gw := range gws.gws { if gw.IP.To4() != nil && firstV4Addr == nil { firstV4Addr = gw.IP } err = ensureBridgeAddr(br, gws.family, &gw, n.ForceAddress) if err != nil { return fmt.Errorf("failed to set bridge addr: %v", err) } } if gws.gws != nil { if err = enableIPForward(gws.family); err != nil { return fmt.Errorf("failed to enable forwarding: %v", err) } } } } if n.IPMasq { chain := utils.FormatChainName(n.Name, args.ContainerID) comment := utils.FormatComment(n.Name, args.ContainerID) for _, ipc := range result.IPs { if err = ip.SetupIPMasq(ip.Network(&ipc.Address), chain, comment); err != nil { return err } } } } // Refetch the bridge since its MAC address may change when the first // veth is added or after its IP address is set br, err = bridgeByName(n.BrName) if err != nil { return err } brInterface.Mac = br.Attrs().HardwareAddr.String() result.DNS = n.DNS // Return an error requested by testcases, if any if debugPostIPAMError != nil { return debugPostIPAMError } success = true return types.PrintResult(result, cniVersion)} ADD命令： 执行ADD命令时，brdige组件创建一个指定名字的网桥，如果网桥已经存在，就使用已有的网桥； 创建vethpair，将node端的veth设备连接到网桥上； 从ipam获取一个给容器使用的ip数据，并根据返回的数据计算出容器对应的网关； 进入容器网络名字空间，修改容器中网卡名和网卡ip，以及配置路由，并进行arp广播（注>- 意我们只为vethpair的容器端配置ip，node端是没有ip的）； 如果IsGW=true，将网桥配置为网关，具体方法是：将第三步计算得到的网关IP配置到网桥上同时根据需要将网桥上其他ip删除。最后开启网桥的ip_forward内核参数； 如果IPMasq=true，使用iptables增加容器私有网网段到外部网段的masquerade规则，这样容器内部访问外部网络时会进行snat，在很多情况下配置了这条路由后容器内部才能访问外网。（这里代码中会做exist检查，防止生成重复的iptables规则）； 配置结束，整理当前网桥的信息，并返回给调用者。 DEL命令： 根据命令执行的参数，确认要删除的容器ip，调用ipam的del命令，将IP还回IP pool; 进入容器的网络名字空间，根据容器IP将对应的网卡删除； 如果IPMasq=true，在node上删除创建网络时配置的几条iptables规则。 调用 /opt/cni/bin/host-local 命令 1234567891011121314151617type Range struct { RangeStart net.IP `json:"rangeStart,omitempty"` // The first ip, inclusive RangeEnd net.IP `json:"rangeEnd,omitempty"` // The last ip, inclusive Subnet types.IPNet `json:"subnet"` Gateway net.IP `json:"gateway,omitempty"`}type IPAMConfig struct { *Range Name string Type string `json:"type"` Routes []*types.Route `json:"routes"`//交付的ip对应的路由 DataDir string `json:"dataDir"`//本地ip池的数据库目录 ResolvConf string `json:"resolvConf"`//交付的ip对应的dns Ranges []RangeSet `json:"ranges"`//交付的ip所属的网段，网关信息 IPArgs []net.IP `json:"-"` // Requested IPs from CNI_ARGS and args} 12345678910111213141516171819202122232425262728293031#配置文件范例：{ "cniVersion": "0.3.1", "name": "cni0", "type": "bridge", "master": "foo0", "ipam": { "type": "host-local", "resolvConf": "/home/here.resolv", "dataDir": "/home/cni/network", "ranges": [ [ { "subnet": "10.1.2.0/24", "rangeStart": "10.1.2.9", "rangeEnd": "10.1.2.20", "gateway": "10.1.2.30" }, { "subnet": "10.1.4.0/24" } ], [{ "subnet": "11.1.2.0/24", "rangeStart": "11.1.2.9", "rangeEnd": "11.1.2.20", "gateway": "11.1.2.30" }] ] }} host-local组件通过在配置文件中指定的subnet进行网络划分。 host-local在本地通过指定目录（默认为/var/lib/cni/networks）记录当前的ip pool数据。 host-local将IP分配并告知调用者时，还可以告知dns、路由等配置信息。这些信息通过配置文件和对应的resolv文件记录。 调用 /opt/cni/bin/portmap 命令 iptalbes 流量顺序执行图链接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// NetConf describes a network.type NetConf struct { CNIVersion string `json:"cniVersion,omitempty"` Name string `json:"name,omitempty"` Type string `json:"type,omitempty"` Capabilities map[string]bool `json:"capabilities,omitempty"` IPAM IPAM `json:"ipam,omitempty"` DNS DNS `json:"dns"` RawPrevResult map[string]interface{} `json:"prevResult,omitempty"` PrevResult Result `json:"-"`}// PortMapEntry corresponds to a single entry in the port_mappings argument,// see CONVENTIONS.mdtype PortMapEntry struct { HostPort int `json:"hostPort"` ContainerPort int `json:"containerPort"` Protocol string `json:"protocol"` HostIP string `json:"hostIP,omitempty"`}type PortMapConf struct { types.NetConf SNAT *bool `json:"snat,omitempty"` //开启SNAT ConditionsV4 *[]string `json:"conditionsV4"` ConditionsV6 *[]string `json:"conditionsV6"` MarkMasqBit *int `json:"markMasqBit"` ExternalSetMarkChain *string `json:"externalSetMarkChain"` RuntimeConfig struct { PortMaps []PortMapEntry `json:"portMappings,omitempty"` } `json:"runtimeConfig,omitempty"` RawPrevResult map[string]interface{} `json:"prevResult,omitempty"` PrevResult *current.Result `json:"-"` // These are fields parsed out of the config or the environment; // included here for convenience ContainerID string `json:"-"` ContIPv4 net.IP `json:"-"` ContIPv6 net.IP `json:"-"`}// The default mark bit to signal that masquerading is required// Kubernetes uses 14 and 15, Calico uses 20-31.const DefaultMarkBit = 13 12345portmap 配置参数设置{ "capabilities": {"portMappings": true}, "type": "portmap"} 用于在node上配置iptables规则，进行SNAT,DNAT和端口转发。 portmap组件通常在main组件执行完毕后执行，因为它的执行参数仰赖之前的组件提供。 capabilities 中的 portMappings 参数设置true 则会开启ports 中信息增加DNAT, SNAT 根据pod 中ports 段 配置iptables 规则链 123456789101112131415161718spec: containers: - args: - --api - --kubernetes - --logLevel=INFO image: hub-cn-shanghai-2.kce.ksyun.com/ksyun/traefik:latest imagePullPolicy: Always name: traefik-ingress-lb ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP - containerPort: 8080 hostPort: 8080 name: admin protocol: TCP 添加容器网络操作 1234567891011121314151617181920212223242526272829303132# 生成 CNI-HOSTPORT-DNAT 链路存在于NAT表中Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination56243 3305K CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL // 匹配访问访问本地地址的流量如 127.0.0.1 eth0的ip地址Chain CNI-HOSTPORT-DNAT (2 references) # 根据pod 对应yaml 文件中的 ports段内容 生成对应每一个pod 的规则 pkts bytes target prot opt in out source destination 302K 18M CNI-DN-d1d39c44eab8f1c7243ad all -- * * 0.0.0.0/0 0.0.0.0/0 /* dnat name: "cni0" id: "937fa5c4970443a92e469485ac1c8d557fa41e703ca05326ed725a825bbbbae0" */ ### 新版portmap 命令 # 302K 18M CNI-DN-d1d39c44eab8f1c7243ad all -- * * 0.0.0.0/0 0.0.0.0/0 /* dnat name: "cni0" id: "937fa5c4970443a92e469485ac1c8d557fa41e703ca05326ed725a825bbbbae0" */ multiport dports 80,8080Chain CNI-DN-d1d39c44eab8f1c7243ad (1 references) pkts bytes target prot opt in out source destination 2 120 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 to:172.28.2.5:80 1 60 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.28.2.5:8080# 生成 CNI-HOSTPORT-SNAT 链路存在于NAT表中Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 CNI-HOSTPORT-SNAT all -- * * 127.0.0.1 !127.0.0.1Chain CNI-HOSTPORT-SNAT (1 references) pkts bytes target prot opt in out source destination 0 0 CNI-SN-0c252d022e0443e29bc25 all -- * * 0.0.0.0/0 0.0.0.0/0 /* snat name: "cni0" id: "c82fadf4fb3eb9f4ed9ba25778e928292dd9550cf6cd9edc99f7a6f14ccacde1" */Chain CNI-SN-0c252d022e0443e29bc25 (1 references) pkts bytes target prot opt in out source destination 0 0 MASQUERADE tcp -- * * 127.0.0.1 172.28.2.6 tcp dpt:80 0 0 MASQUERADE tcp -- * * 127.0.0.1 172.28.2.6 tcp dpt:8080 添加容器网络操作 12删除 CNI-SN-0c252d022e0443e29bc25, CNI-DN-d1d39c44eab8f1c7243ad 链表清空 CNI-HOSTPORT-SNAT, CNI-HOSTPORT-DNAT 链中特定容器]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>network</tag>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s亲和性]]></title>
    <url>%2F2019%2F03%2F01%2Fk8s%E4%BA%B2%E5%92%8C%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Kubernetes中的亲和性现实中应用的运行对于kubernetes在亲和性上提出了一些要求，可以归类到以下几个方面： 1.Pod固定调度到某些节点之上2.Pod不会调度到某些节点之上3.Pod的多副本调度到相同的节点之上4.Pod的多副本调度到不同的节点之上 k8s 亲和性包含123456789101112// Affinity is a group of affinity scheduling rules.type Affinity struct { // Describes node affinity scheduling rules for the pod. // +optional NodeAffinity *NodeAffinity `json:"nodeAffinity,omitempty" protobuf:"bytes,1,opt,name=nodeAffinity"` // Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). // +optional PodAffinity *PodAffinity `json:"podAffinity,omitempty" protobuf:"bytes,2,opt,name=podAffinity"` // Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). // +optional PodAntiAffinity *PodAntiAffinity `json:"podAntiAffinity,omitempty" protobuf:"bytes,3,opt,name=podAntiAffinity"`} NodeAffinity 相对于nodeSelector机制更加的灵活和丰富, 丰富nodeSelector 机制 PodAffinity 设置pod 的亲和性 PodAntiAffinity 设置pod 的反亲和性 NodeAffinity 实例123456789101112131415161718192021222324252627282930313233apiVersion: v1kind: Podmetadata: name: with-node-affinityspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 10 preference: matchExpressions: - key: key1 operator: In values: - value1 - weight: 2 preference: matchExpressions: - key: key2 operator: In values: - value2 containers: - name: with-node-affinity image: hub.kce.ksyun.com/ksyun/pause-amd64:3.0 表达的语法：支持In,NotIn,Exists,DoesNotExist,Gt,Lt． 支持soft(preference)和hard(requirement),hard表示pod sheduler到某个node上，则必须满足亲和性设置．soft表示scheduler的时候，无法满足节点的时候，会选择非nodeSelector匹配的节点． requiredDuringSchedulingIgnoredDuringExecution 必须满足条件 preferredDuringSchedulingIgnoredDuringExecution 非必须满足条件 如上: node label 中必须有kubernetes.io/e2e-az-name=e2e-az1 或者 kubernetes.io/e2e-az-name=e2e-az2 如上: node label 中有key1 的优先权大于 key2 的节点 补充： nodeSelectorTerms 选项中可以添加 多个 matchExpressions，多个matchFields preferredDuringSchedulingIgnoredDuringExecution 可以写多个 weight项注意：测试v1.10.5， v1.12.3版本nodeSelectorTerms 只需要满足nodeSelectorTerms其中一个选项就可以调度 PodAffinity 实例 这个特性是Kubernetes 1.4后增加的，允许用户通过已经运行的Pod上的标签来决定调度策略，用文字描述就是“如果Node X上运行了一个或多个满足Y条件的Pod，那么这个Pod在Node应该运行在Pod X”，因为Node没有命名空间，Pod有命名空间，这样就允许管理员在配置的时候指定这个亲和性策略适用于哪个命名空间，可以通过topologyKey来指定。topology是一个范围的概念，可以是一个Node、一个机柜、一个机房或者是一个区域（如北美、亚洲）等，实际上对应的还是Node上的标签。有两种类型 requiredDuringSchedulingIgnoredDuringExecution，刚性要求，必须精确匹配 preferredDuringSchedulingIgnoredDuringExecution，软性要求 类似上面node的亲和策略类似，requiredDuringSchedulingIgnoredDuringExecution亲和性可以用于约束不同服务的pod在同一个topology domain的Nod上preferredDuringSchedulingIgnoredDuringExecution反亲和性可以将服务的pod分散到不同的topology domain的Node上．标签支持 标签的判断操作支持In、NotIn、Exists、DoesNotExist。原则上topologyKey可以是节点的合法标签，但是有一些约束： kubernetes.io/hostname ＃Node failure-domain.beta.kubernetes.io/zone ＃Zone failure-domain.beta.kubernetes.io/region #Region 可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: with-pod-affinityspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname containers: - name: with-pod-affinity image: hub.kce.ksyun.com/ksyun/pause-amd64:3.0 例子中指定了pod的亲和性和反亲和性， preferredDuringSchedulingIgnoredDuringExecution指定的规则是pod将会调度到的node尽量会满足如下条件： node上具有failure-domain.beta.kubernetes.io/zone 并且具有相同failure-domain.beta.kubernetes.io/zone的值的node上运行有一个pod,它符合label为securtity=S1. preferredDuringSchedulingIgnoredDuringExecution规则表示将尽量不会调度到node上运行有security=S2的pod． 如果这里我们将topologyKey＝failure-domain.beta.kubernetes.io/zone，那么pod将尽量不会调度到node满足的条件是：node上具有failure-domain.beta.kubernetes.io/zone相同的ｖalue,并且这些相同zone下的node上运行有security=S2的pod.** 总结 Pod间的亲和性策略要求可观的计算量可能显著降低集群的性能，不建议在超过100台节点的范围内使用。 Pod间的反亲和策略要求所有的Node都有一致的标签，例如集群中所有节点都应有匹配topologyKey的标签，如果一些节点缺失这些标签可能导致异常行为。 常用的场景123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: redis-cachespec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: "kubernetes.io/hostname" containers: - name: redis-server image: redis:3.2-alpine 注解：上面的例子中，创建了一个具有三个实例的部署，采用了Pod间的反亲和策略，限制创建的实例的时候，如果节点上已经存在具有相同标签的实例，则不进行部署，避免了一个节点上部署多个相同的实例。 123456789101112131415161718192021222324252627282930313233343536apiVersion: apps/v1kind: Deploymentmetadata: name: web-serverspec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: "kubernetes.io/hostname" podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: "kubernetes.io/hostname" containers: - name: web-app image: nginx:1.12-alpine 注解：再创建3个Web服务的实例，同上面Redis的配置，首先确保两个Web不会部署到相同的节点，然后在应用Pod间亲和策略，优先在有Redis服务的节点上部署Web。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-scheduler启动流程]]></title>
    <url>%2F2019%2F03%2F01%2Fkube-scheduler-start%2F</url>
    <content type="text"><![CDATA[kube-sheduler 启动流程作用简介kube-scheduler是k8s中的调度模块，负责调度Pod，pod是k8s最基本的运行单元。再我们剖析代码之前，可以猜测一下k8s是如何工作的。kube-scheduler需要对未被调度的Pod进行Watch，同时也需要对node进行watch，因为pod需要绑定到具体的Node上，当kube-scheduler监测到未被调度的pod，它会取出这个pod，然后依照内部设定的调度算法，选择合适的node，然后通过apiserver写回到etcd，kube-scheduler的工作也就结束了。实际kube-scheduler的代码就是这样工作的。 kube-sheduler feature-gates 介绍 APIListChunking=true|false (BETA - default=true) APIResponseCompression=true|false (ALPHA - default=false) kube-sheduler 原理介绍1, Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 2, Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 3, 最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 kube-shedule 服务启动分析 使用spf13/cobra 包进行命令行封装 配置leader election 选项则进行 leader 选举功能 LeaderElection 设置true 运行leader 选举机制acquire方法(maybeReportTransition) 在主线程运行sched.Run() 并且设置]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s调度规则]]></title>
    <url>%2F2019%2F02%2F28%2Falgorithm%2F</url>
    <content type="text"><![CDATA[默认调度规则12345678910111213141516171819202122// 默认过滤规则14个NoVolumeZoneConflictPredMaxEBSVolumeCountPredMaxGCEPDVolumeCountPredMaxAzureDiskVolumeCountPredMaxCSIVolumeCountPredMatchInterPodAffinityPredNoDiskConflictPredGeneralPredCheckNodeMemoryPressurePredCheckNodeDiskPressurePredCheckNodePIDPressurePredCheckNodeConditionPredPodToleratesNodeTaintsPredCheckVolumeBindingPred// 过滤规则factory.RegisterFitPredicate("PodFitsPorts", predicates.PodFitsHostPorts)//这个只是为了兼容旧版本，所以仍旧保留，较新的版本已经用PodFitsHostPorts这个名字取代了PodFitsPorts，也就是下面的一个预选算法factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts)factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources)factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost)factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) DefaultProvider 调度检查资源总结 注意：资源调度集合有两种【ClusterAutoscalerProvider | DefaultProvider】默认DefaultProviderThe scheduling algorithm provider to use, one of: ClusterAutoscalerProvider | DefaultProvider (default “DefaultProvider”) CheckNodeConditionPred （开启TaintNodesByCondition（node节点自动污点功能）功能将失效）12345678910111213141516171819202122232425262728# node节点环境情况查看# kubectl get node 10.0.99.14 -o yaml conditions: - lastHeartbeatTime: 2018-09-12T07:13:41Z lastTransitionTime: 2018-08-17T03:44:43Z message: kubelet has sufficient disk space available reason: KubeletHasSufficientDisk status: "False" type: OutOfDisk - lastHeartbeatTime: 2018-09-12T07:13:41Z lastTransitionTime: 2018-08-22T06:09:58Z message: kubelet has sufficient memory available reason: KubeletHasSufficientMemory status: "False" type: MemoryPressure - lastHeartbeatTime: 2018-09-12T07:13:41Z lastTransitionTime: 2018-08-22T06:09:58Z message: kubelet has no disk pressure reason: KubeletHasNoDiskPressure status: "False" type: DiskPressure - lastHeartbeatTime: 2018-09-12T07:13:41Z lastTransitionTime: 2018-08-23T10:09:44Z message: kubelet is posting ready status reason: KubeletReady status: "True" type: Ready 节点状态ready磁盘足够网络是否正确配置是否可以调度unschedulable: true( kubectl cordon node 设置不允许调度/uncordon允许调度) CheckNodeUnschedulablePred(需要开启TaintNodesByCondition功能)123456789# kubectl get node 10.0.99.14 -o yamlspec: podCIDR: 172.16.1.0/24 taints: - effect: NoSchedule key: node.kubernetes.io/unschedulable timeAdded: 2019-02-21T06:58:24Z unschedulable: true 检查node 节点taints node.kubernetes.io/unschedulable=NoSchedule 污点与pod 中Tolerations 是否设置容忍改污点 GeneralPred1234567# kubectl get node 10.0.99.14 -o yaml allocatable: cpu: "1" ephemeral-storage: "18902281390" memory: 1781448Ki pods: "110" 123456789101112# cpu,mem,storage 资源结算规则const ( // CPU, in cores. (500m = .5 cores) ResourceCPU ResourceName = "cpu" // Memory, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024) ResourceMemory ResourceName = "memory" // Volume size, in bytes (e,g. 5Gi = 5GiB = 5 * 1024 * 1024 * 1024) ResourceStorage ResourceName = "storage" // Local ephemeral storage, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024) // The resource name for ResourceEphemeralStorage is alpha and it can change across releases. ResourceEphemeralStorage ResourceName = "ephemeral-storage") 节点所允许分配的最大pod数量默认是110pod requests: cpu,mem,stroage 和node 存在的资源相加 小于 node 上能分配的资源扩展资源的对比(具体yaml 的写法见wiki) HostNamePred1234567891011121314151617181920212223// PodFitsHost checks if a pod spec node name matches the current node.func PodFitsHost(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { if len(pod.Spec.NodeName) == 0 { return true, nil, nil } node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf("node not found") } if pod.Spec.NodeName == node.Name { return true, nil, nil } return false, []algorithm.PredicateFailureReason{ErrPodNotMatchHostName}, nil}```yaml dnsPolicy: ClusterFirst hostNetwork: true nodeName: 10.0.99.12 restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 指定node 节点调度 (在yaml文件中填写nodeName 字段) PodFitsHostPortsPred 获取node中所有的pod的hostip(hostport/protocol) 端口与所要创建的pod 中的 hostip(hostport/protocol) 进行对比eg： node 节点上有pod的端口设置成0.0.0.0(80/tcp) 如果现需创建pod 需要0.0.0.0(80/tcp)端口，则排除改node 节点 MatchNodeSelectorPred12345678910111213141516171819202122#podapiVersion: v1kind: Podmetadata: name: with-node-affinityspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-num operator: In values: - az1 - az2 - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 检查pod 中设置的NodeSelector 选项检查pod 中设置的 affinity.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution 亲和性nodeSelectorTerms 和 MatchFields 可以配置多个，但调度时候如果一个匹配则为调度成功（网上说法为全部匹配成功才算调度成功，源码显示为只需成功匹配一个就行） PodFitsResourcesPred 节点所允许分配的最大pod数量默认是110pod requests: cpu,mem,stroage 和node 存在的资源相加 小于 node 上能分配的资源扩展资源的对比(如GPU 资源 nvidia.com/gpu， 有效资源key 中必须包含 “/“或者 不能以kubernetes.io/开头或者requests.开头) NoDiskConflictPred 检查在此主机上是否存在卷冲突。如果这个主机已经挂载了卷，其它同样使用这个卷的Pod不能调度到这个主机上。GCE, Amazon EBS, and Ceph RBD使用的规则如下：GCE允许同时挂载多个卷，只要这些卷都是只读的。Amazon EBS不允许不同的Pod挂载同一个卷。Ceph RBD不允许任何两个pods分享相同的monitor，match pool和 image。ISCSI允许同时挂载多个卷，只要这些卷都是只读的。 PodToleratesNodeTaintsPred (需要开启TaintNodesByCondition功能) 只检查pod 与 node 中的 NoSchedule， NoExecute 类型的容忍度和污点 12tolerations:- operator: Exists 注解：如上是容忍所有污点注解： NoSchedule 禁止调度， NoExecute 禁止调度，并驱逐调度上的pod， PreferNoSchedule 尽量不要调度上 PodToleratesNodeNoExecuteTaintsPred 检查pod 是否容忍node 节点上的NoExecute 类型的污点在DaemonSetsController 控制器中运用到，如果ds 中pod 没有设置容忍NoExecute 类型的污点，pod 将会调度不上去 CheckNodeLabelPresencePred12345// NodeLabelChecker contains information to check node labels for a predicate.type NodeLabelChecker struct { labels []string presence bool} 检查节点上是否存在所有指定的标签，无论它们的值如何如果“presence”为false，则如果任何请求的标签与任何节点的标签匹配，则返回false，否则返回true。如果“presence”为true，则如果任何请求的标签与任何节点的标签不匹配，则返回false，否则返回true。“presence”为false，node 节点中的label 与kube-scheduler自定的label 相匹配则 不让pod调度上“presence”为true，node 节点中的label 与kube-scheduler自定的label 相匹配则 让pod调度上 CheckServiceAffinityPred 需要自行配置调度Policy策略中的PredicateArgument.ServiceAffinity 标签为属于同一service 的pod 调度到同一组node 上 MaxEBSVolumeCountPred 检查节点aws ebs 盘的个数限制 默认限制39个如果设置FeatureGate（AttachVolumeLimit）功能则最大限制为node 节点所设置的值 MaxGCEPDVolumeCountPred 检查节点gce 类型盘的个数限制 默认限制16个如果设置FeatureGate（AttachVolumeLimit）功能则最大限制为node 节点所设置的值 MaxAzureDiskVolumeCountPred 检查节点 Azure 类型盘的个数限制 默认限制16个如果设置FeatureGate（AttachVolumeLimit）功能则最大限制为node 节点所设置的值 CheckVolumeBindingPred (需要开启VolumeScheduling功能)>12345678910111213141516171819202122232425apiVersion: v1kind: PersistentVolumemetadata: name: example-local-pv annotations: "volume.alpha.kubernetes.io/node-affinity": '{ "requiredDuringSchedulingIgnoredDuringExecution": { "nodeSelectorTerms": [ { "matchExpressions": [ { "key": "kubernetes.io/hostname", "operator": "In", "values": ["example-node"] } ]} ]} }'spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disk 检查pod 中所有pvc类型卷是否挂载(根据pvc 中的 pv.kubernetes.io/bind-completed 信息判断) 如果没有挂载并且开启VolumeScheduling功能和设置StorageClassName 类型并且绑定模式(volumeBindingMode) 为 WaitForFirstConsumer（如果volume.kubernetes.io/selected-node为true 则不进行预挂载）， 则pod 中的pvc 可以在pod 创建后进行绑定pv或者 StorageClass。 如果pod 中指定 pvc不满足2的要求并且没有绑定 pv 或者StorageClass 则调度失败。 对已经绑定的pvc检查其绑定的pv 指定的亲和性，如上yaml案例 如node 不满足pv的亲和性则pod 不会调度到该node上。 对没进行绑定的pvc 则根据其指定的StorageClass 匹配合适的pv 进行绑定 NoVolumeZoneConflictPred 检查node 的 failure-domain.beta.kubernetes.io/zone 和 failure-domain.beta.kubernetes.io/region 标签 如果pod 的pv 中设置failure-domain.beta.kubernetes.io/zone, failure-domain.beta.kubernetes.io/region 标签，则只能调度到相同标签的node上 如果pod 的pv 中设置failure-domain.beta.kubernetes.io/zone, failure-domain.beta.kubernetes.io/region 标签，则调度该node 成功 改功能可用于 ebs 等区分region 和zone 的存储上 CheckNodeMemoryPressurePred （开启TaintNodesByCondition（node节点自动污点功能）功能将失效）12345678# kubectl get node 1.1.1.1 -o yaml conditions: - lastHeartbeatTime: 2019-02-28T10:17:13Z lastTransitionTime: 2018-12-13T08:46:38Z message: kubelet has sufficient memory available reason: KubeletHasSufficientMemory status: "False" type: MemoryPressure 检查pod的qos 是否为BestEffort qos 是 BestEffort 则检查node 节点内存是否压力过大，反之则跳过检查 node 节点内存压力情况可根据node 详情查看 CheckNodePIDPressurePred （开启TaintNodesByCondition（node节点自动污点功能）功能将失效）12345678# kubectl get node 1.1.1.1 -o yaml conditions: - lastHeartbeatTime: 2019-02-28T10:17:13Z lastTransitionTime: 2018-12-13T08:46:38Z message: kubelet has sufficient PID available reason: KubeletHasSufficientPID status: "False" type: PIDPressure 则检查node 节点pid是否压力过大，反之则跳过检查 node 节点pid压力情况可根据node 详情查看 CheckNodeDiskPressurePred （开启TaintNodesByCondition（node节点自动污点功能）功能将失效）1234567# kubectl get node 1.1.1.1 -o yaml - lastHeartbeatTime: 2019-02-28T10:17:13Z lastTransitionTime: 2018-12-13T08:46:38Z message: kubelet has no disk pressure reason: KubeletHasNoDiskPressure status: "False" type: DiskPressure 则检查node 节点disk是否压力过大，反之则跳过检查 node 节点disk压力情况可根据node 详情查看 MatchInterPodAffinityPred 检查pod 的亲和性设置, 与node 标签进行匹配 NoVolumeZoneConflict 检查给定的zone限制前提下，检查如果在此主机上部署Pod是否存在卷冲突。假定一些volumes可能有zone调度约束， VolumeZonePredicate根据volumes自身需求来评估pod是否满足条件。必要条件就是任何volumes的zone-labels必须与节点上的zone-labels完全匹配。节点上可以有多个zone-labels的约束（比如一个假设的复制卷可能会允许进行区域范围内的访问）。目前，这个只对PersistentVolumeClaims支持，而且只在PersistentVolume的范围内查找标签。处理在Pod的属性中定义的volumes（即不使用PersistentVolume）有可能会变得更加困难，因为要在调度的过程中确定volume的zone，这很有可能会需要调用云提供商。 MaxCSIVolumeCountPred 如果设置FeatureGate（AttachVolumeLimit） 功能则最大限制为node 节点所设置的值开启AttachVolumeLimit 功能并且指定了 某种类型的csi dirver （eg: attachable-volumes-csi-xx）则xx 类型csi 卷在节点上不得超过指定的limits限制]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pod服务质量qos解析]]></title>
    <url>%2F2019%2F01%2F21%2Fpod%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8Fqos%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Pod 配置服务质量等级 Pod 可以配置特定的服务质量（QoS）等级。Kubernetes 使用 QoS 等级来确定何时调度和终结 Pod 。 QoS 等级当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级： Guaranteed Burstable BestEffort 创建一个 Pod 并分配 QoS 等级为 Guaranteed想要给 Pod 分配 QoS 等级为 Guaranteed: Pod 里的每个容器都必须有内存限制和请求，而且必须是一样的。 Pod 里的每个容器都必须有 CPU 限制和请求，而且必须是一样的。 pod 中的mem， cpu 的 limit 和request 值必须一样。 这是一个含有一个容器的 Pod 的配置文件。这个容器配置了内存限制和请求，都是200MB。它还有 CPU 限制和请求，都是700 millicpu: 12345678910111213141516apiVersion: v1kind: Podmetadata: name: qos-demo namespace: qos-examplespec: containers: - name: qos-demo-ctr image: nginx resources: limits: memory: "200Mi" cpu: "700m" requests: memory: "200Mi" cpu: "700m" 查看 Pod 的详细信息: 1kubectl get pod qos-demo --namespace=qos-example --output=yaml 输出显示了 Kubernetes 给 Pod 配置的 QoS 等级为 Guaranteed 。也验证了容器的内存和 CPU 的限制都满足了它的请求。 123456789101112spec: containers: ... resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi... qosClass: Guaranteed 创建一个 Pod 并分配 QoS 等级为 BestEffort要给一个 Pod 配置 BestEffort 的 QoS 等级, Pod 里的容器必须没有任何内存或者 CPU 的限制或请求。 不配置容器 resource 信息 下面是一个 Pod 的配置文件，包含一个容器。这个容器没有内存或者 CPU 的限制或者请求： 123456789apiVersion: v1kind: Podmetadata: name: qos-demo-3 namespace: qos-examplespec: containers: - name: qos-demo-3-ctr image: nginx 查看 Pod 的详细信息: 1kubectl get pod qos-demo-3 --namespace=qos-example --output=yaml 输出显示了 Kubernetes 给 Pod 配置的 QoS 等级是 BestEffort. 123456spec: containers: ... resources: {} ... qosClass: BestEffort 创建一个 Pod 并分配 QoS 等级为 Burstable当出现下面的情况时，则是一个 Pod 被分配了 QoS 等级为 Burstable : 该 Pod 不满足 QoS 等级 Guaranteed 的要求。 Pod 里至少有一个容器有内存或者 CPU 请求。 不满足Guaranteed，BestEffort 的pod 都是Burstable。 这是 Pod 的配置文件，里面有一个容器。这个容器配置了200MB的内存限制和100MB的内存申请。 123456789101112131415qos-pod-2.yaml Copy qos-pod-2.yaml to clipboardapiVersion: v1kind: Podmetadata: name: qos-demo-2 namespace: qos-examplespec: containers: - name: qos-demo-2-ctr image: nginx resources: limits: memory: "200Mi" requests: memory: "100Mi" 查看 Pod 的详细信息: 1kubectl get pod qos-demo-2 --namespace=qos-example --output=yaml 输出显示了 Kubernetes 给这个 Pod 配置了 QoS 等级为 Burstable. 123456789101112spec: containers: - image: nginx imagePullPolicy: Always name: qos-demo-2-ctr resources: limits: memory: 200Mi requests: memory: 100Mi... qosClass: Burstable 总结在k8s中，会根据pod的limit 和 requests的配置将pod划分为不同的qos类别： Guaranteed Burstable BestEffort 当机器可用资源不够时，kubelet会根据qos级别划分迁移驱逐pod。被驱逐的优先级：BestEffort > Burstable > Guaranteed]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leader选举源码分析]]></title>
    <url>%2F2019%2F01%2F08%2Fleader%E9%80%89%E4%B8%BE%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[kube-sheduler leader 选举代码分析 所使用的包clinet-go 模块中的tools/leaderelection 包 1234567891011121314151617181920// If leader election is enabled, run via LeaderElector until done and exit. if c.LeaderElection != nil { // 设置callbacks 方法started/stopped c.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: run, OnStoppedLeading: func() { utilruntime.HandleError(fmt.Errorf("lost master")) }, } leaderElector, err := leaderelection.NewLeaderElector(*c.LeaderElection) if err != nil { return fmt.Errorf("couldn't create leader elector: %v", err) } // 启动选举方法 leaderElector.Run() return fmt.Errorf("lost lease") } 1234567891011121314151617// 使用client-go 模块中的tools/leaderelection包// Run starts the leader election loopfunc (le *LeaderElector) Run() { // 设置函数返回之前执行的callbacks方法 defer func() { runtime.HandleCrash() le.config.Callbacks.OnStoppedLeading() }() // 获取和选举leader信息 如果不是leader 则一直循环选举等待 le.acquire() stop := make(chan struct{}) // 运行scheduler 的逻辑代码 stop 管道设置为阻塞状态 go le.config.Callbacks.OnStartedLeading(stop) // 选举新的leader le.renew() close(stop)} 12345678910111213141516171819202122// acquire loops calling tryAcquireOrRenew and returns immediately when tryAcquireOrRenew succeeds.// 周期性选举leader，直到自己成为leader后才推出循环func (le *LeaderElector) acquire() { stop := make(chan struct{}) // 输出命名空间和名称eg: kube-system/kube-scheduler desc := le.config.Lock.Describe() glog.Infof("attempting to acquire leader lease %v...", desc) // 调用apimachinery 模块中的wait包中JitterUntil循环执行函数 wait.JitterUntil(func() { succeeded := le.tryAcquireOrRenew() le.maybeReportTransition() // if !succeeded { glog.V(4).Infof("failed to acquire lease %v", desc) return } le.config.Lock.RecordEvent("became leader") glog.Infof("successfully acquired lease %v", desc) // close(stop) }, le.config.RetryPeriod, JitterFactor, true, stop)} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// tryAcquireOrRenew tries to acquire a leader lease if it is not already acquired,// else it tries to renew the lease if it has already been acquired. Returns true// on success else returns false.// 获取leader信息是否是本机func (le *LeaderElector) tryAcquireOrRenew() bool { now := metav1.Now() leaderElectionRecord := rl.LeaderElectionRecord{ HolderIdentity: le.config.Lock.Identity(), LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), RenewTime: now, AcquireTime: now, } // 1. obtain or create the ElectionRecord oldLeaderElectionRecord, err := le.config.Lock.Get() if err != nil { if !errors.IsNotFound(err) { glog.Errorf("error retrieving resource lock %v: %v", le.config.Lock.Describe(), err) return false } if err = le.config.Lock.Create(leaderElectionRecord); err != nil { glog.Errorf("error initially creating leader election record: %v", err) return false } le.observedRecord = leaderElectionRecord le.observedTime = time.Now() return true } // 2. Record obtained, check the Identity & Time if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) { le.observedRecord = *oldLeaderElectionRecord le.observedTime = time.Now() } if le.observedTime.Add(le.config.LeaseDuration).After(now.Time) && oldLeaderElectionRecord.HolderIdentity != le.config.Lock.Identity() { glog.V(4).Infof("lock is held by %v and has not yet expired", oldLeaderElectionRecord.HolderIdentity) return false } // 3. We're going to try to update. The leaderElectionRecord is set to it's default // here. Let's correct it before updating. if oldLeaderElectionRecord.HolderIdentity == le.config.Lock.Identity() { leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions } else { leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1 } // update the lock itself if err = le.config.Lock.Update(leaderElectionRecord); err != nil { glog.Errorf("Failed to update lock: %v", err) return false } le.observedRecord = leaderElectionRecord le.observedTime = time.Now() return true} 12345678910111213141516171819// renew loops calling tryAcquireOrRenew and returns immediately when tryAcquireOrRenew fails.// 间歇性获取leader信息是否是本机func (le *LeaderElector) renew() { stop := make(chan struct{}) wait.Until(func() { err := wait.Poll(le.config.RetryPeriod, le.config.RenewDeadline, func() (bool, error) { return le.tryAcquireOrRenew(), nil }) le.maybeReportTransition() desc := le.config.Lock.Describe() if err == nil { glog.V(4).Infof("successfully renewed lease %v", desc) return } le.config.Lock.RecordEvent("stopped leading") glog.Infof("failed to renew lease %v: %v", desc, err) close(stop) }, 0, stop)}]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于github运用hexo的next主题+google-adsense广告]]></title>
    <url>%2F2019%2F01%2F01%2Fhexo%2F</url>
    <content type="text"><![CDATA[申请github账户 配置 git page 仓库]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>next</tag>
        <tag>adsense</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables屏蔽]]></title>
    <url>%2F2017%2F01%2F29%2Fiptables%E5%B1%8F%E8%94%BD%2F</url>
    <content type="text"><![CDATA[用 iptables 屏蔽来自某个国家的 IP方法很容易，先到 IPdeny 下载以国家代码编制好的 IP 地址列表，比如下载 cn.zone： wget http://www.ipdeny.com/ipblocks/data/countries/cn.zone 有了国家的所有 IP 地址，要想屏蔽这些 IP 就很容易了，直接写个脚本逐行读取 cn.zone 文件并加入到 iptables 中： 1234567891011121314151617181920212223242526272829303132333435#!/bin/bash# Block traffic from a specific country# written by vpsee.comCOUNTRY="cn"IPTABLES=/sbin/iptablesEGREP=/bin/egrepif [ "$(id -u)" != "0" ]; then echo "you must be root" 1>&2 exit 1firesetrules() {$IPTABLES -F$IPTABLES -t nat -F$IPTABLES -t mangle -F$IPTABLES -X}resetrulesfor c in $COUNTRYdo country_file=$c.zone IPS=$($EGREP -v "^#|^$" $country_file) for ip in $IPS do echo "blocking $ip" $IPTABLES -A INPUT -s $ip -j DROP donedoneexit 0]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ipset关于neutron用法]]></title>
    <url>%2F2016%2F04%2F27%2Fipset%E5%85%B3%E4%BA%8Eneutron%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[ipset 关于neutron 的使用方法使用 iptables 封 IP，是一种比较简单的应对网络攻击的方式，也算是比较常见。有时候可能会封禁成千上万个 IP，如果添加成千上万条规则，在一台注重性能的服务器或者本身性能就很差的设备上，这就是个问题了。ipset 就是为了避免这个问题而生的。 关于 iptables，要知道这两点。 iptables 包含几个表，每个表由链组成。默认的是 filter 表，最常用的也是 filter 表，另一个比较常用的是 nat 表。一般封 IP 就是在 filter 表的 INPUT 链添加规则。在进行规则匹配时，是从规则列表中从头到尾一条一条进行匹配。这像是在链表中搜索指定节点费力。ipset 提供了把这个 O(n) 的操作变成 O(1) 的方法：就是把要处理的 IP 放进一个集合，对这个集合设置一条 iptables 规则。像 iptable 一样，IP sets 是 Linux 内核中的东西，ipset 这个命令是对它进行操作的一个工具。 简单的流程可以用这几条命令概括使用 ipset 和 iptables 进行 IP 封禁的流程 ipset create vader hash:ipiptables -I INPUT -m set –match-set vader src -j DROPipset add vader 4.5.6.7ipset add vader 1.2.3.4ipset add vader …ipset list vader # 查看 vader 集合的内容下面分别对各条命令进行描述。 创建一个集合ipset create vader hash:ip这条命令创建了名为 vader 的集合，以 hash 方式存储，存储内容是 IP 地址。 添加 iptables 规则iptables -I INPUT -m set –match-set vader src -j DROP如果源地址(src)属于 vader 这个集合，就进行 DROP 操作。这条命令中，vader 是作为黑名单的，如果要把某个集合作为白名单，添加一个 ‘!’ 符号就可以。 iptables -I INPUT -m set ! –match-set yoda src -j DROP到现在虽然创建了集合，添加了过滤规则，但是现在集合还是空的，需要往集合里加内容。 找出“坏” IP找出要封禁的 IP，这是封禁过程中重要的步骤，不过不是这里的重点。简要说明一下两种方法思路。 netstat -ntu | tail -n +3 | awk ‘{print $5}’ | sort | uniq -c | sort -nr直接通过 netstat 的信息，把与本地相关的各种状态的 IP 都计数，排序列出来。 或者从 nginx 或者其他 web server 的日志里找请求数太多的 IP awk ‘{print $1}’ /var/log/nginx/access.log | sort | uniq -c | sort -nr后半部分，排序，去重，再按次数进行逆向排序的操作，跟上面命令是一样的。 找出“坏” IP，往之前创建的集合里添加就可以了。 ipset add vader 4.5.6.7有多少“坏” IP，就添加多少 IP，因为针对这些封禁的 IP 只需要一条 iptables 规则，而这些 IP 是以 hash 方式存储，所以封禁大量的 IP 也不会影响性能，这也是 ipset 存在的最大目的。 ipset 更多的用法存储类型前面例子中的 vader 这个集合是以 hash 方式存储 IP 地址，也就是以 IP 地址为 hash 的键。除了 IP 地址，还可以是网络段，端口号（支持指定 TCP/UDP 协议），mac 地址，网络接口名称，或者上述各种类型的组合。 比如指定 hash:ip,port就是 IP 地址和端口号共同作为 hash 的键。查看 ipset 的帮助文档可以看到它支持的所有类型。 下面以两个例子说明。 hash:netipset create r2d2 hash:netipset add r2d2 1.2.3.0/24ipset add r2d2 1.2.3.0/30 nomatchipset add r2d2 6.7.8.9ipset test r2d2 1.2.3.2hash:net 指定了可以往 r2d2 这个集合里添加 IP 段或 IP 地址。 第三条命令里的 nomatch 的作用简单来说是把 1.2.3.0/30 从 1.2.3.0/24 这一范围相对更大的段里“剥离”了出来，也就是说执行完 ipset add r2d2 1.2.3.0/24 只后1.2.3.0/24 这一段 IP 是属于 r2d2 集合的，执行了 ipset add r2d2 1.2.3.0/30 nomatch 之后，1.2.3.0/24 里 1.2.3.0/30 这部分，就不属于 r2d2 集合了。执行 ipset test r2d2 1.2.3.2 就会得到结果 1.2.3.2 is NOT in set r2d2. hash:ip,portipset create c-3po hash:ip,portipset add c-3po 3.4.5.6,80ipset add c-3po 5.6.7.8,udp:53ipset add c-3po 1.2.3.4,80-86第二条命令添加的是 IP 地址为 3.4.5.6，端口号是 80 的项。没有注明协议，默认就是 TCP，下面一条命令则是指明了是 UDP 的 53 端口。最后一条命令指明了一个 IP 地址和一个端口号范围，这也是合法的命令。 自动过期，解封ipset 支持 timeout 参数，这就意味着，如果一个集合是作为黑名单使用，通过 timeout 参数，就可以到期自动从黑名单里删除内容。 ipset create obiwan hash:ip timeout 300ipset add obiwan 1.2.3.4ipset add obiwan 6.6.6.6 timeout 60上面第一条命令创建了名为 obiwan 的集合，后面多加了 timeout 参数，值为 300，往集合里添加条目的默认 timeout 时间就是 300。第三条命令在向集合添加 IP 时指定了一个不同于默认值的 timeout 值 60，那么这一条就会在 60 秒后自动删除。 隔几秒执行一次 ipset list obiwan 可以看到这个集合里条目的 timeout 一直在随着时间变化，标志着它们在多少秒之后会被删除。 如果要重新为某个条目指定 timeout 参数，要使用 -exit 这一选项。 ipset -exist add obiwan 1.2.3.4 timeout 100 这样 1.2.3.4 这一条数据的 timeout 值就变成了 100，如果这里设置 300，那么它的 timeout，也就是存活时间又重新变成 300。 如果在创建集合是没有指定 timeout，那么之后添加条目也就不支持 timeout 参数，执行 add 会收到报错。想要默认条目不会过期（自动删除），又需要添加某些条目时加上 timeout 参数，可以在创建集合时指定 timeout 为 0。 ipset create luke hash:ipipset add luke 5.5.5.5 timeout 100得到报错信息 kernel error received: Unknown error -1更大！hashsize, maxelem 这两个参数分别指定了创建集合时初始的 hash 大小，和最大存储的条目数量。 ipset create yoda hash:ip,port hashsize 4096 maxelem 1000000ipset add yoda 3.4.5.6,3306这样创建了名为 yoda 的集合，初始 hash 大小是 4096，如果满了，这个 hash 会自动扩容为之前的两倍。最大能存储的数量是 100000 个。 如果没有指定，hashsize 的默认值是 1024，maxelem 的默认值是 65536。 另外几条常用命令ipset del yoda x.x.x.x # 从 yoda 集合中删除内容ipset list yoda # 查看 yoda 集合内容ipset list # 查看所有集合的内容ipset flush yoda # 清空 yoda 集合ipset flush # 清空所有集合ipset destroy yoda # 销毁 yoda 集合ipset destroy # 销毁所有集合ipset save yoda # 输出 yoda 集合内容到标准输出ipset save # 输出所有集合内容到标准输出ipset restore # 根据输入内容恢复集合内容还有……如果创建集合是指定的存储内容包含 ip, 例如 hash:ip 或 hash:ip,port ，在添加条目时，可以填 IP 段，但是仍然是以单独一个个 IP 的方式来存。上面所有的例子都是用 hash 的方式进行存储，实际上 ipset 还可以以 bitmap 或者 link 方式存储，用这两种方式创建的集合大小，是固定的。通过 man upset 和 ipset —help 可以查到更多的内容，包括各种选项，支持的类型等等。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>neutron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack之rabbitmq]]></title>
    <url>%2F2016%2F04%2F21%2Frabbitmq%2F</url>
    <content type="text"><![CDATA[rabbitmq 的工作流程和详细分析1,rabbitmq 的 在openstack 项目中的调用过程分析 在openstack 的项目中需要 oslo.messaging 插件包 oslo.messaging 的依赖包 kombu, pika. 在这里我们只分析 rabbitmq 相关的内容OpenStack RPC 通信Openstack 组件内部的 RPC（Remote Producer Call）机制的实现是基于 AMQP(Advanced Message Queuing Protocol)作为通讯模型，从而满足组件内部的松耦合性。AMQP 是用于异步消息通讯的消息中间件协议，AMQP 模型有四个重要的角色: Exchange：根据 Routing key 转发消息到对应的 Message Queue 中 Routing key：用于 Exchange 判断哪些消息需要发送对应的 Message Queue Publisher：消息发送者，将消息发送的 Exchange 并指明 Routing Key，以便 Message Queue 可以正确的收到消息 Consumer：消息接受者，从 Message Queue 获取消息 消息发布者 Publisher 将 Message 发送给 Exchange 并且说明 Routing Key。Exchange 负责根据 Message 的 Routing Key 进行路由，将 Message 正确地转发给相应的 Message Queue。监听在 Message Queue 上的 Consumer 将会从 Queue 中读取消息。Routing Key 是 Exchange 转发信息的依据，因此每个消息都有一个 Routing Key 表明可以接受消息的目的地址，而每个 Message Queue 都可以通过将自己想要接收的 Routing Key 告诉 Exchange 进行 binding，这样 Exchange 就可以将消息正确地转发给相应的 Message Queue。图 2 就是 AMQP 消息模型。图 2. AMQP 消息模型 AMQP 定义了三种类型的 Exchange，不同类型 Exchange 实现不同的 routing 算法： Direct Exchange：Point-to-Point 消息模式，消息点对点的通信模式，Direct Exchange 根据 Routing Key 进行精确匹配，只有对应的 Message Queue 会接受到消息 Topic Exchange：Publish-Subscribe(Pub-sub)消息模式，Topic Exchange 根据 Routing Key 进行模式匹配，只要符合模式匹配的 Message Queue 都会收到消息 (模糊匹配) Fanout Exchange：广播消息模式，Fanout Exchange 将消息转发到所有绑定的 Message Queue 2， neutron 中所使用的 oslo.messaging 的服务分析这个用dhcp-agent 服务为例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159# neutron/cmd/eventlet/agents/dhcp.pyfrom neutron.agent import dhcp_agentdef main(): dhcp_agent.main()# neutron/agent/dhcp_agent.pyfrom neutron.common import config as common_configdef main(): register_options(cfg.CONF) common_config.init(sys.argv[1:]) config.setup_logging() server = neutron_service.Service.create( binary='neutron-dhcp-agent', topic=topics.DHCP_AGENT, report_interval=cfg.CONF.AGENT.report_interval, manager='neutron.agent.dhcp.agent.DhcpAgentWithStateReport') service.launch(cfg.CONF, server).wait()# 这里分析 common_config.init(sys.argv[1:])def init(args, **kwargs): cfg.CONF(args=args, project='neutron', version='%%(prog)s %s' % version.version_info.release_string(), **kwargs) # FIXME(ihrachys): if import is put in global, circular import # failure occurs from neutron.common import rpc as n_rpc n_rpc.init(cfg.CONF) # Validate that the base_mac is of the correct format msg = attributes._validate_regex(cfg.CONF.base_mac, attributes.MAC_PATTERN) if msg: msg = _("Base MAC: %s") % msg raise Exception(msg)# neutron/common/rpc.py# 这里分析n_rpc.initTRANSPORT_ALIASES = { 'neutron.openstack.common.rpc.impl_fake': 'fake', 'neutron.openstack.common.rpc.impl_qpid': 'qpid', 'neutron.openstack.common.rpc.impl_kombu': 'rabbit', 'neutron.openstack.common.rpc.impl_zmq': 'zmq', 'neutron.rpc.impl_fake': 'fake', 'neutron.rpc.impl_qpid': 'qpid', 'neutron.rpc.impl_kombu': 'rabbit', 'neutron.rpc.impl_zmq': 'zmq',}def init(conf): global TRANSPORT, NOTIFIER exmods = get_allowed_exmods() TRANSPORT = oslo_messaging.get_transport(conf, allowed_remote_exmods=exmods, aliases=TRANSPORT_ALIASES) serializer = RequestContextSerializer() NOTIFIER = oslo_messaging.Notifier(TRANSPORT, serializer=serializer)# 这里重点分析 oslo_messaging.get_transport() 方法# oslo.messaging/transport.py 文件# def get_transport(conf, url=None, allowed_remote_exmods=None, aliases=None): allowed_remote_exmods = allowed_remote_exmods or [] # 这里就涉及到 3 个 配置文件的选项 # transport_url = transport://user:pass@host1:port[,hostN:portN]/virtual_host # rpc_backend = rabbitmq # control_exchange = oepnstack conf.register_opts(_transport_opts) if not isinstance(url, TransportURL): url = url or conf.transport_url parsed = TransportURL.parse(conf, url, aliases) if not parsed.transport: raise InvalidTransportURL(url, 'No scheme specified in "%s"' % url) url = parsed kwargs = dict(default_exchange=conf.control_exchange, allowed_remote_exmods=allowed_remote_exmods) # url.transport.split('+')[0] = 'rabbit' try: mgr = driver.DriverManager('oslo.messaging.drivers', url.transport.split('+')[0], invoke_on_load=True, invoke_args=[conf, url], invoke_kwds=kwargs) # 导入rabbitmq的namespace "oslo.messaging.drivers;" 的setup.cfg 相关的driver 类 # invoke_on_load=True 并且初始化 # 初始化的参数invoke_args=[conf, url]， invoke_kwds=kwargs #oslo.messaging.drivers = # rabbit = oslo_messaging._drivers.impl_rabbit:RabbitDriver except RuntimeError as ex: raise DriverLoadFailure(url.transport, ex) #初始化 Transport 类 return Transport(mgr.driver)# 代码中的driver.DriverManager 为初始化 driver类class RabbitDriver(amqpdriver.AMQPDriverBase): """RabbitMQ Driver The ``rabbit`` driver is the default driver used in OpenStack's integration tests. The driver is aliased as ``kombu`` to support upgrading existing installations with older settings. """ def __init__(self, conf, url, default_exchange=None, allowed_remote_exmods=None): opt_group = cfg.OptGroup(name='oslo_messaging_rabbit', title='RabbitMQ driver options') conf.register_group(opt_group) conf.register_opts(rabbit_opts, group=opt_group) conf.register_opts(rpc_amqp.amqp_opts, group=opt_group) conf.register_opts(base.base_opts, group=opt_group) self.missing_destination_retry_timeout = ( conf.oslo_messaging_rabbit.kombu_missing_consumer_retry_timeout) self.prefetch_size = ( conf.oslo_messaging_rabbit.rabbit_qos_prefetch_count) connection_pool = pool.ConnectionPool( conf, conf.oslo_messaging_rabbit.rpc_conn_pool_size, url, Connection) super(RabbitDriver, self).__init__( conf, url, connection_pool, default_exchange, allowed_remote_exmods )#这里我们分析 Transport 类的初始化class Transport(object): """A messaging transport. This is a mostly opaque handle for an underlying messaging transport driver. It has a single 'conf' property which is the cfg.ConfigOpts instance used to construct the transport object. """ def __init__(self, driver): self.conf = driver.conf self._driver = driver#到这里我们的rpc 相关的都初始化完成了 分析rpc 服务的代码流程1, rpc 服务的 server 端 也就是 receive 端接受消息 rpc server 端就是一个 接受 消息端 rpc server 端需要一个 回调方法 collback 方法 rpc server 端是创建 consumer (消费者) 分析 oslo.messaging 包的分析 oslo_messaging/_drivers/./impl_rabbit.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# 创建一个 direct 队列# 队列 routing_key = queue_name = topicdef declare_direct_consumer(self, topic, callback): """Create a 'direct' queue. In nova's use, this is generally a msg_id queue used for responses for call/multicall """ consumer = Consumer(exchange_name=topic, queue_name=topic, routing_key=topic, type='direct', durable=False, exchange_auto_delete=True, queue_auto_delete=False, callback=callback, rabbit_ha_queues=self.rabbit_ha_queues, rabbit_queue_ttl=self.rabbit_transient_queues_ttl) self.declare_consumer(consumer) def declare_topic_consumer(self, exchange_name, topic, callback=None, queue_name=None): """Create a 'topic' consumer.""" consumer = Consumer(exchange_name=exchange_name, queue_name=queue_name or topic, routing_key=topic, type='topic', durable=self.amqp_durable_queues, exchange_auto_delete=self.amqp_auto_delete, queue_auto_delete=self.amqp_auto_delete, callback=callback, rabbit_ha_queues=self.rabbit_ha_queues) self.declare_consumer(consumer)#创建一个 fanout 的 consumer #第一步 需要 验证 exchange 是否存在self.exchange = kombu.entity.Exchange( name=exchange_name, type=type, durable=self.durable, auto_delete=self.exchange_auto_delete)# 第二步 创建 queue 指定 exchange 并且给定 routing_keydef declare(self, conn): """Re-declare the queue after a rabbit (re)connect.""" self.queue = kombu.entity.Queue( name=self.queue_name, channel=conn.channel, exchange=self.exchange, durable=self.durable, auto_delete=self.queue_auto_delete, routing_key=self.routing_key, queue_arguments=self.queue_arguments) try: LOG.trace('ConsumerBase.declare: ' 'queue %s', self.queue_name) self.queue.declare() except conn.connection.channel_errors as exc: # NOTE(jrosenboom): This exception may be triggered by a race # condition. Simply retrying will solve the error most of the time # and should work well enough as a workaround until the race # condition itself can be fixed. # See https://bugs.launchpad.net/neutron/+bug/1318721 for details. if exc.code == 404: self.queue.declare() else: raise#在创建 fanout 的消费者时候 不需要指定 routing_key def declare_fanout_consumer(self, topic, callback): """Create a 'fanout' consumer.""" unique = uuid.uuid4().hex exchange_name = '%s_fanout' % topic queue_name = '%s_fanout_%s' % (topic, unique) consumer = Consumer(exchange_name=exchange_name, queue_name=queue_name, routing_key=topic, type='fanout', durable=False, exchange_auto_delete=True, queue_auto_delete=False, callback=callback, rabbit_ha_queues=self.rabbit_ha_queues, rabbit_queue_ttl=self.rabbit_transient_queues_ttl) self.declare_consumer(consumer) RPC 发送请求Client 端发送 RPC 请求由 publisher 发送消息并声明消息地址，consumer 接收消息并进行消息处理，如果需要消息应答则返回处理请求的结果消息。OpenStack RPC 模块提供了 : rpc.call，rpc.cast,rpc.fanout_cast, 三种 RPC 调用方法，发送和接收 RPC 请求。 rpc.call 发送 RPC 请求并返回请求处理结果， 请求处理流程如图 5 所示，由 Topic Publisher 发送消息，Topic Exchange 根据消息地址进行消息转发至对应的 Message Queue 中，Topic Consumer 监听 Message Queue，发现需要处理的消息则进行消息处理，并由 Direct Publisher 将请求处理结果消息，请求发送方创建 Direct Consumer 监听消息的返回结果 rpc.cast 发送 RPC 请求无返回，请求处理流程如图 6 所示，与 rpc.call 不同之处在于，不需要请求处理结果的返回，因此没有 Direct Publisher 和 Direct Consumer 处理。 图 7. RPC.fanout 消息处理 cast, call 代码分析12345678910111213141516171819202122232425262728293031323334353637# cast 没有返回结果 def cast(self, ctxt, method, **kwargs): """Invoke a method and return immediately. See RPCClient.cast().""" msg = self._make_message(ctxt, method, kwargs) ctxt = self.serializer.serialize_context(ctxt) if self.version_cap: self._check_version_cap(msg.get('version')) try: self.transport._send(self.target, ctxt, msg, retry=self.retry) except driver_base.TransportDriverError as ex: raise ClientSendError(self.target, ex)# call 有返回结果def call(self, ctxt, method, **kwargs): """Invoke a method and wait for a reply. See RPCClient.call().""" if self.target.fanout: raise exceptions.InvalidTarget('A call cannot be used with fanout', self.target) msg = self._make_message(ctxt, method, kwargs) msg_ctxt = self.serializer.serialize_context(ctxt) timeout = self.timeout if self.timeout is None: timeout = self.conf.rpc_response_timeout if self.version_cap: self._check_version_cap(msg.get('version')) try: result = self.transport._send(self.target, msg_ctxt, msg, wait_for_reply=True, timeout=timeout, retry=self.retry) except driver_base.TransportDriverError as ex: raise ClientSendError(self.target, ex) return self.serializer.deserialize_entity(ctxt, result)]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nova 创建虚拟机的调用流程]]></title>
    <url>%2F2016%2F04%2F20%2Fnova-%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E8%B0%83%E7%94%A8%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[nova 创建虚拟机的调用流程1, 根据 route 找到 controller 所对应的action2, 调用 compute api 中的create 方法 3，调用 compute_task_api 这个api 是conductor.ComputeTaskAPI() conductor.ComputeTaskAPI() 根据 配置文件中的选项use_local 是否启用conductor 服务4, 调用 conductor 的rpcapi5, rpcapi 通过发送cast的消息给 conductor manager6, 在conductor manager 中首先是调用 scheduler 的 rpcapi6, scheduler rpcapi 通过 call 方法 发送消息给 scheduler manager7, 在scheduler 的 manager 中 进行 filter， weight 过滤选择，8，把选中的 host 返回给 conductor manager9，conductor manager 在 调用computer rpcapi10,computer rpcapi 获取指定host的 cctxt 通过cast 的方式把消息发给 compute manager11，computer manager 去调用相应的 driver 去 创建虚拟机]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neutron 相关agent 服务分析]]></title>
    <url>%2F2016%2F04%2F19%2Fneutron-%E7%9B%B8%E5%85%B3agent-%E6%9C%8D%E5%8A%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[neutron 相关agent 服务分析我们回到 dhcp_agent 服务在来看代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970def init(args, **kwargs): cfg.CONF(args=args, project='neutron', version='%%(prog)s %s' % version.version_info.release_string(), **kwargs) # FIXME(ihrachys): if import is put in global, circular import # failure occurs from neutron.common import rpc as n_rpc n_rpc.init(cfg.CONF) # Validate that the base_mac is of the correct format msg = attributes._validate_regex(cfg.CONF.base_mac, attributes.MAC_PATTERN) if msg: msg = _("Base MAC: %s") % msg raise Exception(msg)# 接着来分析 attributes._validate_regex(cfg.CONF.base_mac,# attributes.MAC_PATTERN)通过re 模块来判断 mac 地址是否合格def main(): register_options(cfg.CONF) common_config.init(sys.argv[1:]) config.setup_logging() server = neutron_service.Service.create( binary='neutron-dhcp-agent', topic=topics.DHCP_AGENT, report_interval=cfg.CONF.AGENT.report_interval, manager='neutron.agent.dhcp.agent.DhcpAgentWithStateReport') service.launch(cfg.CONF, server).wait()# 接着分析 server = neutron_service.Service.create() @classmethod def create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None): """Instantiates class and passes back application object. :param host: defaults to CONF.host :param binary: defaults to basename of executable :param topic: defaults to bin_name - 'neutron-' part :param manager: defaults to CONF._manager :param report_interval: defaults to CONF.report_interval :param periodic_interval: defaults to CONF.periodic_interval :param periodic_fuzzy_delay: defaults to CONF.periodic_fuzzy_delay """ if not host: host = CONF.host if not binary: binary = os.path.basename(inspect.stack()[-1][1]) if not topic: topic = binary.rpartition('neutron-')[2] topic = topic.replace("-", "_") if not manager: # manager = neutron.agent.dhcp.agent.DhcpAgentWithStateReport manager = CONF.get('%s_manager' % topic, None) if report_interval is None: report_interval = CONF.report_interval if periodic_interval is None: periodic_interval = CONF.periodic_interval if periodic_fuzzy_delay is None: periodic_fuzzy_delay = CONF.periodic_fuzzy_delay service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_interval=periodic_interval, periodic_fuzzy_delay=periodic_fuzzy_delay) return service_obj ##初始化manager 服务的详细分析 manager = neutron.agent.dhcp.agent.DhcpAgentWithStateReport 的详细分析123456789101112131415161718192021222324252627282930313233343536373839404142434445class DhcpAgentWithStateReport(DhcpAgent): def __init__(self, host=None, conf=None): super(DhcpAgentWithStateReport, self).__init__(host=host, conf=conf) self.state_rpc = agent_rpc.PluginReportStateAPI(topics.PLUGIN) #获取 rpc 服务器 self.agent_state = { 'binary': 'neutron-dhcp-agent', 'host': host, 'topic': topics.DHCP_AGENT, 'configurations': { 'dhcp_driver': self.conf.dhcp_driver, 'use_namespaces': self.conf.use_namespaces, 'dhcp_lease_duration': self.conf.dhcp_lease_duration, 'log_agent_heartbeats': self.conf.AGENT.log_agent_heartbeats}, 'start_flag': True, 'agent_type': constants.AGENT_TYPE_DHCP} #获取 rpc 更新的状态值 agent_state report_interval = self.conf.AGENT.report_interval self.use_call = True #循环执行每隔 report_interval 秒 if report_interval: self.heartbeat = loopingcall.FixedIntervalLoopingCall( self._report_state) self.heartbeat.start(interval=report_interval) #状态更新 def _report_state(self): try: self.agent_state.get('configurations').update( self.cache.get_state()) ctx = context.get_admin_context_without_session() self.state_rpc.report_state(ctx, self.agent_state, self.use_call) self.use_call = False except AttributeError: # This means the server does not support report_state LOG.warn(_LW("Neutron server does not support state report." " State report for this agent will be disabled.")) self.heartbeat.stop() self.run() return except Exception: LOG.exception(_LE("Failed reporting state!")) return if self.agent_state.pop('start_flag', None): self.run() ##dhcp 服务所要执行的linux的shell 命令汇总 kill -9 $pid 关闭dnsmasq 服务 /var/lib/neutron/dhcp/827$uuid 删除dnsmasq 的配置文件 u’ip netns exec qdhcp-827b355e-4348-41e8-96c7-c99e0ba08126 ip link set ns-b9f61118-2f up’ 启动网卡 [‘sudo’, ‘ip’, ‘netns’, ‘exec’, ‘qdhcp-827b355e-4348-41e8-96c7-c99e0ba08126’, ‘ip’, ‘-o’, ‘link’, ‘show’, ‘ns-b9f61118-2f’] sudo ip netns exec qdhcp-827b355e-4348-41e8-96c7-c99e0ba08126 ip addr show ns-b9f61118-2f permanent sudo ip netns exec qdhcp-827b355e-4348-41e8-96c7-c99e0ba08126 ip route list dev ns-b9f61118-2f sudo ip netns exec qdhcp-827b355e-4348-41e8-96c7-c99e0ba08126 dnsmasq –no-hosts –no-resolv –strict-order –except-interface=lo –pid-file=/var/lib/neutron/dhcp/827b355e-4348-41e8-96c7-c99e0ba08126/pid –dhcp-hostsfile=/var/lib/neutron/dhcp/827b355e-4348-41e8-96c7-c99e0ba08126/host –addn-hosts=/var/lib/neutron/dhcp/827b355e-4348-41e8-96c7-c99e0ba08126/addn_hosts –dhcp-optsfile=/var/lib/neutron/dhcp/827b355e-4348-41e8-96c7-c99e0ba08126/opts –dhcp-leasefile=/var/lib/neutron/dhcp/827b355e-4348-41e8-96c7-c99e0ba08126/leases –dhcp-match=set:ipxe,175 –bind-interfaces –interface=ns-b9f61118-2f –dhcp-range=set:tag0,192.168.222.0,static,86400s –dhcp-lease-max=256 –conf-file= –domain=openstacklocal 启动 dnsmasq 服务总结 每一个namespace网络都会有相应的 dnsmasq 服务 ##linux bridge 服务所要执行的linux的shell命令汇总 ip link add vxlan-1 type vlan id 1 dev eth0 proxy bridge fdb append 00:00:00:00:00:00 dev vxlan-1 dst 1.1.1.1 ip -o link show vxlan-1 sudo ip link set vxlan-1 down sudo ip link delete vxlan-1]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>neutron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux namespace 资源隔离]]></title>
    <url>%2F2016%2F04%2F19%2FLinux-namespace-%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[Linux内核namespace机制Linux Namespaces机制提供一种资源隔离方案。PID,IPC,Network等系统资源不再是全局性的，而是属于某个特定的Namespace。每个namespace下的资源对于其他namespace下的资源都是透明，不可见的。因此在操作系统层面上看，就会出现多个相同pid的进程。系统中可以同时存在两个进程号为0,1,2的进程，由于属于不同的namespace，所以它们之间并不冲突。而在用户层面上只能看到属于用户自己namespace下的资源，例如使用ps命令只能列出自己namespace下的进程。这样每个namespace看上去就像一个单独的Linux系统。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nova liberty]]></title>
    <url>%2F2016%2F04%2F04%2Fnova-liberty%2F</url>
    <content type="text"><![CDATA[nova liberty 版本中 修改 admin passowd 的流程分析在nova 的liberty 版本中 修改 admin password 的方法和线上产品修改passowd 的方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 def set_admin_password(self, context, instance, new_pass): """Set the root/admin password for an instance on this host. This is generally only called by API password resets after an image has been built. @param context: Nova auth context. @param instance: Nova instance object. @param new_pass: The admin password for the instance. """ import pdb;pdb.set_trace() context = context.elevated() if new_pass is None: # Generate a random password new_pass = utils.generate_password() current_power_state = self._get_power_state(context, instance) expected_state = power_state.RUNNING if current_power_state != expected_state: instance.task_state = None instance.save(expected_task_state=task_states.UPDATING_PASSWORD) _msg = _('instance %s is not running') % instance.uuid raise exception.InstancePasswordSetFailed( instance=instance.uuid, reason=_msg) try: self.driver.set_admin_password(instance, new_pass) LOG.info(_LI("Root password set"), instance=instance) instance.task_state = None instance.save( expected_task_state=task_states.UPDATING_PASSWORD) except NotImplementedError: LOG.warning(_LW('set_admin_password is not implemented ' 'by this driver or guest instance.'), instance=instance) instance.task_state = None instance.save( expected_task_state=task_states.UPDATING_PASSWORD) raise NotImplementedError(_('set_admin_password is not ' 'implemented by this driver or guest ' 'instance.')) except exception.UnexpectedTaskStateError: # interrupted by another (most likely delete) task # do not retry raise except Exception: # Catch all here because this could be anything. LOG.exception(_LE('set_admin_password failed'), instance=instance) self._set_instance_obj_error_state(context, instance) # We create a new exception here so that we won't # potentially reveal password information to the # API caller. The real exception is logged above _msg = _('error setting admin password') raise exception.InstancePasswordSetFailed( instance=instance.uuid, reason=_msg)总结： 在openstack L 版本中 libvirt >= 1.2.16 就已经实现了 qemu set_admin_password 的功能 注意： vm 必须是running 状态。 线上的实现方法是： 利用 libvirt_qemu 模块进行命令修改。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cinder-api]]></title>
    <url>%2F2015%2F06%2F17%2Fcinder-api%2F</url>
    <content type="text"><![CDATA[cinder api服务启动子进程分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277#!/usr/bin/evn python#author leidong#@ 依赖包的分析#@ webob 模块 简单的说，WebOb是一个用于对WSGI request环境进行包装（也就是变得易用）以及用于创建WSGI response的一个包。#@ PasteDeploy生成WSGI的Application 就是url 对应的是那些类, 简单来说就是 Application 中的各种url对应的类 使用了 webob 和 Routes 这两个模块 #@ cinder-api 服务的启动 主要包括 主进程 和 子进程#@ 主进程 功能分析和 start-volume.py 分析一样#@ 以下分析的是 子进程if __name__ == '__main__': CONF(sys.argv[1:], project='cinder', version=version.version_string()) logging.setup("cinder") utils.monkey_patch() #@ monkey_pathch 调用的是 __import__(module) #@ monkey_patch_modules = cinder.volume.volume_types:paxes_cinder.volume.volume_type_decorator #@ cinder.volume.volume_types 此类 是 __import__('cinder.volume.volume_types') 导入是一个模块 cinder.volume.volume_types.py #@ paxes_cinder.volume.volume_type_decorator 做为装饰类 importutils.import_class(paxes_cinder.volume.volume_type_decorator) 导入的是 一个类 或者是 一个 函数 #@ module_data = pyclbr.readmodule_ex(module) 返回字典形式的 模块中的 所有 class 和 function #@ {'A': , #@ 'B': , #@ 'f': } #@ for method, func in inspect.getmembers(clz, inspect.ismethod): #@ setattr( #@ clz, method, #@ decorator("%s.%s.%s" % (module, key, method), func)) #@ 此函数的功能是 给 指定模块所有方法列出来 在根据 decorator 装饰类的 指定给定的模块那个方法进行 重新设置属性值setattr rpc.init(CONF) launcher = service.process_launcher() server = service.WSGIService('osapi_volume') launcher.launch_service(server, workers=server.workers or 1) launcher.wait()#@ 主要分析的是以下代码 server = service.WSGIService('osapi_volume') #@ 初始化 cinder/service:WSGIService 类 def __init__(self, name, loader=None): """Initialize, but do not start the WSGI server. :param name: The name of the WSGI server given to the loader. :param loader: Loads the WSGI application using the given name. :returns: None """ self.name = name #@ self.name = 'osapi_volume' self.manager = self._get_manager() #@ #@ fl = '%s_manager' % self.name #@ f1 = osapi_volume_manager if fl not in CONF: #@ 在 配置文件中 没有 osapi_volume_manager 选项 return None manager_class_name = CONF.get(fl, None) #@ 如果有则获取 api 的管理类 if not manager_class_name: #@ 有选项没有指定管理类 return None manager_class = importutils.import_class(manager_class_name) #@ 导入api 的管理类 return manager_class() self.loader = loader or wsgi.Loader() self.app = self.loader.load_app(name) #@ 主要是解析 api-paste.ini 文件 #@ /v1: openstack_volume_api_v1 #@ [composite:openstack_volume_api_v1] use = call:cinder.api.middleware.auth:pipeline_factory #@ 调用的是 :pipeline_factory 方法 #@ pipeline = local_conf[CONF.auth_strategy] #@ 获取的是 api-paste.ini 文件中的pipline = request_id faultwrap sizelimit authtoken keystonecontext apiv1 if not CONF.api_rate_limit: limit_name = CONF.auth_strategy + '_nolimit' pipeline = local_conf.get(limit_name, pipeline) #@ 获取的是 api-paste.ini 文件中的keystone_nolimit = request_id faultwrap sizelimit authtoken keystonecontext apiv1 pipeline = pipeline.split() filters = [loader.get_filter(n) for n in pipeline[:-1]] #@ 获取列表 #@ request_id ： cinder.openstack.common.middleware.request_id:RequestIdMiddleware.factory 对应方法 #@ faultwrap ： cinder.api.middleware.fault:FaultWrapper.factory 对应方法 #@ sizelimit : cinder.api.middleware.sizelimit:RequestBodySizeLimiter.factory 对应方法 #@ authtoken : keystoneclient.middleware.auth_token:filter_factory #@ keystonecontext : cinder.api.middleware.auth:CinderKeystoneContext.factory app = loader.get_app(pipeline[-1]) #@ 获取 v1 ： cinder.api.v1.router:APIRouter.factory filters.reverse() #@ 反转 filters 列表 for filter in filters: app = filter(app) #@ 装饰的顺序为 #@ request_id(faultwrap(sizelimit(authtoken(keystonecontext(v1))))) return app #@ app = request_id(faultwrap(sizelimit(authtoken(keystonecontext(v1))))) #@ app v1 初始化的过程: v1=cinder.api.v1.router:APIRouter.factory #@ 调用 父类(cinder.api.openstack.APIRouter) 的 类方法 factory #@ def factory(cls, global_config, **local_config): return cls() #@ 初始化自己 调用 父类的 cinder.api.openstack.APIRouter.__init__ 方法 #@ ExtensionManager = extensions.ExtensionManager #@ cinder.api.extensions.ExtensionManager 类 #@ if ext_mgr is None: if self.ExtensionManager: ext_mgr = self.ExtensionManager() #@ 初始化cinder.api.extensions.ExtensionManager #@ def __init__(self): LOG.audit(_('Initializing extension manager.')) #@ 打印 日志文件的 第二部分 #@ 2015-06-10 02:32:21.251 1038 AUDIT cinder.api.extensions [-] Initializing extension manager. self.cls_list = CONF.osapi_volume_extension #@ osapi_volume_extension=cinder.api.contrib.standard_extensions self.extensions = {} self._load_extensions() else: raise Exception(_("Must specify an ExtensionManager class")) mapper = ProjectMapper() self.resources = {} self._setup_routes(mapper, ext_mgr) self._setup_ext_routes(mapper, ext_mgr) self._setup_extensions(ext_mgr) super(APIRouter, self).__init__(mapper) #@ 最后返回 app #@ if not CONF.enable_v1_api: del local_conf['/v1'] if not CONF.enable_v2_api: del local_conf['/v2'] return paste.urlmap.urlmap_factory(loader, global_conf, **local_conf) #@ 返回一个 dict self.host = getattr(CONF, '%s_listen' % name, "0.0.0.0") self.port = getattr(CONF, '%s_listen_port' % name, 0) self.workers = getattr(CONF, '%s_workers' % name, None) if self.workers < 1: LOG.warn(_("Value of config option %(name)s_workers must be " "integer greater than 1. Input value ignored.") % {'name': name}) # Reset workers to default self.workers = None self.server = wsgi.Server(name, self.app, host=self.host, port=self.port)]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cinder reset api接口封装流程]]></title>
    <url>%2F2015%2F06%2F17%2Fcinder-reset-api%E6%8E%A5%E5%8F%A3%E5%B0%81%E8%A3%85%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Cinder接口到处理函数的流程分析 当发送一个 api 请求的时候调用方法的流程:一个接口 到其 处理 函数的流程: eg : curl -g -i -X GET http://192.168.122.166:8776/v1/d32d063b07414d9099a1b176e4898d2b/os-quota-sets/d32d063b07414d9099a1b176e4898d2b?usage=False cinder.api.contrib.quotas.QuotaSetsController.show 根据 api-paste.ini 文件知道： request_id faultwrap sizelimit authtoken keystonecontext apiv1 v1 api的封装流程–> request_id(faultwrap(sizelimit(authtoken(keystonecontext(apiv1))))) 当api服务接受到 请求时候 调用的 request_id faultwrap sizelimit authtoken keystonecontext 各个类的call 方法: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682691, cinder.openstack.common.middleware.request_id:RequestIdMiddleware #@ 调用其父类的__call__ 方法: cinder.openstack.common.middleware.request_id:RequestIdMiddleware def __call__(self, req): #@ req = Request: GET /v1/d32d063b07414d9099a1b176e4898d2b/os-quota-sets/d32d063b07414d9099a1b176e4898d2b?usage=False HTTP/1.0 #@ Accept: application/json #@ Accept-Encoding: gzip, deflate, sdch #@ Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4 #@ Cache-Control: no-cache #@ Connection: keep-alive #@ Content-Type: text/plain #@ Csp: active #@ Host: 192.168.122.166:8776 #@ User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36 #@ X-Auth-Token: 0ede531e8d774ebcb7689f20bef0caca response = self.process_request(req) #@ 该函数功能就是 设置请求req 的id 然后加入到请求的变量中 #@ def process_request(self, req): #@ self.req_id = context.generate_request_id() self.req_id = str: req-8e464c2e-90b2-4b28-b321-1e3486dde93a #@cinder.openstack.common.context #@ def generate_request_id(): #@ return 'req-%s' % uuidutils.generate_uuid() #@ str(uuid.uuid4()) #@ str: req-8e464c2e-90b2-4b28-b321-1e3486dde93a #@ req.environ[ENV_REQUEST_ID] = self.req_id req.environ[ENV_REQUEST_ID] = req-8e464c2e-90b2-4b28-b321-1e3486dde93a if response: #@ response = None return response response = req.get_response(self.application) #@ req.get_response 会调用cinder.api.middleware.fault:FaultWrapper 类的 __call__ 方法 #@ 分析 会调用cinder.api.middleware.fault:FaultWrapper.__call__ 方法 def __call__(self, req): try: return req.get_response(self.application) #@ 分析 会调用 cinder.api.middleware.sizelimit:RequestBodySizeLimiter.__call__ 方法 def __call__(self, req): #@ req.content_length = None if req.content_length > CONF.osapi_max_request_body_size: #@ 判断 请求的长度是否大于 配置文件中的 osapi_max_request_body_size= 114688 msg = _("Request is too large.") raise webob.exc.HTTPRequestEntityTooLarge(explanation=msg) if req.content_length is None and req.is_body_readable: limiter = LimitingReader(req.body_file, CONF.osapi_max_request_body_size) req.body_file = limiter return self.application #@ 分析 cinder.api.middleware.sizelimit:RequestBodySizeLimiter 类的作用是在 检查 req bady 的长度 #@ 分析 会调用keystoneclient.middleware.auth_token:filter_factory.__call__ 方法 def __call__(self, env, start_response): self.LOG.debug('Authenticating user token') #@ env = dict: {'HTTP_CSP': 'active', 'SCRIPT_NAME': '/v1', 'webob.adhoc_attrs': {'response': }, 'REQUEST_METHOD': 'GET', 'PATH_INFO': '/d32d063b07414d9099a1b176e4898d2b/os-quota-sets/d32d063b07414d9099a1b176e4898d2b', 'SERVER_PROTOCOL': 'HTTP/1.0', 'QUERY_STRING': 'usage=False', 'HTTP_X_AUTH_TOKEN': '0ede531e8d774ebcb7689f20bef0caca', 'HTTP_USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36', 'HTTP_CONNECTION': 'keep-alive', 'REMOTE_PORT': '32828', 'SERVER_NAME': '192.168.122.166', 'REMOTE_ADDR': '192.168.122.1', 'eventlet.input': , 'wsgi.url_scheme': 'http', 'SERVER_PORT': '8776', 'wsgi.input': , 'HTTP_HOST': '192.168.122.166:8776', 'wsgi.multithread': True, 'HTTP_CACHE_CONTROL': 'no-cache', 'eventlet.posthooks': [], 'HTTP_ACCEPT': 'application/json', 'openstack.request_id': 'req-78794fde-a219-40be-a181-d416d8a4d29e... self._token_cache.initialize(env) try: self._remove_auth_headers(env) user_token = self._get_user_token_from_header(env) token_info = self._validate_user_token(user_token, env) #@ 验证token 方法 env['keystone.token_info'] = token_info user_headers = self._build_user_headers(token_info) self._add_headers(env, user_headers) return self.app(env, start_response) except InvalidUserToken: if self.delay_auth_decision: self.LOG.info( 'Invalid user token - deferring reject downstream') self._add_headers(env, {'X-Identity-Status': 'Invalid'}) return self.app(env, start_response) else: self.LOG.info('Invalid user token - rejecting request') return self._reject_request(env, start_response) except ServiceError as e: self.LOG.critical('Unable to obtain admin token: %s', e) resp = MiniResp('Service unavailable', env) start_response('503 Service Unavailable', resp.headers) return resp.body #@ 分析 keystoneclient.middleware.auth_token:filter_factory 验证 token #@ 分析 cinder.api.middleware.auth:CinderKeystoneContext.__call__ 方法 @webob.dec.wsgify(RequestClass=base_wsgi.Request) def __call__(self, req): user_id = req.headers.get('X_USER') user_id = req.headers.get('X_USER_ID', user_id) if user_id is None: LOG.debug("Neither X_USER_ID nor X_USER found in request") return webob.exc.HTTPUnauthorized() # get the roles roles = [r.strip() for r in req.headers.get('X_ROLE', '').split(',')] if 'X_TENANT_ID' in req.headers: # This is the new header since Keystone went to ID/Name project_id = req.headers['X_TENANT_ID'] else: # This is for legacy compatibility project_id = req.headers['X_TENANT'] project_name = req.headers.get('X_TENANT_NAME') req_id = req.environ.get(request_id.ENV_REQUEST_ID) # Get the auth token auth_token = req.headers.get('X_AUTH_TOKEN', req.headers.get('X_STORAGE_TOKEN')) # Build a context, including the auth_token... remote_address = req.remote_addr service_catalog = None if req.headers.get('X_SERVICE_CATALOG') is not None: try: catalog_header = req.headers.get('X_SERVICE_CATALOG') service_catalog = jsonutils.loads(catalog_header) except ValueError: raise webob.exc.HTTPInternalServerError( _('Invalid service catalog json.')) if CONF.use_forwarded_for: remote_address = req.headers.get('X-Forwarded-For', remote_address) ctx = context.RequestContext(user_id, project_id, project_name=project_name, roles=roles, auth_token=auth_token, remote_address=remote_address, service_catalog=service_catalog, request_id=req_id) req.environ['cinder.context'] = ctx return self.application #@ cinder.api.middleware.auth:CinderKeystoneContext.__call__ 作用是 创建Context 上下文 封装 req 请求 #@ 接着会调用 cinder.api.openstack.wsgi.Resource 类的__call__ 方法 #@ Resource.__call__ ---> Resource._process_stack ---> Resource.dispatch 调用流程 #@ def dispatch(self, method, request, action_args): #@ """Dispatch a call to the action-specific method.""" #@ return method(req=request, **action_args) #@ method = instancemethod: > #@ 分析 method 方法则可知 dispatch 会映射到 cinder.api.contrib.quotas.QuotaSetsController.show 方法 except Exception as ex: return self._error(ex, req) #@ 分析 会调用cinder.api.middleware.fault:FaultWrapper 作用在 异常处理 return self.process_response(response)]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cinder-volume]]></title>
    <url>%2F2015%2F06%2F17%2Fcinder-volume%2F</url>
    <content type="text"><![CDATA[cinder-volume服务主进程分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791#!/usr/bin/env python#author zwei#@ cinder-volume 为例子######## cinder-volume 服务启动的 流程 ############## CONF(sys.argv[1:], project='cinder', version=version.version_string()) #@ 初始化 /etc/cinder/cinder.conf 配置文件中 的选项值 如果配置文件中没有则 以 代码中 注册的 值为默认值 logging.setup("cinder") #@ 设置 日志文件的目录 和 和 日志处理的 方式 和 日志 输出的 格式 utils.monkey_patch() #@ 动态导入 模块 CONF.monkey_patch: #@ 根据 配置文件 中的 monkey_patch 的值 True 和 False CONF.monkey_patch_modules #@ 如果 CONF.monkey_patch 为True 则 会 导入 CONF.monkey_patch_modules 中的 各种 类 使用 __import__(module) launcher = service.get_launcher() #@ 使用 cindr/service 中的 get_launcher() if os.name == 'nt': #@ 判断 操作系统类型 nt 表示 windows return Launcher() #@class Launcher(object): #@ def __init__(self): #@ self.launch_service = serve #@ self.wait = wait #@ 表示初始话一个新的类 没有 任何 功能方法 所有 cinder-volume 不能在 windows 上运行 其他服务 类似 else: return process_launcher() #@ return service.ProcessLauncher() #@ 会调用 cinder/openstack/common/service.py 中的ProcessLauncher 类 初始化 该 类 #@ 初始化 process_launcher 类 self.children = {} self.sigcaught = None self.running = True self.wait_interval = wait_interval = 0.01 #@ 默认值 rfd, self.writepipe = os.pipe() #@ 返回一个 rfd read 文件描述符 self.writepipe write 文件描述符 self.readpipe = eventlet.greenio.GreenPipe(rfd, 'r') self.handle_signal() #@ def handle_signal(self): #@ _set_signals_handler(self._handle_signal) #@ def _set_signals_handler(handler): signal.signal(signal.SIGTERM, handler) #@ 表示 当程序 收到 signal.SIGTERM 信号的时候 会调用 _handle_signal 函数 #@ signal.SIGTERM 参考 kill -l 和 man 7 signal 查看详细信息 #@ SIGTERM 15 Term Termination signal #@ 程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出，shell命令kill缺省产生这个信号。如果进程终止不了，我们才会尝试SIGKILL #@ 当程序自己 正常结束所 收到的信号 signal.signal(signal.SIGINT, handler) #@ 当后台作业要从用户终端读数据时, 该作业中的所有进程会收到SIGTTIN信号. 缺省时这些进程会停止执行. if _sighup_supported(): #@ return hasattr(signal, 'SIGHUP') #@ 查看 signal 是否 有 SIGHUP 属性 signal.signal(signal.SIGHUP, handler) #@ 本信号在用户终端连接(正常或非正常)结束时发出, 通常是在终端的控制进程结束时, 通知同一session内的各个作业, 这时它们与控制终端不再关联。 #@ http://blog.chinaunix.net/uid-26111972-id-3794962.html 参考文档 #@ 会调用 def _handle_signal(self, signo, frame): self.sigcaught = signo self.running = False for backend in CONF.enabled_backends: #@获取 配置文件中的 enabled_backends 选项 eg ['vg1','vg2','nfs1','nfs2'] host = "%s@%s" % (CONF.host, backend) #eg: ivm140@nfs1 server = service.Service.create(host=host, service_name=backend, binary='cinder-volume') #@ 调用cinder/service.py:service 类的 create 类方法: def create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None, service_name=None): if not host: host = CONF.host #@ 在 不使用enabled_backends 选项时候会去调用 配置文件中的 host 选项 默认值 是eg default=socket.gethostname() 当前系统的主机名称 在 cinder/common/config.py 144 行配置 注册 binary = os.path.basename(inspect.stack()[-1][1]) #@ 获取当前执行的脚本名称（cinder-volume） subtopic = topic.rpartition('cinder-')[2] #@ 将字符串转换为 元组 ('','cinder-','volume') manager = CONF.get('%s_manager' % subtopic, None) #@ 获取 卷的 管理器 的 类 volume_manager = cinder.volume.manager.VolumeManager report_interval = CONF.report_interval #@ 报告 cinder-volume 服务的状态信息 periodic_interval = CONF.periodic_interval periodic_fuzzy_delay = CONF.periodic_fuzzy_delay service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_interval=periodic_interval, periodic_fuzzy_delay=periodic_fuzzy_delay, service_name=service_name) #@ 初始化 cinder/service/Service 类 def __init__(self, host, binary, topic, manager, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None, service_name=None, *args, **kwargs): super(Service, self).__init__() #@ 初始化 父类 #@ def __init__(self, threads=1000): self.tg = threadgroup.ThreadGroup(threads) #@ 设置协程 池的大小 初始化ThreadGroup 类 #@ cinder/openstack/common/threadgroup.py:ThreadGroup #@ def __init__(self, thread_pool_size=10): self.pool = greenpool.GreenPool(thread_pool_size) #@ 初始化 greenpool.GreenPool(10) 协程池 self.threads = [] self.timers = [] # signal that the service is done shutting itself down: self._done = threading.Event() #@ 初始化 threading 模块中的 Event 类 if not rpc.initialized(): #@ rpc.initialized() 返回值 false rpc.init(CONF) #@ rpc 初始化 def init(conf): global TRANSPORT, NOTIFIER exmods = get_allowed_exmods() TRANSPORT = messaging.get_transport(conf, allowed_remote_exmods=exmods, aliases=TRANSPORT_ALIASES) #@ 会加载 oslo.messaging-1.4.1.dist-info/entry_points.txt 文件中的 oslo.messaging.drivers #@ 打印的日志 #@ 服务启动时候 最开始打印的日志 #@ 2015-06-08 23:05:02.697 27346 DEBUG stevedore.extension [-] #@ found extension EntryPoint.parse('qpid = oslo.messaging._drivers.impl_qpid:QpidDriver') #@ _load_plugins /usr/lib/python2.6/site-packages/stevedore/extension.py:157 serializer = RequestContextSerializer(JsonPayloadSerializer()) NOTIFIER = messaging.Notifier(TRANSPORT, serializer=serializer) #@ 此段中 会注册 notification_driver 和 notification_topics 两个选项 #@ 使用 notification_driver 选项在 oslo/messaging/notifly/notifier:Notifier,__init__ 125 line #@ 使用 notification_topics 选项在 oslo/messaging/notifly/notifier:Notifier,__init__ 128 line #@ 调用以下方法 #@ self._driver_mgr = named.NamedExtensionManager( 'oslo.messaging.notify.drivers', names=self._driver_names, invoke_on_load=True, invoke_args=[transport.conf], invoke_kwds={ 'topics': self._topics, 'transport': self.transport, } #@ 最后调用 pkg_resources.iter_entry_points(namespace) namespace='oslo.messaging.notify.drivers' #可以知道 pkg_resources 会查看 所有的entry_points.txt文件 查找 oslo.messaging.notify.drivers #@ 打印的日志 #@ 服务启动时候 第二部分打印的日志 #@ 会加载 cinder.egg-info/entry_points.txt 文件中的 oslo.messaging.notify.drivers #@ 2015-06-08 23:34:38.060 27346 DEBUG stevedore.extension [-] #@ found extension EntryPoint.parse('cinder.openstack.common.notifier.no_op_notifier = oslo.messaging.notify._impl_noop:NoOpDriver') #@ _load_plugins /usr/lib/python2.6/site-packages/stevedore/extension.py:157 #@ 会加载 oslo.messaging-1.4.1.dist-info/entry_points.txt 文件中的 oslo.messaging.notify.drivers #@ 2015-06-08 23:38:28.369 27346 DEBUG stevedore.extension [-] #@ found extension EntryPoint.parse('log = oslo.messaging.notify._impl_log:LogDriver') _load_plugins /usr/lib/python2.6/site-packages/stevedore/extension.py:157 self.host = host #@str: cinder.flftuu.com@vg1 self.binary = binary #@ str: cinder-volume self.topic = topic #@ str: cinder-volume self.manager_class_name = manager #@ str: cinder.volume.manager.VolumeManager manager_class = importutils.import_class(self.manager_class_name) #@ _PeriodicTasksMeta: self.manager = manager_class(host=self.host, service_name=service_name, *args, **kwargs) #@ 初始化 cinder.volume.manager.VolumeManager 类 self.report_interval = report_interval #@ int: 10 10s 跟新数据库状态 self.periodic_interval = periodic_interval #@ int: 60 60s 发送消息60s 一次 到 rabbitmq self.periodic_fuzzy_delay = periodic_fuzzy_delay #@ int: 60 self.basic_config_check() #@ if CONF.service_down_time]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql]]></title>
    <url>%2F2013%2F09%2F19%2Fmysql%2F</url>
    <content type="text"><![CDATA[mysql 主从复制方法两台主机hostname : linux-web ip : 10.5.5.23hostname : linux-mysql ip : 10.5.5.27 host1: linux-web 123456789101112vim /etc/my.cnfserver-id=1log-bin=master-binmaster-host=10.5.5.27 #hostname linux-mysqlmaster-user=slave #mysql 中的账户 slave(在linux-mysql) 用于相互链接master-password=slavereplicate-ignore-db=mysql #不同步mysql databasereplicate-do-db=sync #指定同步database host2:linux-mysql 同上需要改 server-id=2 1234cd /var/lib/mysql#删除所有的以前的日志service mysqld restart #(两台都重启) 如何修改MySQL监听IP地址1.编辑/etc/my.cnf 在[mysqld]节中增加下面一行： bind-address=0.0.0.0 #全部地址或者指定的ip地址 2.重启服务 service mysqld restart 3.验证 netstat -tln]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-服务启动]]></title>
    <url>%2F2013%2F09%2F10%2Flinux-%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[判断linux 系统服务 是否启动的脚本12345678910#！/bin/bashif service sshd status &> /dev/nllthen echo "service is running"else echo "service is stopd"fi 修改系统的默认语言环境 vim /etc/sysconfig/i18n LANG=”en_US.UTF-8”]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
</search>
